{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hugging Face Pipelines üöÄ\n",
    "\n",
    "Pipelines are the easiest way to use pre-trained models in Hugging Face. They provide a high-level interface that handles tokenization, model inference, and post-processing automatically.\n",
    "\n",
    "## What are Pipelines?\n",
    "\n",
    "Pipelines are **one-line solutions** for common NLP tasks:\n",
    "- **Input**: Raw text\n",
    "- **Output**: Ready-to-use results\n",
    "- **Magic**: Everything happens behind the scenes!\n",
    "\n",
    "```python\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this!\")\n",
    "# ‚Üí [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Use all major pipeline types\n",
    "2. Customize pipeline behavior and models\n",
    "3. Handle batch processing and performance optimization\n",
    "4. Build custom pipelines for specific needs\n",
    "5. Compare pipeline performance and choose the right one\n",
    "6. Integrate pipelines into real applications\n",
    "\n",
    "Let's dive into the world of pipelines! üåä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModel\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_name = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Using device: {device_name}\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of Available Pipelines\n",
    "\n",
    "Let's start by exploring all available pipeline tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all supported pipeline tasks\n",
    "print(\"üéØ Available Pipeline Tasks:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Group tasks by category with examples\n",
    "task_categories = {\n",
    "    \"Text Analysis\": [\n",
    "        \"sentiment-analysis\",\n",
    "        \"text-classification\", \n",
    "        \"zero-shot-classification\",\n",
    "        \"token-classification\",\n",
    "        \"ner\"\n",
    "    ],\n",
    "    \"Text Generation\": [\n",
    "        \"text-generation\",\n",
    "        \"text2text-generation\",\n",
    "        \"fill-mask\",\n",
    "        \"conversational\"\n",
    "    ],\n",
    "    \"Question & Answer\": [\n",
    "        \"question-answering\",\n",
    "        \"table-question-answering\"\n",
    "    ],\n",
    "    \"Text Processing\": [\n",
    "        \"summarization\",\n",
    "        \"translation\",\n",
    "        \"feature-extraction\"\n",
    "    ],\n",
    "    \"Audio & Vision\": [\n",
    "        \"automatic-speech-recognition\",\n",
    "        \"audio-classification\",\n",
    "        \"image-classification\",\n",
    "        \"object-detection\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tasks in task_categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for task in tasks:\n",
    "        print(f\"  ‚úÖ {task}\")\n",
    "\n",
    "print(f\"\\Total tasks shown: {sum(len(tasks) for tasks in task_categories.values())}\")\n",
    "print(f\"We'll explore the most popular ones in this notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis Pipeline\n",
    "\n",
    "Let's start with the most popular pipeline - sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment analysis pipeline\n",
    "print(\"üé≠ Sentiment Analysis Pipeline\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Default sentiment pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Test with various examples\n",
    "test_texts = [\n",
    "    \"I absolutely love this new AI technology!\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"The weather is okay today, nothing special.\",\n",
    "    \"Amazing breakthrough in machine learning! So excited!\",\n",
    "    \"I'm not sure how I feel about this change...\",\n",
    "    \"The service was disappointing and slow.\",\n",
    "    \"Perfect! Everything worked exactly as expected.\",\n",
    "    \"Meh, it's just average.\"\n",
    "]\n",
    "\n",
    "print(\"Testing sentiment analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "results = sentiment_pipeline(test_texts)\n",
    "\n",
    "for text, result in zip(test_texts, results):\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # Add emoji based on sentiment\n",
    "    emoji = \"üòä\" if label == \"POSITIVE\" else \"üòû\" if label == \"NEGATIVE\" else \"üòê\"\n",
    "    \n",
    "    print(f\"{emoji} '{text}'\")\n",
    "    print(f\"   ‚Üí {label} (confidence: {score:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different sentiment models\n",
    "print(\"Comparing Different Sentiment Models\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sentiment_models = [\n",
    "    (\"Default\", \"sentiment-analysis\", None),\n",
    "    (\"Twitter-tuned\", \"sentiment-analysis\", \"cardiffnlp/twitter-roberta-base-sentiment-latest\"),\n",
    "    (\"Financial\", \"sentiment-analysis\", \"ProsusAI/finbert\")\n",
    "]\n",
    "\n",
    "test_sentence = \"The stock market is showing positive trends today.\"\n",
    "print(f\"Test sentence: '{test_sentence}'\\n\")\n",
    "\n",
    "for name, task, model in sentiment_models:\n",
    "    try:\n",
    "        if model:\n",
    "            pipe = pipeline(task, model=model, device=device)\n",
    "        else:\n",
    "            pipe = sentiment_pipeline  # Use the one we already created\n",
    "        \n",
    "        result = pipe(test_sentence)[0]\n",
    "        print(f\"{name} model:\")\n",
    "        print(f\"  Label: {result['label']}\")\n",
    "        print(f\"  Score: {result['score']:.3f}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{name} model: Error - {str(e)[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classification Pipeline\n",
    "\n",
    "Beyond sentiment, we can classify text into multiple categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification - classify without training data!\n",
    "print(\" Zero-Shot Classification Pipeline\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", device=device)\n",
    "\n",
    "# Define candidate labels\n",
    "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"science\", \"business\"]\n",
    "\n",
    "test_articles = [\n",
    "    \"Apple announced a new iPhone with revolutionary AI capabilities.\",\n",
    "    \"The Lakers won their championship game last night in overtime.\",\n",
    "    \"Scientists discovered a new planet in a distant galaxy.\",\n",
    "    \"The new Marvel movie breaks box office records worldwide.\",\n",
    "    \"Stock markets rally as tech companies report strong earnings.\",\n",
    "    \"New research shows promising results for cancer treatment.\"\n",
    "]\n",
    "\n",
    "print(f\"Classifying into categories: {candidate_labels}\\n\")\n",
    "\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    result = zero_shot_classifier(article, candidate_labels)\n",
    "    \n",
    "    print(f\"{i}. '{article}'\")\n",
    "    print(f\"   üèÜ Top prediction: {result['labels'][0]} ({result['scores'][0]:.3f})\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    print(\"   üìä All predictions:\")\n",
    "    for label, score in zip(result['labels'][:3], result['scores'][:3]):\n",
    "        print(f\"      ‚Ä¢ {label}: {score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER) Pipeline\n",
    "\n",
    "Extract people, organizations, locations, and other entities from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "print(\" Named Entity Recognition Pipeline\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "test_texts_ner = [\n",
    "    \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\",\n",
    "    \"Elon Musk announced that Tesla will build a new factory in Berlin, Germany.\",\n",
    "    \"The meeting between Joe Biden and Emmanuel Macron was held in Paris.\",\n",
    "    \"Microsoft's headquarters in Redmond employs over 50,000 people.\",\n",
    "    \"Amazon reported $469 billion in revenue last year.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts_ner, 1):\n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    print(f\"{i}. Text: '{text}'\")\n",
    "    print(\"   Entities found:\")\n",
    "    \n",
    "    if entities:\n",
    "        # Group entities by type\n",
    "        entity_groups = {}\n",
    "        for entity in entities:\n",
    "            entity_type = entity['entity_group']\n",
    "            if entity_type not in entity_groups:\n",
    "                entity_groups[entity_type] = []\n",
    "            entity_groups[entity_type].append(entity)\n",
    "        \n",
    "        for entity_type, entities_list in entity_groups.items():\n",
    "            print(f\"   üìç {entity_type}:\")\n",
    "            for entity in entities_list:\n",
    "                print(f\"      ‚Ä¢ {entity['word']} (confidence: {entity['score']:.3f})\")\n",
    "    else:\n",
    "        print(\"   No entities found.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NER results\n",
    "def visualize_ner(text, entities):\n",
    "    \"\"\"Create a simple HTML visualization of NER results.\"\"\"\n",
    "    \n",
    "    # Color mapping for different entity types\n",
    "    colors = {\n",
    "        'PER': '#FFB6C1',      # Person - Light Pink\n",
    "        'ORG': '#87CEEB',      # Organization - Sky Blue  \n",
    "        'LOC': '#98FB98',      # Location - Pale Green\n",
    "        'MISC': '#DDA0DD',     # Miscellaneous - Plum\n",
    "        'PERSON': '#FFB6C1',\n",
    "        'ORGANIZATION': '#87CEEB',\n",
    "        'LOCATION': '#98FB98'\n",
    "    }\n",
    "    \n",
    "    # Sort entities by start position\n",
    "    sorted_entities = sorted(entities, key=lambda x: x['start'])\n",
    "    \n",
    "    html_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for entity in sorted_entities:\n",
    "        start = entity['start'] + offset\n",
    "        end = entity['end'] + offset\n",
    "        entity_type = entity['entity_group']\n",
    "        word = entity['word']\n",
    "        score = entity['score']\n",
    "        \n",
    "        color = colors.get(entity_type, '#FFFFE0')  # Default to light yellow\n",
    "        \n",
    "        # Create highlighted span\n",
    "        highlighted = f'<span style=\"background-color: {color}; padding: 2px 4px; border-radius: 3px; font-weight: bold;\" title=\"{entity_type}: {score:.3f}\">{word}</span>'\n",
    "        \n",
    "        # Replace the original word with highlighted version\n",
    "        html_text = html_text[:start] + highlighted + html_text[end:]\n",
    "        \n",
    "        # Update offset\n",
    "        offset += len(highlighted) - (end - start)\n",
    "    \n",
    "    return html_text\n",
    "\n",
    "# Example visualization\n",
    "example_text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "example_entities = ner_pipeline(example_text)\n",
    "\n",
    "print(\" NER Visualization Example:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Original: {example_text}\")\n",
    "print(\"\\nVisualized (colors represent entity types):\")\n",
    "\n",
    "# Show color legend\n",
    "legend_html = \"\"\"\n",
    "<div style='margin: 10px 0;'>\n",
    "    <b>Legend:</b><br>\n",
    "    <span style='background-color: #FFB6C1; padding: 2px 4px; border-radius: 3px;'>Person</span>\n",
    "    <span style='background-color: #87CEEB; padding: 2px 4px; border-radius: 3px;'>Organization</span>\n",
    "    <span style='background-color: #98FB98; padding: 2px 4px; border-radius: 3px;'>Location</span>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "if example_entities:\n",
    "    visualization = visualize_ner(example_text, example_entities)\n",
    "    display(HTML(legend_html + \"<div style='font-size: 14px; line-height: 1.5;'>\" + visualization + \"</div>\"))\n",
    "else:\n",
    "    print(\"No entities found in the example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Question Answering Pipeline\n",
    "\n",
    "Extract answers from context using question answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "print(\" Question Answering Pipeline\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "\n",
    "# Provide context about Hugging Face\n",
    "context = \"\"\"\n",
    "Hugging Face is an American company founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf. \n",
    "The company is headquartered in New York City with additional offices in Paris, France. \n",
    "Hugging Face is known for developing the Transformers library, which provides thousands of pre-trained models \n",
    "for natural language processing, computer vision, and audio tasks. The library supports both PyTorch and \n",
    "TensorFlow frameworks. In 2021, the company raised $40 million in Series B funding. \n",
    "The Hugging Face Hub hosts over 100,000 models and 10,000 datasets, making it the largest repository \n",
    "of open-source AI models. The company's mission is to democratize artificial intelligence.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Who are the founders of Hugging Face?\", \n",
    "    \"Where is Hugging Face headquartered?\",\n",
    "    \"What is the Transformers library?\",\n",
    "    \"How much funding did they raise in 2021?\",\n",
    "    \"How many models are on the Hugging Face Hub?\",\n",
    "    \"What is Hugging Face's mission?\",\n",
    "    \"Which frameworks does the library support?\"\n",
    "]\n",
    "\n",
    "print(f\"Context: {context[:100]}...\\n\")\n",
    "print(\"Questions and Answers:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    answer = result['answer']\n",
    "    score = result['score']\n",
    "    \n",
    "    # Add confidence indicator\n",
    "    confidence_emoji = \"üü¢\" if score > 0.8 else \"üü°\" if score > 0.5 else \"üî¥\"\n",
    "    \n",
    "    print(f\"{i}. {confidence_emoji} Q: {question}\")\n",
    "    print(f\"   A: {answer} (confidence: {score:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation Pipeline\n",
    "\n",
    "Generate creative text continuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Pipeline\n",
    "print(\" Text Generation Pipeline\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Load text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
    "\n",
    "prompts = [\n",
    "    \"Artificial intelligence will\",\n",
    "    \"The future of technology\",\n",
    "    \"Machine learning models are\",\n",
    "    \"In the year 2030,\",\n",
    "    \"The most exciting thing about AI is\"\n",
    "]\n",
    "\n",
    "print(\"Generating text completions...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"{i}. Prompt: '{prompt}'\")\n",
    "    \n",
    "    # Generate multiple variations\n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=30,  # Number of new tokens to generate\n",
    "        num_return_sequences=2,  # Generate 2 variations\n",
    "        temperature=0.7,  # Control randomness\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,  # Handle padding\n",
    "        do_sample=True,  # Enable sampling\n",
    "        truncation=True  # Enable truncation\n",
    "    )\n",
    "    \n",
    "    print(\"   Generated completions:\")\n",
    "    for j, result in enumerate(results, 1):\n",
    "        generated_text = result['generated_text']\n",
    "        # Extract only the new part (after the prompt)\n",
    "        completion = generated_text[len(prompt):].strip()\n",
    "        print(f\"   {j}. '{prompt} {completion}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fill-Mask Pipeline\n",
    "\n",
    "Fill in missing words in sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-Mask Pipeline\n",
    "print(\" Fill-Mask Pipeline\")\n",
    "print(\"=\" * 22)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", device=device)\n",
    "\n",
    "masked_sentences = [\n",
    "    \"The capital of France is <mask>.\",\n",
    "    \"Machine learning is a branch of <mask>.\",\n",
    "    \"The <mask> is the largest planet in our solar system.\",\n",
    "    \"Python is a popular <mask> language.\",\n",
    "    \"<mask> revolutionized the field of natural language processing.\",\n",
    "    \"The iPhone was invented by <mask>.\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(masked_sentences, 1):\n",
    "    print(f\"{i}. Sentence: '{sentence}'\")\n",
    "    \n",
    "    # Get top predictions\n",
    "    results = fill_mask(sentence)\n",
    "    \n",
    "    print(\"   Top predictions:\")\n",
    "    for j, result in enumerate(results[:3], 1):  # Show top 3\n",
    "        token = result['token_str'].strip()\n",
    "        score = result['score']\n",
    "        filled_sentence = result['sequence']\n",
    "        \n",
    "        print(f\"   {j}. '{token}' (score: {score:.3f})\")\n",
    "        print(f\"      ‚Üí '{filled_sentence}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summarization Pipeline\n",
    "\n",
    "Create concise summaries of long texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization Pipeline\n",
    "print(\" Summarization Pipeline\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", device=device)\n",
    "\n",
    "# Long article about AI\n",
    "long_article = \"\"\"\n",
    "Artificial Intelligence (AI) has become one of the most transformative technologies of the 21st century, \n",
    "fundamentally changing how we work, communicate, and solve complex problems. The field of AI encompasses \n",
    "machine learning, deep learning, natural language processing, computer vision, and robotics. \n",
    "\n",
    "Machine learning, a subset of AI, enables computers to learn and improve from experience without being \n",
    "explicitly programmed. Deep learning, which uses neural networks with multiple layers, has been \n",
    "particularly successful in tasks such as image recognition, speech recognition, and language translation.\n",
    "\n",
    "Natural language processing (NLP) has made significant strides, with models like GPT and BERT \n",
    "demonstrating remarkable capabilities in understanding and generating human language. These advances \n",
    "have led to applications such as chatbots, virtual assistants, automated translation services, and \n",
    "content generation tools.\n",
    "\n",
    "Computer vision technology has enabled machines to interpret and understand visual information, \n",
    "leading to applications in autonomous vehicles, medical imaging, security systems, and quality \n",
    "control in manufacturing.\n",
    "\n",
    "The impact of AI extends across various industries including healthcare, finance, education, \n",
    "entertainment, and transportation. In healthcare, AI assists in drug discovery, medical diagnosis, \n",
    "and personalized treatment plans. In finance, it helps with fraud detection, algorithmic trading, \n",
    "and risk assessment.\n",
    "\n",
    "However, the rapid advancement of AI also raises important ethical considerations and challenges. \n",
    "Issues such as bias in AI systems, privacy concerns, job displacement, and the need for AI governance \n",
    "and regulation are becoming increasingly important topics of discussion among researchers, policymakers, \n",
    "and society at large.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Original article length: {len(long_article)} characters\")\n",
    "print(f\"Word count: approximately {len(long_article.split())} words\")\n",
    "print()\n",
    "\n",
    "# Generate different types of summaries\n",
    "summary_configs = [\n",
    "    {\"max_length\": 100, \"min_length\": 50, \"name\": \"Short Summary\"},\n",
    "    {\"max_length\": 150, \"min_length\": 80, \"name\": \"Medium Summary\"},\n",
    "    {\"max_length\": 200, \"min_length\": 120, \"name\": \"Detailed Summary\"}\n",
    "]\n",
    "\n",
    "for config in summary_configs:\n",
    "    print(f\"üìÑ {config['name']}:\")\n",
    "    \n",
    "    summary = summarizer(\n",
    "        long_article,\n",
    "        max_length=config['max_length'],\n",
    "        min_length=config['min_length'],\n",
    "        do_sample=False  # Use deterministic summarization\n",
    "    )\n",
    "    \n",
    "    summary_text = summary[0]['summary_text']\n",
    "    \n",
    "    print(f\"   ‚Üí {summary_text}\")\n",
    "    print(f\"   Length: {len(summary_text)} characters\")\n",
    "    print(f\"   Word count: {len(summary_text.split())} words\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
