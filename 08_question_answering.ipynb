{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with Hugging Face Transformers\n",
    "\n",
    "This notebook demonstrates how to use pre-trained models for question answering tasks using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Question Answering with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load pre-trained BERT model for question answering\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Create a question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle, \n",
    "is a moist broadleaf forest that covers most of the Amazon basin of South America. \n",
    "This basin encompasses 7,000,000 square kilometers (2,700,000 sq mi), of which \n",
    "5,500,000 square kilometers (2,100,000 sq mi) are covered by the rainforest.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How large is the Amazon basin?\"\n",
    "\n",
    "# Get answer\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer['answer']}\")\n",
    "print(f\"Confidence: {answer['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Questions on Same Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple questions about the same context\n",
    "questions = [\n",
    "    \"What is another name for the Amazon rainforest?\",\n",
    "    \"How much area does the rainforest cover?\",\n",
    "    \"What type of forest is the Amazon?\",\n",
    "    \"Which continent is the Amazon rainforest located?\"\n",
    "]\n",
    "\n",
    "print(\"Question Answering Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"{i}. Q: {question}\")\n",
    "    print(f\"   A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Answering on Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset for question answering\n",
    "custom_data = [\n",
    "    {\n",
    "        \"context\": \"Python is a high-level programming language created by Guido van Rossum. It was first released in 1991 and is known for its simple syntax and readability.\",\n",
    "        \"questions\": [\n",
    "            \"Who created Python?\",\n",
    "            \"When was Python first released?\",\n",
    "            \"What is Python known for?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every scenario.\",\n",
    "        \"questions\": [\n",
    "            \"What is machine learning?\",\n",
    "            \"What does machine learning enable computers to do?\",\n",
    "            \"Is machine learning part of artificial intelligence?\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process custom dataset\n",
    "print(\"Custom Dataset Question Answering:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, data in enumerate(custom_data, 1):\n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    print(f\"Context: {data['context'][:100]}...\")\n",
    "    print(\"\\nQuestions and Answers:\")\n",
    "    \n",
    "    for j, question in enumerate(data['questions'], 1):\n",
    "        result = qa_pipeline(question=question, context=data['context'])\n",
    "        print(f\"  {j}. Q: {question}\")\n",
    "        print(f\"     A: {result['answer']} (score: {result['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual Tokenization for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Manual approach without pipeline\n",
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question, \n",
    "        context, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get start and end positions\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    # Convert back to tokens and then to string\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer_tokens = tokens[start_index:end_index+1]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "    \n",
    "    # Calculate confidence\n",
    "    start_prob = torch.softmax(start_scores, dim=1)[0, start_index].item()\n",
    "    end_prob = torch.softmax(end_scores, dim=1)[0, end_index].item()\n",
    "    confidence = start_prob * end_prob\n",
    "    \n",
    "    return answer, confidence\n",
    "\n",
    "# Test the manual function\n",
    "question = \"What does the Amazon basin encompass?\"\n",
    "answer, confidence = answer_question(question, context, model, tokenizer)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different QA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different question answering models\n",
    "models_to_compare = [\n",
    "    \"distilbert-base-cased-distilled-squad\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"roberta-base-squad2\"\n",
    "]\n",
    "\n",
    "test_context = \"\"\"\n",
    "Artificial Intelligence (AI) is intelligence demonstrated by machines, in contrast to \n",
    "the natural intelligence displayed by humans and animals. Leading AI textbooks define \n",
    "the field as the study of \"intelligent agents\": any device that perceives its environment \n",
    "and takes actions that maximize its chance of successfully achieving its goals.\n",
    "\"\"\"\n",
    "\n",
    "test_question = \"What is artificial intelligence?\"\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    try:\n",
    "        # Create pipeline for each model\n",
    "        qa_pipe = pipeline(\"question-answering\", model=model_name)\n",
    "        result = qa_pipe(question=test_question, context=test_context)\n",
    "        \n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Confidence: {result['score']:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {str(e)}\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
