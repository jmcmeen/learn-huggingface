{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Working with Custom Datasets in Hugging Face\n\nThis notebook demonstrates how to create, load, and use custom datasets with Hugging Face Transformers and Datasets libraries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install datasets transformers torch pandas"
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Creating a Custom Dataset from Lists",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import Dataset, DatasetDict\nimport pandas as pd\n\n# Create sample data for sentiment analysis\ntexts = [\n    \"I love this movie! It's amazing.\",\n    \"This film is terrible and boring.\",\n    \"Great acting and wonderful story.\",\n    \"Not my favorite, but decent.\",\n    \"Absolutely fantastic! Highly recommend.\",\n    \"Waste of time. Very disappointing.\",\n    \"Good movie with great visuals.\",\n    \"Average film, nothing special.\"\n]\n\nlabels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\nlabel_names = [\"negative\", \"positive\"]\n\n# Create dataset from dictionary\ndataset_dict = {\n    'text': texts,\n    'label': labels\n}\n\ndataset = Dataset.from_dict(dataset_dict)\n\nprint(\"Dataset created:\")\nprint(f\"Number of examples: {len(dataset)}\")\nprint(f\"Features: {dataset.features}\")\nprint(\"\\nFirst 3 examples:\")\nfor i in range(3):\n    example = dataset[i]\n    print(f\"{i+1}. Text: {example['text']}\")\n    print(f\"   Label: {example['label']} ({label_names[example['label']]})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Loading Custom Dataset from CSV File",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom datasets import load_dataset\n\n# Create a sample CSV file\nsample_data = {\n    'text': [\n        \"The weather is beautiful today.\",\n        \"I'm feeling sad about the news.\",\n        \"This is the best day ever!\",\n        \"Traffic is really annoying.\",\n        \"I love spending time with family.\",\n        \"Work is stressful these days.\",\n        \"The sunset looks amazing tonight.\",\n        \"I'm worried about the future.\",\n        \"This book is incredibly interesting.\",\n        \"The food was disappointing.\"\n    ],\n    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n    'category': ['weather', 'news', 'personal', 'transport', 'family', 'work', 'nature', 'personal', 'books', 'food']\n}\n\n# Save to CSV\ndf = pd.DataFrame(sample_data)\ncsv_path = 'custom_sentiment_data.csv'\ndf.to_csv(csv_path, index=False)\n\n# Load dataset from CSV\ncsv_dataset = load_dataset('csv', data_files=csv_path)['train']\n\nprint(\"Dataset loaded from CSV:\")\nprint(f\"Number of examples: {len(csv_dataset)}\")\nprint(f\"Features: {csv_dataset.features}\")\nprint(\"\\nFirst 3 examples:\")\nfor i in range(3):\n    example = csv_dataset[i]\n    print(f\"{i+1}. Text: {example['text']}\")\n    print(f\"   Label: {example['label']}, Category: {example['category']}\")\n\n# Clean up\nos.remove(csv_path)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Tokenizing and Preprocessing Custom Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n\n# Apply tokenization to dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\nprint(\"Tokenized dataset:\")\nprint(f\"Features: {tokenized_dataset.features}\")\nprint(\"\\nExample tokenized data:\")\nexample = tokenized_dataset[0]\nprint(f\"Text: {dataset[0]['text']}\")\nprint(f\"Input IDs: {example['input_ids'][:10]}...\")  # Show first 10 tokens\nprint(f\"Attention Mask: {example['attention_mask'][:10]}...\")  # Show first 10 positions\nprint(f\"Label: {example['label']}\")\n\n# Decode tokens back to text to verify\ndecoded_text = tokenizer.decode(example['input_ids'])\nprint(f\"\\nDecoded tokens: {decoded_text}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Creating Train/Validation Splits",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a larger dataset for meaningful splits\nextended_texts = [\n    \"I love this product! Highly recommended.\",\n    \"Terrible quality, waste of money.\",\n    \"Amazing service and fast delivery.\",\n    \"Not worth the price, very disappointing.\",\n    \"Excellent quality and great value.\",\n    \"Poor customer service experience.\",\n    \"Beautiful design and works perfectly.\",\n    \"Broke after one week of use.\",\n    \"Outstanding performance, very satisfied.\",\n    \"Difficult to use and confusing interface.\",\n    \"Great features and easy to use.\",\n    \"Overpriced for what you get.\",\n    \"Perfect for my needs, love it!\",\n    \"Quality control issues, not recommended.\",\n    \"Impressive build quality and design.\",\n    \"Arrived damaged and took forever to ship.\",\n    \"Exceeded my expectations in every way.\",\n    \"Cheap materials and poor construction.\",\n    \"Fantastic customer support team.\",\n    \"Worst purchase I've made this year.\"\n]\n\nextended_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n\n# Create extended dataset\nextended_dataset = Dataset.from_dict({\n    'text': extended_texts,\n    'label': extended_labels\n})\n\n# Split dataset into train and validation\ntrain_test_split = extended_dataset.train_test_split(test_size=0.3, seed=42)\ntrain_dataset = train_test_split['train']\nval_dataset = train_test_split['test']\n\n# Further split validation into validation and test\nval_test_split = val_dataset.train_test_split(test_size=0.5, seed=42)\nval_dataset = val_test_split['train']\ntest_dataset = val_test_split['test']\n\n# Create DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})\n\nprint(\"Dataset splits:\")\nprint(f\"Train: {len(dataset_dict['train'])} examples\")\nprint(f\"Validation: {len(dataset_dict['validation'])} examples\")\nprint(f\"Test: {len(dataset_dict['test'])} examples\")\n\n# Apply tokenization to all splits\ntokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n\nprint(\"\\nTokenized dataset splits:\")\nfor split_name, split_dataset in tokenized_datasets.items():\n    print(f\"{split_name}: {len(split_dataset)} examples\")\n    \n# Show label distribution\nprint(\"\\nLabel distribution:\")\nfor split_name, split_dataset in dataset_dict.items():\n    labels = split_dataset['label']\n    pos_count = sum(labels)\n    neg_count = len(labels) - pos_count\n    print(f\"{split_name}: {pos_count} positive, {neg_count} negative\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Custom Dataset for Question Answering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a custom QA dataset\nqa_data = {\n    'context': [\n        \"Python is a high-level programming language created by Guido van Rossum in 1991. It emphasizes code readability and simplicity.\",\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming.\",\n        \"The Transformer architecture was introduced in 2017 and revolutionized natural language processing with its attention mechanism.\",\n        \"BERT (Bidirectional Encoder Representations from Transformers) was developed by Google and released in 2018.\",\n        \"GPT (Generative Pre-trained Transformer) models are autoregressive language models trained to predict the next word in a sequence.\"\n    ],\n    'question': [\n        \"Who created Python?\",\n        \"What is machine learning?\",\n        \"When was the Transformer architecture introduced?\",\n        \"Which company developed BERT?\",\n        \"What type of models are GPT models?\"\n    ],\n    'answer': [\n        \"Guido van Rossum\",\n        \"a subset of artificial intelligence\",\n        \"2017\",\n        \"Google\",\n        \"autoregressive language models\"\n    ]\n}\n\n# Create QA dataset\nqa_dataset = Dataset.from_dict(qa_data)\n\nprint(\"Question Answering Dataset:\")\nprint(f\"Number of examples: {len(qa_dataset)}\")\nprint(f\"Features: {qa_dataset.features}\")\n\n# Display examples\nprint(\"\\nQA Examples:\")\nfor i in range(len(qa_dataset)):\n    example = qa_dataset[i]\n    print(f\"\\n{i+1}. Context: {example['context'][:60]}...\")\n    print(f\"   Question: {example['question']}\")\n    print(f\"   Answer: {example['answer']}\")\n\n# Tokenize for QA model\ndef tokenize_qa(examples):\n    return tokenizer(\n        examples['question'],\n        examples['context'],\n        truncation=True,\n        padding=True,\n        max_length=256\n    )\n\n# Apply tokenization\ntokenized_qa_dataset = qa_dataset.map(tokenize_qa, batched=True)\n\nprint(\"\\nTokenized QA Dataset:\")\nprint(f\"Features: {tokenized_qa_dataset.features}\")\n\n# Show tokenized example\nexample = tokenized_qa_dataset[0]\nprint(f\"\\nTokenized example:\")\nprint(f\"Input IDs shape: {len(example['input_ids'])}\")\nprint(f\"Decoded input: {tokenizer.decode(example['input_ids'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}