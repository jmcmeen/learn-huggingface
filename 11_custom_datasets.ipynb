{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Working with Custom Datasets in Hugging Face\n\nThis notebook demonstrates how to create, load, and use custom datasets with Hugging Face Transformers and Datasets libraries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets transformers torch pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Creating a Custom Dataset from Lists"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data for sentiment analysis\n",
    "texts = [\n",
    "    \"I love this movie! It's amazing.\",\n",
    "    \"This film is terrible and boring.\",\n",
    "    \"Great acting and wonderful story.\",\n",
    "    \"Not my favorite, but decent.\",\n",
    "    \"Absolutely fantastic! Highly recommend.\",\n",
    "    \"Waste of time. Very disappointing.\",\n",
    "    \"Good movie with great visuals.\",\n",
    "    \"Average film, nothing special.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "\n",
    "# Create dataset from dictionary\n",
    "dataset_dict = {\n",
    "    'text': texts,\n",
    "    'label': labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(f\"Number of examples: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(3):\n",
    "    example = dataset[i]\n",
    "    print(f\"{i+1}. Text: {example['text']}\")\n",
    "    print(f\"   Label: {example['label']} ({label_names[example['label']]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Loading Custom Dataset from CSV File"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create a sample CSV file\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"I'm feeling sad about the news.\",\n",
    "        \"This is the best day ever!\",\n",
    "        \"Traffic is really annoying.\",\n",
    "        \"I love spending time with family.\",\n",
    "        \"Work is stressful these days.\",\n",
    "        \"The sunset looks amazing tonight.\",\n",
    "        \"I'm worried about the future.\",\n",
    "        \"This book is incredibly interesting.\",\n",
    "        \"The food was disappointing.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
    "    'category': ['weather', 'news', 'personal', 'transport', 'family', 'work', 'nature', 'personal', 'books', 'food']\n",
    "}\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(sample_data)\n",
    "csv_path = 'custom_sentiment_data.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Load dataset from CSV\n",
    "csv_dataset = load_dataset('csv', data_files=csv_path)['train']\n",
    "\n",
    "print(\"Dataset loaded from CSV:\")\n",
    "print(f\"Number of examples: {len(csv_dataset)}\")\n",
    "print(f\"Features: {csv_dataset.features}\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(3):\n",
    "    example = csv_dataset[i]\n",
    "    print(f\"{i+1}. Text: {example['text']}\")\n",
    "    print(f\"   Label: {example['label']}, Category: {example['category']}\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Tokenizing and Preprocessing Custom Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Apply tokenization to dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenized dataset:\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")\n",
    "print(\"\\nExample tokenized data:\")\n",
    "example = tokenized_dataset[0]\n",
    "print(f\"Text: {dataset[0]['text']}\")\n",
    "print(f\"Input IDs: {example['input_ids'][:10]}...\")  # Show first 10 tokens\n",
    "print(f\"Attention Mask: {example['attention_mask'][:10]}...\")  # Show first 10 positions\n",
    "print(f\"Label: {example['label']}\")\n",
    "\n",
    "# Decode tokens back to text to verify\n",
    "decoded_text = tokenizer.decode(example['input_ids'])\n",
    "print(f\"\\nDecoded tokens: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Creating Train/Validation Splits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for meaningful splits\n",
    "extended_texts = [\n",
    "    \"I love this product! Highly recommended.\",\n",
    "    \"Terrible quality, waste of money.\",\n",
    "    \"Amazing service and fast delivery.\",\n",
    "    \"Not worth the price, very disappointing.\",\n",
    "    \"Excellent quality and great value.\",\n",
    "    \"Poor customer service experience.\",\n",
    "    \"Beautiful design and works perfectly.\",\n",
    "    \"Broke after one week of use.\",\n",
    "    \"Outstanding performance, very satisfied.\",\n",
    "    \"Difficult to use and confusing interface.\",\n",
    "    \"Great features and easy to use.\",\n",
    "    \"Overpriced for what you get.\",\n",
    "    \"Perfect for my needs, love it!\",\n",
    "    \"Quality control issues, not recommended.\",\n",
    "    \"Impressive build quality and design.\",\n",
    "    \"Arrived damaged and took forever to ship.\",\n",
    "    \"Exceeded my expectations in every way.\",\n",
    "    \"Cheap materials and poor construction.\",\n",
    "    \"Fantastic customer support team.\",\n",
    "    \"Worst purchase I've made this year.\"\n",
    "]\n",
    "\n",
    "extended_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Create extended dataset\n",
    "extended_dataset = Dataset.from_dict({\n",
    "    'text': extended_texts,\n",
    "    'label': extended_labels\n",
    "})\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_test_split = extended_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']\n",
    "\n",
    "# Further split validation into validation and test\n",
    "val_test_split = val_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train: {len(dataset_dict['train'])} examples\")\n",
    "print(f\"Validation: {len(dataset_dict['validation'])} examples\")\n",
    "print(f\"Test: {len(dataset_dict['test'])} examples\")\n",
    "\n",
    "# Apply tokenization to all splits\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized dataset splits:\")\n",
    "for split_name, split_dataset in tokenized_datasets.items():\n",
    "    print(f\"{split_name}: {len(split_dataset)} examples\")\n",
    "    \n",
    "# Show label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "for split_name, split_dataset in dataset_dict.items():\n",
    "    labels = split_dataset['label']\n",
    "    pos_count = sum(labels)\n",
    "    neg_count = len(labels) - pos_count\n",
    "    print(f\"{split_name}: {pos_count} positive, {neg_count} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Custom Dataset for Question Answering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom QA dataset\n",
    "qa_data = {\n",
    "    'context': [\n",
    "        \"Python is a high-level programming language created by Guido van Rossum in 1991. It emphasizes code readability and simplicity.\",\n",
    "        \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without explicit programming.\",\n",
    "        \"The Transformer architecture was introduced in 2017 and revolutionized natural language processing with its attention mechanism.\",\n",
    "        \"BERT (Bidirectional Encoder Representations from Transformers) was developed by Google and released in 2018.\",\n",
    "        \"GPT (Generative Pre-trained Transformer) models are autoregressive language models trained to predict the next word in a sequence.\"\n",
    "    ],\n",
    "    'question': [\n",
    "        \"Who created Python?\",\n",
    "        \"What is machine learning?\",\n",
    "        \"When was the Transformer architecture introduced?\",\n",
    "        \"Which company developed BERT?\",\n",
    "        \"What type of models are GPT models?\"\n",
    "    ],\n",
    "    'answer': [\n",
    "        \"Guido van Rossum\",\n",
    "        \"a subset of artificial intelligence\",\n",
    "        \"2017\",\n",
    "        \"Google\",\n",
    "        \"autoregressive language models\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create QA dataset\n",
    "qa_dataset = Dataset.from_dict(qa_data)\n",
    "\n",
    "print(\"Question Answering Dataset:\")\n",
    "print(f\"Number of examples: {len(qa_dataset)}\")\n",
    "print(f\"Features: {qa_dataset.features}\")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nQA Examples:\")\n",
    "for i in range(len(qa_dataset)):\n",
    "    example = qa_dataset[i]\n",
    "    print(f\"\\n{i+1}. Context: {example['context'][:60]}...\")\n",
    "    print(f\"   Question: {example['question']}\")\n",
    "    print(f\"   Answer: {example['answer']}\")\n",
    "\n",
    "# Tokenize for QA model\n",
    "def tokenize_qa(examples):\n",
    "    return tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_qa_dataset = qa_dataset.map(tokenize_qa, batched=True)\n",
    "\n",
    "print(\"\\nTokenized QA Dataset:\")\n",
    "print(f\"Features: {tokenized_qa_dataset.features}\")\n",
    "\n",
    "# Show tokenized example\n",
    "example = tokenized_qa_dataset[0]\n",
    "print(\"\\nTokenized example:\")\n",
    "print(f\"Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"Decoded input: {tokenizer.decode(example['input_ids'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}