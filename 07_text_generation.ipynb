{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Multiple prompts for batch processing\nprompts = [\n    \"The benefits of renewable energy include\",\n    \"Space exploration has revealed that\",\n    \"The future of work will be shaped by\",\n    \"Climate change solutions require\"\n]\n\nprint(\"Batch Text Generation Results:\\n\")\n\n# Generate responses for all prompts\nbatch_results = []\nfor prompt in prompts:\n    result = generator(\n        prompt,\n        max_length=80,\n        temperature=0.7,\n        do_sample=True,\n        num_return_sequences=1,\n        pad_token_id=generator.tokenizer.eos_token_id\n    )\n    batch_results.append(result[0]['generated_text'])\n\n# Display results\nfor i, (prompt, result) in enumerate(zip(prompts, batch_results), 1):\n    print(f\"Prompt {i}: {prompt}\")\n    print(f\"Generated: {result}\")\n    print(\"=\" * 70)\n\nprint(\"\\nüéâ Text generation examples completed!\")\nprint(\"Try experimenting with different prompts and parameters!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 5: Batch Text Generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def generate_contextual_response(context, question, max_length=150):\n    \"\"\"Generate contextual responses based on given context\"\"\"\n    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n    \n    result = generator(\n        prompt,\n        max_length=max_length,\n        temperature=0.6,\n        do_sample=True,\n        pad_token_id=generator.tokenizer.eos_token_id\n    )\n    \n    return result[0]['generated_text']\n\n# Test different contexts\ncontexts = [\n    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\",\n    \"Climate change refers to long-term shifts in global temperatures and weather patterns.\",\n    \"Renewable energy comes from natural sources that replenish themselves over time.\"\n]\n\nquestions = [\n    \"How does it work?\",\n    \"What are the main causes?\", \n    \"What are the benefits?\"\n]\n\nprint(\"Contextual Text Generation Examples:\\n\")\nfor context, question in zip(contexts, questions):\n    response = generate_contextual_response(context, question)\n    print(f\"Context: {context[:50]}...\")\n    print(f\"Question: {question}\")\n    print(f\"Generated Answer: {response.split('Answer:')[-1].strip()}\")\n    print(\"-\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 4: Conditional Text Generation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load model and tokenizer separately for more control\nmodel_name = \"gpt2-medium\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Set padding token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Generate with top-k and top-p sampling\nprompt = \"Python programming is\"\ninputs = tokenizer.encode(prompt, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model.generate(\n        inputs,\n        max_length=100,\n        num_return_sequences=2,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.9,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\nprint(\"Generated texts with top-k and top-p sampling:\")\nfor i, output in enumerate(outputs):\n    text = tokenizer.decode(output, skip_special_tokens=True)\n    print(f\"\\nGeneration {i+1}:\")\n    print(text)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 3: Manual Model Usage with Advanced Control",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Different generation strategies\nprompt = \"Once upon a time in a magical forest,\"\n\n# Creative generation (high temperature)\ncreative = generator(\n    prompt,\n    max_length=120,\n    temperature=0.8,\n    do_sample=True,\n    pad_token_id=generator.tokenizer.eos_token_id\n)\n\nprint(\"Creative Generation (temperature=0.8):\")\nprint(creative[0]['generated_text'])\nprint(\"\\n\" + \"=\" * 60 + \"\\n\")\n\n# Conservative generation (low temperature)  \nconservative = generator(\n    prompt,\n    max_length=120,\n    temperature=0.3,\n    do_sample=True,\n    pad_token_id=generator.tokenizer.eos_token_id\n)\n\nprint(\"Conservative Generation (temperature=0.3):\")\nprint(conservative[0]['generated_text'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 2: Controlling Generation Parameters",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a text generation pipeline using GPT-2\ngenerator = pipeline('text-generation', model='gpt2')\n\n# Generate text from a prompt\nprompt = \"The future of artificial intelligence is\"\nresult = generator(prompt, max_length=100, num_return_sequences=2)\n\nfor i, text in enumerate(result):\n    print(f\"Generation {i+1}:\")\n    print(text['generated_text'])\n    print(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Hugging Face ‚úçÔ∏è\n",
    "\n",
    "Text generation is one of the most exciting applications of modern NLP. From creative writing to code generation, chatbots to content creation, generative models are transforming how we interact with AI.\n",
    "\n",
    "## What is Text Generation?\n",
    "\n",
    "**Text Generation** produces human-like text:\n",
    "- **Input**: Prompt or seed text\n",
    "- **Output**: Coherent continuation or completion\n",
    "- **Examples**: Story writing, code completion, dialogue systems\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Use different generation strategies (greedy, beam search, sampling)\n",
    "2. Control generation with parameters (temperature, top-k, top-p)\n",
    "3. Fine-tune models for specific generation tasks\n",
    "4. Handle different types of generation (completion, chat, code)\n",
    "5. Evaluate generated text quality\n",
    "6. Build practical generation applications\n",
    "\n",
    "Let's start generating! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import essential libraries\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\nprint('Text generation notebook ready!')"
  },
  {
   "cell_type": "markdown",
   "source": "## Example 1: Basic Text Generation with Pipeline",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}