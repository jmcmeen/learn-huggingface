{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Hugging Face\n",
    "\n",
    "Text generation is one of the most exciting applications of modern NLP. From creative writing to code generation, chatbots to content creation, generative models are transforming how we interact with AI.\n",
    "\n",
    "## What is Text Generation?\n",
    "\n",
    "**Text Generation** produces human-like text:\n",
    "- **Input**: Prompt or seed text\n",
    "- **Output**: Coherent continuation or completion\n",
    "- **Examples**: Story writing, code completion, dialogue systems\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Use different generation strategies (greedy, beam search, sampling)\n",
    "2. Control generation with parameters (temperature, top-k, top-p)\n",
    "3. Fine-tune models for specific generation tasks\n",
    "4. Handle different types of generation (completion, chat, code)\n",
    "5. Evaluate generated text quality\n",
    "6. Build practical generation applications\n",
    "\n",
    "Let's start generating! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\learn-huggingface\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Text Generation with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Text Generation Results:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: The benefits of renewable energy include\n",
      "Generated: The benefits of renewable energy include greater efficiency, lower costs of maintenance, and greater access to clean water.\n",
      "\n",
      "The US Department of Energy recently announced that it will spend $5.3 billion to develop a new coal-fired power plant in California, a project that will produce more than 5 gigawatts of power by 2020. That will generate enough electricity to power 1.2 million homes, about 1 percent of the nation's total population.\n",
      "\n",
      "The US Department of Energy also announced in June that it will be developing a new renewable energy plant in Kentucky in the coming years. It will produce enough electricity to power 1.2 million homes, about 1 percent of the nation's total population.\n",
      "\n",
      "The US Department of Energy has worked to create a clean energy economy in the U.S., and it is working with industry to develop and implement a cleaner energy economy. This is an important step toward making it easier for Americans to cut carbon emissions.\n",
      "\n",
      "The US can use this opportunity to make better decisions about the future of our energy.\n",
      "\n",
      "The United States has the best energy security in the world.\n",
      "\n",
      "This is the case for all of us. Together.\n",
      "\n",
      "Here's why.\n",
      "\n",
      "Clean energy is the most efficient energy in the world.\n",
      "\n",
      "Energy security is\n",
      "======================================================================\n",
      "Prompt 2: Space exploration has revealed that\n",
      "Generated: Space exploration has revealed that NASA's Jet Propulsion Laboratory (JPL) is building a highly efficient, 3D-printed rocket and capsule that's the size of a human hair.\n",
      "\n",
      "The new rocket will carry several components to a new, new orbit. It will be the largest rocket ever built.\n",
      "\n",
      "The spacecraft, which will be developed at JPL from an existing JPL-built LAB-2 rocket and a 3D-printed prototype, is called the Proton and Proton-2. The first Proton-2 is to launch from a launchpad in the Mojave Desert in July 2016, and is expected to be able to conduct several payloads, including a Dragon capsule for a mission to the International Space Station.\n",
      "\n",
      "NASA's Jet Propulsion Laboratory is also working on a second Proton-2 that will be able to carry a crew to the International Space Station. The Space Launch System (SLS) is the largest rocket ever built and will be able to carry two astronauts to the International Space Station.\n",
      "\n",
      "The Proton-2 is not the only new rocket in development at JPL. The agency is also working on a second Proton-2 that will be able to carry two astronauts to a new orbit. The Proton-2\n",
      "======================================================================\n",
      "Prompt 3: The future of work will be shaped by\n",
      "Generated: The future of work will be shaped by the impact of these findings on the development of new technologies.\"\n",
      "\n",
      "The study is the first to focus on the impact of low-energy particle physics on people working in the UK, particularly those who do not work in the UK, or who are at risk of serious illnesses.\n",
      "\n",
      "The study, published today in the journal Nature, aims to identify the key factors that can cause serious health problems in people who work in the UK, and what they could be doing to reduce their health risks by reducing their exposure to low-energy particles.\n",
      "\n",
      "This research is funded by the National Cancer Institute and is part of a Â£3.6m programme to build up research into the effects of low-energy particle physics and other energy-related health issues.\n",
      "\n",
      "The study says: \"Work by the National Cancer Institute and the National Institute of Economic and Social Research (NIESR) has found that the presence of low-energy particle physics in high-energy environments is associated with more serious health problems. This exposure is likely to be a high risk factor for a host of health outcomes including respiratory diseases, cardiovascular disease, cancer and type 2 diabetes.\"\n",
      "\n",
      "This research was funded by the National Institute of Economic and Social Research.\n",
      "======================================================================\n",
      "Prompt 4: Climate change solutions require\n",
      "Generated: Climate change solutions require the use of a range of new technologies to mitigate the risks, and to be able to manage them.\n",
      "\n",
      "In the context of climate change, there is a great deal the public and media can do to understand the causes and effects of climate change. The public has to understand that there is a need to reduce greenhouse gas emissions, that there are risks with the use of renewable energy, that there are opportunities for public health and safety, and that there are opportunities for economic growth.\n",
      "\n",
      "Climate change is not a political issue. It is a human-caused problem and that has to be worked out. We have to make sure we have the technology to address the risks. If we start looking at our own energy system, we'll be able to address climate change, but we will have to make sure we do that to the extent that we can in the future.\n",
      "\n",
      "The role we play in ensuring that we're not taking our country's energy system into the future will be to make sure that we're not taking our government's energy system and putting it in a negative financial position.\n",
      "\n",
      "In fact, there are a number of initiatives that are underway. One of them is the Carbon Tracker initiative, which I'm going to describe shortly here. Carbon Tracker\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ‰ Text generation examples completed!\n",
      "Try experimenting with different prompts and parameters!\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "\n",
    "# Multiple prompts for batch processing\n",
    "prompts = [\n",
    "    \"The benefits of renewable energy include\",\n",
    "    \"Space exploration has revealed that\",\n",
    "    \"The future of work will be shaped by\",\n",
    "    \"Climate change solutions require\"\n",
    "]\n",
    "\n",
    "print(\"Batch Text Generation Results:\\n\")\n",
    "\n",
    "# Generate responses for all prompts\n",
    "batch_results = []\n",
    "for prompt in prompts:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=80,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    batch_results.append(result[0]['generated_text'])\n",
    "\n",
    "# Display results\n",
    "for i, (prompt, result) in enumerate(zip(prompts, batch_results), 1):\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print(f\"Generated: {result}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Text generation examples completed!\")\n",
    "print(\"Try experimenting with different prompts and parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Controlling Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1:\n",
      "The future of artificial intelligence is rapidly approaching. If you're an AI that needs to learn how to learn about the world around you, then you're probably pretty good at it right now. It's the same as it's always been, but it needs to learn new things.\n",
      "\n",
      "In my opinion, the future of artificial intelligence is rapidly approaching. If you're an AI that needs to learn how to learn about the world around you, then you're probably pretty good at it right now. It's the same as it's always been, but it needs to learn new things. The world is so vastly different now than it was 2 years ago. You have the internet, you have internet connectivity, you have a global transportation network, you have a lot of things that are completely different in every single way. There is no need to learn that much. The world is so vastly different now than it was 2 years ago. You have the internet, you have internet connectivity, you have a global transportation network, you have a lot of things that are completely different in every single way. There is no need to learn that much. The world is so vastly different now than it was 2 years ago. You have the internet, you have internet connectivity, you have a global transportation network, you have a\n",
      "--------------------------------------------------\n",
      "Generation 2:\n",
      "The future of artificial intelligence is in doubt. It is hard to imagine an economy without artificial intelligence. The question is \"How far will we go from artificial intelligence?\"\n",
      "\n",
      "\"I think we're going to have to wait a long time,\" said Mike Gorman, an analyst with the National Center for Advancing AI. But Gorman and his colleagues have a number of questions: How far will we go from artificial intelligence to superintelligence, or will we need it to do all of the hard work of building computers and computers for us?\n",
      "\n",
      "What kind of machine would be able to do all of the hard research? Will it be able to understand problems in an environment where we can't do that? Will it be able to solve a problem that we didn't have to deal with before?\n",
      "\n",
      "The machine has to be able to predict and solve problems that we don't know of, and it has to be able to solve problems that we don't have or don't want. And those are all kinds of questions. And it's hard to know in advance what's going to happen, to determine exactly what the next step will be.\n",
      "\n",
      "\"We're making the same kind of decisions that we're making now, with this new kind of information,\" said Gorman. \"We\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a text generation pipeline using GPT-2\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate text from a prompt\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "result = generator(prompt, max_length=100, num_return_sequences=2)\n",
    "\n",
    "for i, text in enumerate(result):\n",
    "    print(f\"Generation {i+1}:\")\n",
    "    print(text['generated_text'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Manual Model Usage with Advanced Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Generation (temperature=0.8):\n",
      "Once upon a time in a magical forest, the Shadow of the Forest dwellers in this region discovered, the world had a very different future. In the year 646, a mysterious power, known as the Shadow of the Forest, had begun to awaken in the minds of children. From this moment on, the Shadow of the Forest remained the same in the minds of the children.\n",
      "\n",
      "The children of Light, who lived in the Shadow of the Forest, became very good at fighting and winning against darkness and darkness. The Shadow of the Forest then became known as the Shadow of the Land.\n",
      "\n",
      "In the years 649, the shadows of the Forest grew. From these dark shadows, the children of Light began to become strong. They fought by battle and they lost. In 654, they finally conquered the Shadow of the Forest. The children of Light became powerful in the Dark Realms and they were able to defeat the Shadow of the Forest and defeat the Shadow of Light.\n",
      "\n",
      "The Shadow of the Forest became a powerful and powerful clan. The Shadow of the Forest's clan, the Shadow of the Forest Council, was formed by the children of Light. The Shadow of the Forest Council was composed of nine Light clans. The twelve children of Light were the leaders of the Shadow of the Forest. The Shadow\n",
      "\n",
      "============================================================\n",
      "\n",
      "Conservative Generation (temperature=0.3):\n",
      "Once upon a time in a magical forest, a young girl named Yuna was born.\n",
      "\n",
      "She was a girl who had been raised by the magical forest. The magical forest was a place where the magic of the humans was not practiced, and she was not a normal girl.\n",
      "\n",
      "The magic of the humans was practiced by the humans, and the humans were not happy with the magic of the magic of the magic of the humans.\n",
      "\n",
      "Yuna was born to a magical forest.\n",
      "\n",
      "The magic of the humans was practiced by the humans, and the humans were not happy with the magic of the magic of the magic of the humans.\n",
      "\n",
      "Yuna was born to a magical forest.\n",
      "\n",
      "The magic of the humans was practiced by the humans, and the humans were not happy with the magic of the magic of the magic of the humans.\n",
      "\n",
      "Yuna was born to a magical forest.\n",
      "\n",
      "The magic of the humans was practiced by the humans, and the humans were not happy with the magic of the magic of the magic of the humans.\n",
      "\n",
      "Yuna was born to a magical forest.\n",
      "\n",
      "The magic of the humans was practiced by the humans, and the humans were not happy with the magic of the magic of the magic of the humans.\n",
      "\n",
      "Yuna\n"
     ]
    }
   ],
   "source": [
    "# Different generation strategies\n",
    "prompt = \"Once upon a time in a magical forest,\"\n",
    "\n",
    "# Creative generation (high temperature)\n",
    "creative = generator(\n",
    "    prompt,\n",
    "    max_length=120,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"Creative Generation (temperature=0.8):\")\n",
    "print(creative[0]['generated_text'])\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Conservative generation (low temperature)  \n",
    "conservative = generator(\n",
    "    prompt,\n",
    "    max_length=120,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"Conservative Generation (temperature=0.3):\")\n",
    "print(conservative[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Conditional Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\learn-huggingface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\johnm\\.cache\\huggingface\\hub\\models--gpt2-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated texts with top-k and top-p sampling:\n",
      "\n",
      "Generation 1:\n",
      "Python programming is just a matter of writing and reading code.\n",
      "\n",
      "It is not as if you can write a program that does nothing but read a file and make it readable. You can't, and you can't make it do anything.\n",
      "\n",
      "That is, until now.\n",
      "\n",
      "The first step is to figure out how to make your code readable.\n",
      "\n",
      "I am going to assume that you already know what a file is and where it lives in your computer's memory.\n",
      "\n",
      "Generation 2:\n",
      "Python programming is not for everyone. It has its strengths and weaknesses, but there are certain techniques that will make you a better programmer.\n",
      "\n",
      "If you're like me, you've been using Ruby for years, and you know how to use it to solve your problems. If you've been programming for a long time, you probably have some basic programming knowledge. If you've been programming for just a few months, you probably know some of the basics, and you probably don't need to know\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer separately for more control\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate with top-k and top-p sampling\n",
    "prompt = \"Python programming is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print(\"Generated texts with top-k and top-p sampling:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"\\nGeneration {i+1}:\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Batch Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Text Generation Examples:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Machine learning is a subset of artificial intelli...\n",
      "Question: How does it work?\n",
      "Generated Answer: Machine learning is a subset of artificial intelligence that enables computers to learn from data.\n",
      "\n",
      "Question: How does it work\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Climate change refers to long-term shifts in globa...\n",
      "Question: What are the main causes?\n",
      "Generated Answer: The main contributors of the observed changes in climate are the anthropogenic emissions of greenhouse gases, and the global warming caused by human activities.\n",
      "\n",
      "Question: What are the main causes of the observed changes\n",
      "--------------------------------------------------------------------------------\n",
      "Context: Renewable energy comes from natural sources that r...\n",
      "Question: What are the benefits?\n",
      "Generated Answer: The benefits are obvious. But the question is whether or not we should consider energy as a resource.\n",
      "\n",
      "Question: What\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_contextual_response(context, question, max_length=150):\n",
    "    \"\"\"Generate contextual responses based on given context\"\"\"\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        temperature=0.6,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Test different contexts\n",
    "contexts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\",\n",
    "    \"Climate change refers to long-term shifts in global temperatures and weather patterns.\",\n",
    "    \"Renewable energy comes from natural sources that replenish themselves over time.\"\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"How does it work?\",\n",
    "    \"What are the main causes?\", \n",
    "    \"What are the benefits?\"\n",
    "]\n",
    "\n",
    "print(\"Contextual Text Generation Examples:\\n\")\n",
    "for context, question in zip(contexts, questions):\n",
    "    response = generate_contextual_response(context, question)\n",
    "    print(f\"Context: {context[:50]}...\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {response.split('Answer:')[-1].strip()}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
