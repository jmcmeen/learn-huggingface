{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Tokenization \n",
    "\n",
    "Tokenization is the process of converting text into tokens that machine learning models can understand. It's a fundamental step in NLP that bridges the gap between human language and numerical representations.\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "**Tokenization** breaks down text into smaller units called **tokens**. These tokens are then converted to numerical IDs that models can process.\n",
    "\n",
    "```\n",
    "\"Hello world!\" → [\"Hello\", \"world\", \"!\"] → [7592, 2088, 999]\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "1. Different types of tokenization approaches\n",
    "2. How to use Hugging Face tokenizers\n",
    "3. Special tokens and their purposes\n",
    "4. Handling different text scenarios\n",
    "5. Comparing tokenizer performance\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of Tokenization\n",
    "\n",
    "Let's explore different tokenization approaches by comparing how they handle the same text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text to tokenize\n",
    "sample_text = \"Hugging Face revolutionizes natural language processing!\"\n",
    "\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "print(f\"Text length: {len(sample_text)} characters\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Word-level tokenization (simple split)\n",
    "word_tokens = sample_text.split()\n",
    "print(f\"Word tokenization: {word_tokens}\")\n",
    "print(f\"Number of tokens: {len(word_tokens)}\")\n",
    "print()\n",
    "\n",
    "# Character-level tokenization\n",
    "char_tokens = list(sample_text)\n",
    "print(f\"Character tokenization: {char_tokens}\")\n",
    "print(f\"Number of tokens: {len(char_tokens)}\")\n",
    "print()\n",
    "\n",
    "# Subword tokenization (we'll see this with actual tokenizers below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Tokenizers\n",
    "\n",
    "Let's load different tokenizers and see how they handle the same text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers\n",
    "tokenizers = {\n",
    "    'BERT (WordPiece)': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'GPT-2 (BPE)': AutoTokenizer.from_pretrained('gpt2'),\n",
    "    'T5 (SentencePiece)': AutoTokenizer.from_pretrained('t5-small'),\n",
    "    'DistilBERT': AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "}\n",
    "\n",
    "# Compare tokenization across different models\n",
    "test_sentence = \"The quick-thinking AI researcher's breakthrough was unprecedented!\"\n",
    "\n",
    "print(f\"Input: '{test_sentence}'\\n\")\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(test_sentence)\n",
    "    token_ids = tokenizer.encode(test_sentence, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Count: {len(tokens)} tokens\")\n",
    "    print(f\"  IDs: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Special Tokens\n",
    "\n",
    "Special tokens have specific meanings and help models understand text structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine BERT's special tokens\n",
    "bert_tokenizer = tokenizers['BERT (WordPiece)']\n",
    "\n",
    "print(\"=== BERT Special Tokens ===\")\n",
    "special_tokens = {\n",
    "    'CLS Token': bert_tokenizer.cls_token,\n",
    "    'SEP Token': bert_tokenizer.sep_token, \n",
    "    'PAD Token': bert_tokenizer.pad_token,\n",
    "    'UNK Token': bert_tokenizer.unk_token,\n",
    "    'MASK Token': bert_tokenizer.mask_token\n",
    "}\n",
    "\n",
    "for name, token in special_tokens.items():\n",
    "    token_id = bert_tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"{name}: '{token}' (ID: {token_id})\")\n",
    "\n",
    "print(\"\\n=== What they do ===\")\n",
    "print(\"[CLS]: Classification token - placed at the beginning\")\n",
    "print(\"[SEP]: Separator token - separates sentences\")\n",
    "print(\"[PAD]: Padding token - fills sequences to equal length\")\n",
    "print(\"[UNK]: Unknown token - replaces out-of-vocabulary words\")\n",
    "print(\"[MASK]: Mask token - used for masked language modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See special tokens in action\n",
    "sentence1 = \"I love machine learning.\"\n",
    "sentence2 = \"It's fascinating and powerful.\"\n",
    "\n",
    "# Tokenize with and without special tokens\n",
    "tokens_without = bert_tokenizer.tokenize(sentence1 + \" \" + sentence2)\n",
    "tokens_with = bert_tokenizer.tokenize(sentence1, sentence2, add_special_tokens=True)\n",
    "\n",
    "# Using encode for proper special token handling\n",
    "encoded = bert_tokenizer.encode(sentence1, sentence2, add_special_tokens=True)\n",
    "tokens_from_ids = bert_tokenizer.convert_ids_to_tokens(encoded)\n",
    "\n",
    "print(\"Two sentences:\")\n",
    "print(f\"Sentence 1: '{sentence1}'\")\n",
    "print(f\"Sentence 2: '{sentence2}'\")\n",
    "print()\n",
    "\n",
    "print(\"Without special tokens:\")\n",
    "print(f\"Tokens: {tokens_without}\")\n",
    "print()\n",
    "\n",
    "print(\"With special tokens:\")\n",
    "print(f\"Tokens: {tokens_from_ids}\")\n",
    "print(f\"Token IDs: {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding and Decoding\n",
    "\n",
    "Let's understand the full tokenization pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete tokenization pipeline\n",
    "text = \"Tokenization is the first step in NLP preprocessing!\"\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Text → Tokens\n",
    "tokens = bert_tokenizer.tokenize(text)\n",
    "print(f\"1. Tokenize: {tokens}\")\n",
    "\n",
    "# Step 2: Tokens → IDs\n",
    "token_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"2. Convert to IDs: {token_ids}\")\n",
    "\n",
    "# Step 3: IDs → Tokens\n",
    "tokens_back = bert_tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f\"3. Convert back to tokens: {tokens_back}\")\n",
    "\n",
    "# Step 4: Tokens → Text\n",
    "text_back = bert_tokenizer.convert_tokens_to_string(tokens_back)\n",
    "print(f\"4. Convert back to text: '{text_back}'\")\n",
    "\n",
    "# All-in-one methods\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All-in-one methods:\")\n",
    "encoded = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "decoded = bert_tokenizer.decode(encoded)\n",
    "print(f\"Encode: {encoded}\")\n",
    "print(f\"Decode: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Out-of-Vocabulary (OOV) Words\n",
    "\n",
    "Let's see how different tokenizers handle words they haven't seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with unusual/made-up words\n",
    "oov_text = \"The splendiferous AI researcher invented the wonderflabber technique!\"\n",
    "\n",
    "print(f\"Text with OOV words: '{oov_text}'\\n\")\n",
    "\n",
    "# Test different tokenizers\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(oov_text)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    \n",
    "    # Highlight potential OOV handling\n",
    "    oov_indicators = []\n",
    "    for token in tokens:\n",
    "        if '##' in token or token.startswith('▁') or token == tokenizer.unk_token:\n",
    "            oov_indicators.append(token)\n",
    "    \n",
    "    if oov_indicators:\n",
    "        print(f\"  Subword/OOV tokens: {oov_indicators}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenization with Attention Masks and Padding\n",
    "\n",
    "When processing batches, we need consistent sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different length sentences\n",
    "sentences = [\n",
    "    \"Short sentence.\",\n",
    "    \"This is a medium-length sentence with more words.\",\n",
    "    \"This is a very long sentence that contains many words and will require more tokens to represent properly.\"\n",
    "]\n",
    "\n",
    "print(\"=== Tokenizing Multiple Sentences ===\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    tokens = bert_tokenizer.tokenize(sentence)\n",
    "    print(f\"Sentence {i+1}: {len(tokens)} tokens\")\n",
    "    print(f\"  '{sentence}'\")\n",
    "    print(f\"  Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch tokenization with padding\n",
    "tokenized_batch = bert_tokenizer(\n",
    "    sentences,\n",
    "    padding=True,  # Pad to the longest sequence\n",
    "    truncation=True,  # Truncate if too long\n",
    "    max_length=128,  # Maximum sequence length\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"=== Batch Tokenization Results ===\")\n",
    "print(f\"Input IDs shape: {tokenized_batch['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {tokenized_batch['attention_mask'].shape}\")\n",
    "print()\n",
    "\n",
    "# Show the results for each sentence\n",
    "for i in range(len(sentences)):\n",
    "    input_ids = tokenized_batch['input_ids'][i]\n",
    "    attention_mask = tokenized_batch['attention_mask'][i]\n",
    "    \n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(f\"  Input IDs: {input_ids[:15]}...\")\n",
    "    print(f\"  Attention: {attention_mask[:15]}...\")\n",
    "    print(f\"  Non-padding tokens: {attention_mask.sum().item()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Tokenization\n",
    "\n",
    "Let's create some visualizations to better understand tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare token counts across different tokenizers\n",
    "test_sentences = [\n",
    "    \"Hello world!\",\n",
    "    \"Natural language processing is amazing.\",\n",
    "    \"The transformer architecture revolutionized AI.\",\n",
    "    \"Subword tokenization handles out-of-vocabulary words efficiently.\",\n",
    "    \"Machine learning models require numerical representations of text data.\"\n",
    "]\n",
    "\n",
    "# Collect token counts\n",
    "results = []\n",
    "for sentence in test_sentences:\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        results.append({\n",
    "            'Sentence': f\"Sentence {test_sentences.index(sentence) + 1}\",\n",
    "            'Tokenizer': name,\n",
    "            'Token Count': len(tokens),\n",
    "            'Text Length': len(sentence)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df, x='Sentence', y='Token Count', hue='Tokenizer')\n",
    "plt.title('Token Count Comparison Across Different Tokenizers')\n",
    "plt.xlabel('Test Sentences')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the actual sentences\n",
    "print(\"\\nTest sentences:\")\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"{i+1}. '{sentence}' ({len(sentence)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length distribution\n",
    "sample_texts = [\n",
    "    \"AI\", \"GPT\", \"BERT\", \"transformer\", \"attention\", \"mechanism\", \n",
    "    \"tokenization\", \"preprocessing\", \"subword\", \"vocabulary\",\n",
    "    \"neural\", \"network\", \"machine\", \"learning\", \"algorithm\",\n",
    "    \"unprecedented\", \"revolutionary\", \"extraordinary\", \"magnificent\"\n",
    "]\n",
    "\n",
    "tokenizer = bert_tokenizer\n",
    "token_data = []\n",
    "\n",
    "for text in sample_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_data.extend([(text, len(text), len(tokens), len(tokens)/len(text)) for _ in range(len(tokens))])\n",
    "\n",
    "token_df = pd.DataFrame(token_data, columns=['Word', 'Char_Length', 'Token_Count', 'Tokens_per_Char'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(token_df['Char_Length'], token_df['Token_Count'], alpha=0.6)\n",
    "plt.xlabel('Word Length (characters)')\n",
    "plt.ylabel('Token Count')\n",
    "plt.title('Character Length vs Token Count')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(token_df['Char_Length'], token_df['Token_Count'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(token_df['Char_Length'], p(token_df['Char_Length']), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Tokenization Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast tokenizers with offsets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a fast tokenizer\n",
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "text = \"Hugging Face provides amazing NLP tools!\"\n",
    "\n",
    "# Tokenize with offsets (character positions)\n",
    "encoding = fast_tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens = fast_tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "offsets = encoding[\"offset_mapping\"][0]\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(\"\\nTokens with character positions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for token, (start, end) in zip(tokens, offsets):\n",
    "    if start == end == 0:  # Special tokens\n",
    "        print(f\"'{token}' -> Special token\")\n",
    "    else:\n",
    "        original_text = text[start:end]\n",
    "        print(f\"'{token}' -> '{original_text}' (pos {start}-{end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with different languages\n",
    "multilingual_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "multilingual_texts = {\n",
    "    \"English\": \"Hello, how are you?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous?\",\n",
    "    \"German\": \"Hallo, wie geht es dir?\",\n",
    "    \"Japanese\": \"こんにちは、元気ですか？\",\n",
    "    \"Arabic\": \"مرحبا، كيف حالك؟\"\n",
    "}\n",
    "\n",
    "print(\"=== Multilingual Tokenization ===\")\n",
    "for language, text in multilingual_texts.items():\n",
    "    tokens = multilingual_tokenizer.tokenize(text)\n",
    "    print(f\"{language}: '{text}'\")\n",
    "    print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison\n",
    "\n",
    "Let's measure tokenization speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate test data\n",
    "long_text = \"This is a test sentence for measuring tokenization speed. \" * 1000\n",
    "test_texts = [long_text] * 100\n",
    "\n",
    "print(f\"Testing with {len(test_texts)} texts, each {len(long_text):,} characters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    for text in test_texts:\n",
    "        _ = tokenizer.tokenize(text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    performance_results.append((name, duration))\n",
    "    print(f\"{name}: {duration:.3f} seconds\")\n",
    "\n",
    "# Sort by performance\n",
    "performance_results.sort(key=lambda x: x[1])\n",
    "print(\"\\nRanked by speed (fastest first):\")\n",
    "for i, (name, duration) in enumerate(performance_results, 1):\n",
    "    print(f\"{i}. {name}: {duration:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Tips and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Tokenization Best Practices ===\")\n",
    "print()\n",
    "\n",
    "# 1. Always check sequence length\n",
    "def check_sequence_length(text, tokenizer, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    if token_count > max_length:\n",
    "        print(f\"Warning: Text has {token_count} tokens, exceeds max_length of {max_length}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Text has {token_count} tokens, within limit\")\n",
    "        return True\n",
    "\n",
    "example_text = \"This is an example text. \" * 100\n",
    "print(\"1. Checking sequence length:\")\n",
    "check_sequence_length(example_text, bert_tokenizer)\n",
    "print()\n",
    "\n",
    "# 2. Handle truncation properly\n",
    "print(\"2. Proper truncation handling:\")\n",
    "long_text = \"AI revolutionizes everything. \" * 50\n",
    "\n",
    "# Bad: Just truncate\n",
    "truncated_bad = bert_tokenizer.encode(long_text, max_length=50, truncation=True)\n",
    "\n",
    "# Good: Truncate with attention to special tokens\n",
    "truncated_good = bert_tokenizer.encode(\n",
    "    long_text,\n",
    "    max_length=50,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(f\"Bad truncation length: {len(truncated_bad)}\")\n",
    "print(f\"Good truncation length: {len(truncated_good)}\")\n",
    "print(f\"Good truncated text: '{bert_tokenizer.decode(truncated_good)}'\")\n",
    "print()\n",
    "\n",
    "# 3. Use batch processing for efficiency\n",
    "print(\"3. Batch processing example:\")\n",
    "texts_to_process = [\"Short text.\", \"Medium length text here.\", \"Much longer text that needs processing.\"]\n",
    "\n",
    "# Process in batch\n",
    "batch_encoded = bert_tokenizer(\n",
    "    texts_to_process,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"Batch shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(\"Efficient batch processing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "**What we've learned about tokenization:**\n",
    "\n",
    "✅ **Tokenization Types**: Word-level, character-level, and subword tokenization  \n",
    "✅ **Subword Algorithms**: WordPiece (BERT), BPE (GPT-2), SentencePiece (T5)  \n",
    "✅ **Special Tokens**: CLS, SEP, PAD, UNK, MASK and their purposes  \n",
    "✅ **Encoding/Decoding**: Converting text ↔ tokens ↔ IDs  \n",
    "✅ **OOV Handling**: How different tokenizers handle unknown words  \n",
    "✅ **Batch Processing**: Padding, attention masks, and truncation  \n",
    "✅ **Performance**: Speed comparisons and optimization tips  \n",
    "✅ **Multilingual**: Working with different languages  \n",
    "\n",
    "## 🔧 Best Practices Summary\n",
    "\n",
    "1. **Always check sequence lengths** before training\n",
    "2. **Use appropriate max_length** and truncation strategies\n",
    "3. **Process texts in batches** for efficiency\n",
    "4. **Choose the right tokenizer** for your model\n",
    "5. **Handle special tokens** properly for your task\n",
    "6. **Use fast tokenizers** when available for better performance\n",
    "7. **Consider multilingual tokenizers** for cross-lingual tasks\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "Now that you understand tokenization, you're ready to:\n",
    "\n",
    "1. **Move to the next notebook**: `03_model_loading.ipynb`\n",
    "2. **Experiment** with different tokenizers on your own text\n",
    "3. **Practice** handling edge cases like very long texts\n",
    "4. **Explore** tokenizer-specific features in the documentation\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "- [Tokenizers Documentation](https://huggingface.co/docs/tokenizers/)\n",
    "- [Understanding Subword Tokenization](https://huggingface.co/course/chapter6/1)\n",
    "- [Fast Tokenizers Guide](https://huggingface.co/docs/transformers/fast_tokenizers)\n",
    "- [Multilingual Models](https://huggingface.co/docs/transformers/multilingual)\n",
    "\n",
    "## 🧪 Try This Yourself\n",
    "\n",
    "**Exercise 1**: Compare how different tokenizers handle technical jargon in your field\n",
    "\n",
    "**Exercise 2**: Test tokenization on text in different languages\n",
    "\n",
    "**Exercise 3**: Measure tokenization performance on your own dataset\n",
    "\n",
    "**Exercise 4**: Experiment with custom tokenizer settings\n",
    "\n",
    "Happy tokenizing! 🔤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
