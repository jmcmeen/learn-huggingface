{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7BHPTMH9NJ3"
   },
   "source": [
    "# Introduction to Hugging Face ðŸ¤—\n",
    "\n",
    "Welcome to the world of Hugging Face! This notebook will introduce you to the basics of the Hugging Face ecosystem and show you how to get started with pre-trained models.\n",
    "\n",
    "## What is Hugging Face?\n",
    "\n",
    "Hugging Face is a platform and library that provides:\n",
    "- **Pre-trained models** for NLP, computer vision, and audio\n",
    "- **Easy-to-use APIs** for common ML tasks\n",
    "- **Community hub** for sharing models and datasets\n",
    "- **Tools** for training and deploying models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Install and import Hugging Face libraries\n",
    "2. Use pre-trained models with pipelines\n",
    "3. Understand the Hugging Face Hub\n",
    "4. Perform basic NLP tasks\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFrky9J09NJ4"
   },
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's make sure we have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:50:53.826648Z",
     "iopub.status.busy": "2025-08-27T23:50:53.826148Z",
     "iopub.status.idle": "2025-08-27T23:51:03.330225Z",
     "shell.execute_reply": "2025-08-27T23:51:03.330225Z"
    },
    "id": "5FrJbn6b9NJ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.55.4\n",
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch datasets tokenizers\n",
    "\n",
    "# Import essential libraries\n",
    "import transformers\n",
    "import torch\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYRnUaMN9NJ4"
   },
   "source": [
    "## 2. Your First Hugging Face Pipeline\n",
    "\n",
    "Pipelines are the easiest way to use pre-trained models. They handle tokenization, model inference, and post-processing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:03.356744Z",
     "iopub.status.busy": "2025-08-27T23:51:03.356744Z",
     "iopub.status.idle": "2025-08-27T23:51:10.971299Z",
     "shell.execute_reply": "2025-08-27T23:51:10.971299Z"
    },
    "id": "KSZDuVdF9NJ4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'I love learning about AI and machine learning!'\n",
      "Sentiment: POSITIVE (confidence: 1.000)\n",
      "--------------------------------------------------\n",
      "Text: 'This movie was terrible and boring.'\n",
      "Sentiment: NEGATIVE (confidence: 1.000)\n",
      "--------------------------------------------------\n",
      "Text: 'The weather is okay today.'\n",
      "Sentiment: POSITIVE (confidence: 1.000)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test it with some examples\n",
    "texts = [\n",
    "    \"I love learning about AI and machine learning!\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"The weather is okay today.\"\n",
    "]\n",
    "\n",
    "results = sentiment_pipeline(texts)\n",
    "\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZd6rziy9NJ4"
   },
   "source": [
    "## 3. Different Types of Pipelines\n",
    "\n",
    "Hugging Face supports many different tasks through pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:10.973303Z",
     "iopub.status.busy": "2025-08-27T23:51:10.972803Z",
     "iopub.status.idle": "2025-08-27T23:51:20.667260Z",
     "shell.execute_reply": "2025-08-27T23:51:20.667260Z"
    },
    "id": "6thnFagt9NJ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEXT GENERATION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence is a very exciting field.\n",
      "\n",
      "What's next for AI?\n",
      "\n",
      "A lot of the work we do for AI is still going on. But we are getting there.\n",
      "\n",
      "Why are we doing this?\n",
      "\n",
      "We are trying to understand the basic processes we need to do to detect and control things. We are getting more and more sophisticated.\n",
      "\n",
      "Why is this important?\n",
      "\n",
      "We are still much smaller than we were before the start of this.\n",
      "\n",
      "Why are there so many more advances in AI?\n",
      "\n",
      "We are still in development. It's just a matter of time before we get to that point.\n",
      "\n",
      "AI is a very important subject to study. And I think that there is a lot more that could be done.\n",
      "\n",
      "What are your priorities?\n",
      "\n",
      "We are very focused on making sure that the future of AI is as good as it could be.\n",
      "\n",
      "What challenges have you faced in the past?\n",
      "\n",
      "I have been involved in many other fields where there has been much more opportunity for AI and machine learning.\n",
      "\n",
      "Do you think machines will be able to solve all of the problems that AI currently faces?\n",
      "\n",
      "I would say the next frontier will be AI.\n",
      "\n",
      "What are your thoughts on AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "print(\"=== TEXT GENERATION ===\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\"Artificial intelligence is\", max_length=50, num_return_sequences=1)\n",
    "print(result[0]['generated_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:20.668765Z",
     "iopub.status.busy": "2025-08-27T23:51:20.668765Z",
     "iopub.status.idle": "2025-08-27T23:51:21.640569Z",
     "shell.execute_reply": "2025-08-27T23:51:21.640569Z"
    },
    "id": "1zIJC4Gu9NJ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUESTION ANSWERING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: When was Hugging Face founded?\n",
      "A: 2016 (confidence: 0.983)\n",
      "\n",
      "Q: What is the main product of Hugging Face?\n",
      "A: Transformers library (confidence: 0.790)\n",
      "\n",
      "Q: Who are the founders?\n",
      "A: ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf (confidence: 0.955)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question Answering\n",
    "print(\"=== QUESTION ANSWERING ===\")\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning.\n",
    "The company was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf.\n",
    "Their main product is the Transformers library, which provides thousands of pre-trained models.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"What is the main product of Hugging Face?\",\n",
    "    \"Who are the founders?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer['answer']} (confidence: {answer['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:21.642076Z",
     "iopub.status.busy": "2025-08-27T23:51:21.642076Z",
     "iopub.status.idle": "2025-08-27T23:51:22.156516Z",
     "shell.execute_reply": "2025-08-27T23:51:22.156516Z"
    },
    "id": "0e6XCSjW9NJ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NAMED ENTITY RECOGNITION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\n",
      "\n",
      "Entities found:\n",
      "- Apple Inc: ORG (confidence: 1.000)\n",
      "- Steve Jobs: PER (confidence: 0.989)\n",
      "- Cupertino: LOC (confidence: 0.973)\n",
      "- California: LOC (confidence: 0.999)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"=== NAMED ENTITY RECOGNITION ===\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Entities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN1aBXFV9NJ5"
   },
   "source": [
    "## 4. Exploring the Hugging Face Hub\n",
    "\n",
    "The Hugging Face Hub hosts thousands of models. Let's see how to explore and use different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:22.158019Z",
     "iopub.status.busy": "2025-08-27T23:51:22.157519Z",
     "iopub.status.idle": "2025-08-27T23:51:22.160522Z",
     "shell.execute_reply": "2025-08-27T23:51:22.160522Z"
    },
    "id": "A_FNDAeO9NJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pipeline tasks:\n",
      " 1. audio-classification\n",
      " 2. automatic-speech-recognition\n",
      " 3. text-to-audio\n",
      " 4. feature-extraction\n",
      " 5. text-classification\n",
      " 6. token-classification\n",
      " 7. question-answering\n",
      " 8. table-question-answering\n",
      " 9. visual-question-answering\n",
      "10. document-question-answering\n",
      "11. fill-mask\n",
      "12. summarization\n",
      "13. translation\n",
      "14. text2text-generation\n",
      "15. text-generation\n",
      "16. zero-shot-classification\n",
      "17. zero-shot-image-classification\n",
      "18. zero-shot-audio-classification\n",
      "19. image-classification\n",
      "20. image-feature-extraction\n",
      "21. image-segmentation\n",
      "22. image-to-text\n",
      "23. image-text-to-text\n",
      "24. object-detection\n",
      "25. zero-shot-object-detection\n",
      "26. depth-estimation\n",
      "27. video-classification\n",
      "28. mask-generation\n",
      "29. image-to-image\n"
     ]
    }
   ],
   "source": [
    "# List available pipeline tasks\n",
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "\n",
    "print(\"Available pipeline tasks:\")\n",
    "for i, task in enumerate(SUPPORTED_TASKS.keys(), 1):\n",
    "    print(f\"{i:2d}. {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:22.161524Z",
     "iopub.status.busy": "2025-08-27T23:51:22.161524Z",
     "iopub.status.idle": "2025-08-27T23:51:22.771884Z",
     "shell.execute_reply": "2025-08-27T23:51:22.771884Z"
    },
    "id": "1GP_vd049NJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USING A SPECIFIC MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Just finished my first machine learning project! ðŸš€'\n",
      "Sentiment: positive (score: 0.975)\n",
      "\n",
      "Text: 'Traffic is so bad today ðŸ˜¤'\n",
      "Sentiment: negative (score: 0.948)\n",
      "\n",
      "Text: 'Reading a good book by the fireplace'\n",
      "Sentiment: positive (score: 0.884)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a specific model from the Hub\n",
    "print(\"=== USING A SPECIFIC MODEL ===\")\n",
    "\n",
    "# Use a different sentiment analysis model\n",
    "specific_sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "tweet_texts = [\n",
    "    \"Just finished my first machine learning project! ðŸš€\",\n",
    "    \"Traffic is so bad today ðŸ˜¤\",\n",
    "    \"Reading a good book by the fireplace\"\n",
    "]\n",
    "\n",
    "for text in tweet_texts:\n",
    "    result = specific_sentiment(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (score: {result[0]['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDlB1RRV9NJ5"
   },
   "source": [
    "## 5. Understanding Model Information\n",
    "\n",
    "Let's learn how to get information about models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:22.773387Z",
     "iopub.status.busy": "2025-08-27T23:51:22.773387Z",
     "iopub.status.idle": "2025-08-27T23:51:23.331555Z",
     "shell.execute_reply": "2025-08-27T23:51:23.331052Z"
    },
    "id": "EXfI2BvM9NJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased\n",
      "Vocabulary size: 30,522\n",
      "Max sequence length: 512\n",
      "Model parameters: 66,362,880\n",
      "Model size in MB: 253.2 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load a model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length:,}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model size in MB: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfIvQlO39NJ5"
   },
   "source": [
    "## 6. Working with Tokenizers\n",
    "\n",
    "Tokenizers convert text into numbers that models can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:23.333055Z",
     "iopub.status.busy": "2025-08-27T23:51:23.333055Z",
     "iopub.status.idle": "2025-08-27T23:51:23.337659Z",
     "shell.execute_reply": "2025-08-27T23:51:23.337154Z"
    },
    "id": "YGpEUJUe9NJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, Hugging Face! Let's tokenize this text.\n",
      "Tokens: ['hello', ',', 'hugging', 'face', '!', 'let', \"'\", 's', 'token', '##ize', 'this', 'text', '.']\n",
      "Token IDs: [101, 7592, 1010, 17662, 2227, 999, 2292, 1005, 1055, 19204, 4697, 2023, 3793, 1012, 102]\n",
      "Decoded text: [CLS] hello, hugging face! let ' s tokenize this text. [SEP]\n",
      "Number of tokens: 13\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Hello, Hugging Face! Let's tokenize this text.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9sNHDzU9NJ5"
   },
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Process multiple texts efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:23.340165Z",
     "iopub.status.busy": "2025-08-27T23:51:23.339160Z",
     "iopub.status.idle": "2025-08-27T23:51:23.420601Z",
     "shell.execute_reply": "2025-08-27T23:51:23.420601Z"
    },
    "id": "tbOL2qED9NJ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BATCH SENTIMENT ANALYSIS ===\n",
      "ðŸ˜Š 'I love this tutorial!' â†’ POSITIVE (1.000)\n",
      "ðŸ˜ž 'This is confusing.' â†’ NEGATIVE (0.999)\n",
      "ðŸ˜Š 'Machine learning is fascinating.' â†’ POSITIVE (1.000)\n",
      "ðŸ˜ž 'I'm not sure about this.' â†’ NEGATIVE (1.000)\n",
      "ðŸ˜Š 'Great explanation, thank you!' â†’ POSITIVE (1.000)\n"
     ]
    }
   ],
   "source": [
    "# Batch processing with sentiment analysis\n",
    "batch_texts = [\n",
    "    \"I love this tutorial!\",\n",
    "    \"This is confusing.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I'm not sure about this.\",\n",
    "    \"Great explanation, thank you!\"\n",
    "]\n",
    "\n",
    "print(\"=== BATCH SENTIMENT ANALYSIS ===\")\n",
    "batch_results = sentiment_pipeline(batch_texts)\n",
    "\n",
    "for text, result in zip(batch_texts, batch_results):\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    emoji = \"ðŸ˜Š\" if label == \"POSITIVE\" else \"ðŸ˜ž\"\n",
    "    print(f\"{emoji} '{text}' â†’ {label} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srtV1ZTU9NJ5"
   },
   "source": [
    "## 8. Error Handling and Best Practices\n",
    "\n",
    "Let's learn about common issues and how to handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:23.422104Z",
     "iopub.status.busy": "2025-08-27T23:51:23.422104Z",
     "iopub.status.idle": "2025-08-27T23:51:23.428145Z",
     "shell.execute_reply": "2025-08-27T23:51:23.428145Z"
    },
    "id": "7keWDMyN9NJ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 2600 characters\n",
      "Token count: 700 tokens\n",
      "Max model length: 512 tokens\n",
      "Truncated token count: 512 tokens\n"
     ]
    }
   ],
   "source": [
    "# Handling long texts\n",
    "long_text = \"This is a very long text. \" * 100  # Repeat to make it long\n",
    "\n",
    "print(f\"Text length: {len(long_text)} characters\")\n",
    "print(f\"Token count: {len(tokenizer.tokenize(long_text))} tokens\")\n",
    "print(f\"Max model length: {tokenizer.model_max_length} tokens\")\n",
    "\n",
    "# Truncate if necessary\n",
    "truncated_encoding = tokenizer(\n",
    "    long_text,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"Truncated token count: {truncated_encoding['input_ids'].shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJtBud719NJ6"
   },
   "source": [
    "## 9. Saving and Loading Models\n",
    "\n",
    "Learn how to save models locally for offline use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:23.429149Z",
     "iopub.status.busy": "2025-08-27T23:51:23.429149Z",
     "iopub.status.idle": "2025-08-27T23:51:23.799520Z",
     "shell.execute_reply": "2025-08-27T23:51:23.799520Z"
    },
    "id": "GPAU2OLC9NJ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./local_distilbert\n",
      "Files in directory: ['config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.txt']\n",
      "\n",
      "Successfully loaded model from local directory!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save model and tokenizer locally\n",
    "local_model_path = \"./local_distilbert\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "\n",
    "print(f\"Model saved to: {local_model_path}\")\n",
    "print(f\"Files in directory: {os.listdir(local_model_path)}\")\n",
    "\n",
    "# Load from local path\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "local_model = AutoModel.from_pretrained(local_model_path)\n",
    "\n",
    "print(\"\\nSuccessfully loaded model from local directory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HS02O8h9NJ6"
   },
   "source": [
    "## 10. Performance Tips and Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:23.801525Z",
     "iopub.status.busy": "2025-08-27T23:51:23.801525Z",
     "iopub.status.idle": "2025-08-27T23:51:24.007566Z",
     "shell.execute_reply": "2025-08-27T23:51:24.007566Z"
    },
    "id": "4mnqteDv9NJ6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline created on device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Create pipeline with specific device\n",
    "sentiment_gpu = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
    "print(f\"\\nPipeline created on device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xoseaxr09NJ6"
   },
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed your first Hugging Face tutorial! Here's what you've learned:\n",
    "\n",
    "âœ… **Installation and setup** of Hugging Face libraries  \n",
    "âœ… **Using pipelines** for quick model inference  \n",
    "âœ… **Different NLP tasks** (sentiment analysis, QA, NER, text generation)  \n",
    "âœ… **Working with tokenizers** and understanding tokenization  \n",
    "âœ… **Batch processing** for efficiency  \n",
    "âœ… **Model management** (saving and loading)  \n",
    "âœ… **Best practices** for handling long texts and devices  \n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "**Explore more models** on the [Hugging Face Hub](https://huggingface.co/models)\n",
    "**Try different tasks** with various pipeline types in the blank cell below\n",
    "**Move to the next notebook**: `02_tokenizers.ipynb`\n",
    "\n",
    "Happy learning! ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T23:51:24.009070Z",
     "iopub.status.busy": "2025-08-27T23:51:24.008569Z",
     "iopub.status.idle": "2025-08-27T23:51:24.010581Z",
     "shell.execute_reply": "2025-08-27T23:51:24.010581Z"
    },
    "id": "AD6qKQKM9NJ6"
   },
   "outputs": [],
   "source": [
    "# Dream here!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
