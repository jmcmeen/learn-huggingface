{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Save the fine-tuned model\nmodel_save_path = \"./fine_tuned_sentiment_model\"\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)\n\nprint(f\"Model saved to: {model_save_path}\")\n\n# Test the fine-tuned model\ndef predict_sentiment(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        \n    confidence = torch.max(predictions).item()\n    predicted_class = torch.argmax(predictions).item()\n    sentiment = \"positive\" if predicted_class == 1 else \"negative\"\n    \n    return sentiment, confidence\n\n# Test on new examples\ntest_texts = [\n    \"This movie was absolutely incredible!\",\n    \"I hate this product, it's terrible.\",\n    \"The service was okay, nothing special.\",\n    \"Best purchase I've ever made!\",\n    \"Complete waste of time and money.\"\n]\n\nprint(\"\\nTesting fine-tuned model:\")\nprint(\"=\" * 60)\n\nfor text in test_texts:\n    sentiment, confidence = predict_sentiment(text)\n    print(f\"Text: {text}\")\n    print(f\"Prediction: {sentiment} (confidence: {confidence:.3f})\")\n    print(\"-\" * 40)\n\n# Load model from saved checkpoint (demonstration)\nprint(\"\\nDemonstrating model loading from checkpoint:\")\nfrom transformers import pipeline\n\n# Create pipeline from saved model\nsentiment_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=model_save_path,\n    tokenizer=model_save_path\n)\n\n# Quick test\nresult = sentiment_pipeline(\"This fine-tuning worked great!\")\nprint(f\"Pipeline result: {result}\")\n\nprint(\"\\nðŸŽ‰ Fine-tuning completed successfully!\")\nprint(\"Model ready for deployment and inference!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 5: Model Testing and Deployment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Start fine-tuning\nprint(\"Starting fine-tuning...\")\ntrain_result = trainer.train()\n\n# Training results\nprint(\"\\nTraining completed!\")\nprint(f\"Final training loss: {train_result.training_loss:.4f}\")\n\n# Evaluate on validation set\neval_results = trainer.evaluate()\nprint(f\"\\nValidation Results:\")\nfor key, value in eval_results.items():\n    if key.startswith('eval_'):\n        metric_name = key.replace('eval_', '')\n        print(f\"{metric_name.capitalize()}: {value:.4f}\")\n\n# Training history (if available)\nif hasattr(trainer.state, 'log_history'):\n    print(f\"\\nTraining steps completed: {len(trainer.state.log_history)}\")\n    \n    # Show loss progression\n    losses = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\n    if losses:\n        print(f\"Loss progression: {losses[0]:.4f} -> {losses[-1]:.4f}\")\n        print(f\"Loss improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 4: Running the Fine-tuning Process",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Define metrics computation\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n    accuracy = accuracy_score(labels, predictions)\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=10,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Training configuration:\")\nprint(f\"Epochs: {training_args.num_train_epochs}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"Learning rate: {training_args.learning_rate}\")\nprint(f\"Weight decay: {training_args.weight_decay}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 3: Training Configuration and Metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load pre-trained model and tokenizer\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=2  # binary classification\n)\n\nprint(f\"Model: {model_name}\")\nprint(f\"Number of parameters: {model.num_parameters():,}\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'], \n        truncation=True, \n        padding=True,\n        max_length=128\n    )\n\n# Apply tokenization to datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(\"\\nTokenized example:\")\nprint(f\"Input IDs shape: {train_dataset[0]['input_ids'].shape}\")\nprint(f\"Attention mask shape: {train_dataset[0]['attention_mask'].shape}\")\nprint(f\"Label: {train_dataset[0]['label']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Example 2: Model and Tokenizer Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sample data for sentiment analysis fine-tuning\nsample_data = {\n    'text': [\n        \"This product is amazing! I love it so much.\",\n        \"Terrible experience, would not recommend.\",\n        \"It's okay, nothing special but works fine.\",\n        \"Outstanding quality and great customer service!\",\n        \"Worst purchase ever, complete waste of money.\",\n        \"Pretty good value for the price point.\",\n        \"Absolutely fantastic! Exceeded my expectations.\",\n        \"Not bad, but could be better quality.\",\n        \"Love this! Will definitely buy again.\",\n        \"Disappointing results, expected much more.\"\n    ],\n    'label': [1, 0, 1, 1, 0, 1, 1, 0, 1, 0]  # 1: positive, 0: negative\n}\n\n# Create dataset\ndataset = Dataset.from_dict(sample_data)\n\n# Split into train/validation\ntrain_size = int(0.8 * len(dataset))\ntrain_dataset = dataset.select(range(train_size))\nval_dataset = dataset.select(range(train_size, len(dataset)))\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"\\nSample data:\")\nprint(train_dataset[0])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Basics with Hugging Face ðŸŽ¯\n",
    "\n",
    "Fine-tuning adapts pre-trained models to your specific tasks and datasets. It's the key to getting state-of-the-art performance on domain-specific problems.\n",
    "\n",
    "## What is Fine-tuning?\n",
    "\n",
    "**Fine-tuning** customizes pre-trained models:\n",
    "- **Transfer Learning**: Leverage pre-trained knowledge\n",
    "- **Task Adaptation**: Specialize for your specific use case\n",
    "- **Examples**: Custom classification, domain-specific NER, specialized generation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Prepare datasets for fine-tuning\n",
    "2. Set up training configurations\n",
    "3. Monitor training progress and metrics\n",
    "4. Handle overfitting and optimization issues\n",
    "5. Save and load fine-tuned models\n",
    "6. Best practices for different model types\n",
    "\n",
    "Let's fine-tune! âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "print('Fine-tuning basics notebook ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 1: Data Preparation for Fine-tuning",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}