{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with Hugging Face üöÄ\n",
    "\n",
    "Deployment brings your models from development to production, making them accessible to real users and applications.\n",
    "\n",
    "## What is Model Deployment?\n",
    "\n",
    "**Model Deployment** makes models production-ready:\n",
    "- **Serving**: Host models via APIs\n",
    "- **Scaling**: Handle multiple concurrent requests\n",
    "- **Monitoring**: Track performance and usage\n",
    "- **Examples**: REST APIs, web apps, mobile integration\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Deploy models using Hugging Face Inference API\n",
    "2. Create REST APIs with FastAPI and Flask\n",
    "3. Containerize models with Docker\n",
    "4. Handle scaling and load balancing\n",
    "5. Monitor model performance in production\n",
    "6. Implement A/B testing for model updates\n",
    "\n",
    "Let's deploy! üåê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import json\n",
    "print('Deployment notebook ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hugging Face Inference API\n",
    "\n",
    "The simplest deployment method - use Hugging Face's hosted infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Inference API for sentiment analysis\n",
    "API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_HF_TOKEN_HERE\"}  # Replace with your token\n",
    "\n",
    "def query_inference_api(payload):\n",
    "    \"\"\"Send request to Hugging Face Inference API\"\"\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "test_texts = [\n",
    "    \"I love using Hugging Face models!\",\n",
    "    \"This deployment tutorial is confusing\",\n",
    "    \"The weather is okay today\"\n",
    "]\n",
    "\n",
    "print(\"üî• Hugging Face Inference API Results:\")\n",
    "for text in test_texts:\n",
    "    data = {\"inputs\": text}\n",
    "    result = query_inference_api(data)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Result: {result}\")\n",
    "    \n",
    "print(\"\\n‚úÖ Inference API deployment complete!\")\n",
    "print(\"üí° Pros: Zero setup, managed scaling, built-in caching\")\n",
    "print(\"‚ö†Ô∏è Cons: Rate limits, cold starts, less control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FastAPI REST API\n",
    "\n",
    "Create a custom API service for full control over your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI application code\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"HF Model API\", \n",
    "    description=\"Production-ready Hugging Face model deployment\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load model once at startup\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str\n",
    "    \n",
    "class BatchRequest(BaseModel):\n",
    "    texts: List[str]\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    text: str\n",
    "    predictions: List[dict]\n",
    "    processing_time: float\n",
    "\n",
    "# Metrics tracking\n",
    "request_count = 0\n",
    "\n",
    "def log_request():\n",
    "    global request_count\n",
    "    request_count += 1\n",
    "    logger.info(f\"Processing request #{request_count}\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"HF Model API is running!\",\n",
    "        \"model\": \"twitter-roberta-base-sentiment-latest\",\n",
    "        \"total_requests\": request_count\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest, background_tasks: BackgroundTasks):\n",
    "    background_tasks.add_task(log_request)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = classifier(request.text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            text=request.text,\n",
    "            predictions=result[0],\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/batch_predict\")\n",
    "async def batch_predict(request: BatchRequest, background_tasks: BackgroundTasks):\n",
    "    background_tasks.add_task(log_request)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    for text in request.texts:\n",
    "        try:\n",
    "            result = classifier(text)\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"predictions\": result[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"processing_time\": processing_time,\n",
    "        \"total_texts\": len(request.texts)\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    return {\n",
    "        \"total_requests\": request_count,\n",
    "        \"model_loaded\": classifier is not None,\n",
    "        \"uptime\": time.time()\n",
    "    }\n",
    "\n",
    "# To run: uvicorn app:app --host 0.0.0.0 --port 8000 --reload\n",
    "'''\n",
    "\n",
    "# Save the FastAPI code\n",
    "with open('fastapi_deployment.py', 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"üìù FastAPI deployment code saved to 'fastapi_deployment.py'\")\n",
    "print(\"\\nüöÄ To run the API:\")\n",
    "print(\"1. pip install fastapi uvicorn\")\n",
    "print(\"2. uvicorn fastapi_deployment:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "print(\"3. Visit http://localhost:8000/docs for interactive API documentation\")\n",
    "print(\"\\n‚ú® Features:\")\n",
    "print(\"- Single and batch predictions\")\n",
    "print(\"- Built-in metrics tracking\")\n",
    "print(\"- Health checks\")\n",
    "print(\"- Request logging\")\n",
    "print(\"- Auto-generated documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docker Containerization\n",
    "\n",
    "Package your model in a Docker container for consistent, portable deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile for model deployment\n",
    "dockerfile_content = '''\n",
    "# Multi-stage build for smaller final image\n",
    "FROM python:3.9-slim as builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install build dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    g++ \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy installed packages from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "\n",
    "# Copy application code\n",
    "COPY fastapi_deployment.py .\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd --create-home --shell /bin/bash app \\\\\n",
    "    && chown -R app:app /app\n",
    "USER app\n",
    "\n",
    "# Download model at build time (optional - saves startup time)\n",
    "RUN python -c \"from transformers import pipeline; pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')\"\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "# Use exec form for better signal handling\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"fastapi_deployment:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "# Create requirements.txt for Docker\n",
    "requirements_content = '''\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "transformers==4.35.2\n",
    "torch==2.1.0\n",
    "pydantic==2.5.0\n",
    "requests==2.31.0\n",
    "'''\n",
    "\n",
    "# Create docker-compose for easier orchestration\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  huggingface-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - PYTHONUNBUFFERED=1\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "    \n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "    depends_on:\n",
    "      - huggingface-api\n",
    "    restart: unless-stopped\n",
    "'''\n",
    "\n",
    "# Create nginx config for load balancing\n",
    "nginx_config = '''\n",
    "events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream app {\n",
    "        server huggingface-api:8000;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "        \n",
    "        location / {\n",
    "            proxy_pass http://app;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "            proxy_set_header X-Forwarded-Proto $scheme;\n",
    "            \n",
    "            # Timeouts\n",
    "            proxy_connect_timeout 60s;\n",
    "            proxy_send_timeout 60s;\n",
    "            proxy_read_timeout 60s;\n",
    "        }\n",
    "        \n",
    "        location /health {\n",
    "            proxy_pass http://app/health;\n",
    "            access_log off;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save all Docker-related files\n",
    "files_to_create = {\n",
    "    'Dockerfile': dockerfile_content,\n",
    "    'requirements.txt': requirements_content,\n",
    "    'docker-compose.yml': docker_compose_content,\n",
    "    'nginx.conf': nginx_config\n",
    "}\n",
    "\n",
    "for filename, content in files_to_create.items():\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"üê≥ Docker deployment files created successfully!\")\n",
    "print(\"\\nüì¶ Files created:\")\n",
    "for filename in files_to_create.keys():\n",
    "    print(f\"  - {filename}\")\n",
    "\n",
    "print(\"\\nüöÄ To deploy:\")\n",
    "print(\"1. docker build -t huggingface-api .\")\n",
    "print(\"2. docker run -p 8000:8000 huggingface-api\")\n",
    "print(\"\\nüîÑ Or use docker-compose:\")\n",
    "print(\"1. docker-compose up --build\")\n",
    "print(\"2. Access API at http://localhost:80\")\n",
    "\n",
    "print(\"\\n‚ú® Features:\")\n",
    "print(\"- Multi-stage build for smaller image\")\n",
    "print(\"- Non-root user for security\")\n",
    "print(\"- Health checks\")\n",
    "print(\"- Nginx reverse proxy\")\n",
    "print(\"- Production-ready configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradio Web Interface\n",
    "\n",
    "Create an interactive web demo for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio web interface\n",
    "gradio_code = '''\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Analytics tracking\n",
    "usage_stats = {\n",
    "    \"total_predictions\": 0,\n",
    "    \"start_time\": time.time()\n",
    "}\n",
    "\n",
    "def analyze_sentiment(text, return_confidence=True):\n",
    "    \"\"\"Analyze sentiment with detailed results\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"‚ö†Ô∏è Please enter some text to analyze.\", None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        results = classifier(text)\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Update usage stats\n",
    "        usage_stats[\"total_predictions\"] += 1\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        confidence_data = []\n",
    "        \n",
    "        for result in results[0]:\n",
    "            label = result['label'].replace('LABEL_', '')\n",
    "            if label == '0':\n",
    "                label = 'üò¢ Negative'\n",
    "            elif label == '1':\n",
    "                label = 'üòê Neutral'\n",
    "            elif label == '2':\n",
    "                label = 'üòä Positive'\n",
    "            \n",
    "            score = result['score']\n",
    "            formatted_results.append(f\"{label}: {score:.3f} ({score*100:.1f}%)\")\n",
    "            confidence_data.append((label, score))\n",
    "        \n",
    "        # Create response\n",
    "        response = \"\\n\".join(formatted_results)\n",
    "        response += f\"\\n\\n‚ö° Processing time: {processing_time:.3f}s\"\n",
    "        response += f\"\\nüìä Total predictions: {usage_stats['total_predictions']}\"\n",
    "        \n",
    "        # Log the prediction\n",
    "        logger.info(f\"Prediction #{usage_stats['total_predictions']}: {processing_time:.3f}s\")\n",
    "        \n",
    "        if return_confidence:\n",
    "            return response, confidence_data\n",
    "        else:\n",
    "            return response, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return error_msg, None\n",
    "\n",
    "def get_usage_stats():\n",
    "    \"\"\"Get current usage statistics\"\"\"\n",
    "    uptime = time.time() - usage_stats[\"start_time\"]\n",
    "    hours = int(uptime // 3600)\n",
    "    minutes = int((uptime % 3600) // 60)\n",
    "    \n",
    "    return f\"\"\"üìà Usage Statistics:\n",
    "‚Ä¢ Total Predictions: {usage_stats['total_predictions']}\n",
    "‚Ä¢ Uptime: {hours}h {minutes}m\n",
    "‚Ä¢ Model: twitter-roberta-base-sentiment-latest\n",
    "‚Ä¢ Status: ‚úÖ Running\"\"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"HF Sentiment Analysis\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ü§ó Hugging Face Sentiment Analysis\n",
    "    \n",
    "    Analyze the sentiment of any text using a RoBERTa model fine-tuned on Twitter data.\n",
    "    This model can detect **positive**, **negative**, and **neutral** sentiments.\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Tab(\"Single Prediction\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text_input = gr.Textbox(\n",
    "                    lines=4,\n",
    "                    placeholder=\"Enter text to analyze sentiment...\",\n",
    "                    label=\"Input Text\"\n",
    "                )\n",
    "                confidence_checkbox = gr.Checkbox(\n",
    "                    label=\"Show confidence chart\", \n",
    "                    value=True\n",
    "                )\n",
    "                analyze_btn = gr.Button(\"üîç Analyze Sentiment\", variant=\"primary\")\n",
    "                \n",
    "            with gr.Column():\n",
    "                result_output = gr.Textbox(\n",
    "                    label=\"Analysis Results\",\n",
    "                    lines=6,\n",
    "                    interactive=False\n",
    "                )\n",
    "                confidence_chart = gr.BarPlot(\n",
    "                    label=\"Confidence Scores\",\n",
    "                    x=\"label\",\n",
    "                    y=\"score\",\n",
    "                    title=\"Sentiment Confidence\",\n",
    "                    y_lim=[0, 1]\n",
    "                )\n",
    "    \n",
    "    with gr.Tab(\"Batch Analysis\"):\n",
    "        batch_input = gr.Textbox(\n",
    "            lines=8,\n",
    "            placeholder=\"Enter multiple texts, one per line...\",\n",
    "            label=\"Multiple Texts\"\n",
    "        )\n",
    "        batch_btn = gr.Button(\"üîç Analyze All\", variant=\"primary\")\n",
    "        batch_output = gr.Textbox(\n",
    "            label=\"Batch Results\",\n",
    "            lines=10,\n",
    "            interactive=False\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"Statistics\"):\n",
    "        stats_output = gr.Textbox(\n",
    "            label=\"Usage Statistics\",\n",
    "            lines=6,\n",
    "            interactive=False\n",
    "        )\n",
    "        refresh_stats_btn = gr.Button(\"üîÑ Refresh Stats\")\n",
    "    \n",
    "    # Event handlers\n",
    "    analyze_btn.click(\n",
    "        fn=analyze_sentiment,\n",
    "        inputs=[text_input, confidence_checkbox],\n",
    "        outputs=[result_output, confidence_chart]\n",
    "    )\n",
    "    \n",
    "    def batch_analyze(batch_text):\n",
    "        if not batch_text.strip():\n",
    "            return \"‚ö†Ô∏è Please enter texts to analyze.\"\n",
    "        \n",
    "        texts = [line.strip() for line in batch_text.split('\\n') if line.strip()]\n",
    "        results = []\n",
    "        \n",
    "        for i, text in enumerate(texts, 1):\n",
    "            result, _ = analyze_sentiment(text, return_confidence=False)\n",
    "            results.append(f\"**Text {i}:** {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "            results.append(result)\n",
    "            results.append(\"---\")\n",
    "        \n",
    "        return \"\\n\".join(results)\n",
    "    \n",
    "    batch_btn.click(\n",
    "        fn=batch_analyze,\n",
    "        inputs=batch_input,\n",
    "        outputs=batch_output\n",
    "    )\n",
    "    \n",
    "    refresh_stats_btn.click(\n",
    "        fn=get_usage_stats,\n",
    "        outputs=stats_output\n",
    "    )\n",
    "    \n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"I absolutely love this new feature! It's amazing! üéâ\"],\n",
    "            [\"This is terrible and completely disappointing.\"],\n",
    "            [\"The weather is okay today, nothing special.\"],\n",
    "            [\"Just deployed my first ML model using Hugging Face! üöÄ\"],\n",
    "            [\"The meeting was cancelled again. So frustrating! üò§\"]\n",
    "        ],\n",
    "        inputs=text_input,\n",
    "        label=\"Example Texts\"\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=False,  # Set to True for public links\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        show_error=True\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Save Gradio code\n",
    "with open('gradio_demo.py', 'w') as f:\n",
    "    f.write(gradio_code)\n",
    "\n",
    "print(\"üé® Gradio web interface created successfully!\")\n",
    "print(\"\\nüìÅ File saved: gradio_demo.py\")\n",
    "print(\"\\nüöÄ To run:\")\n",
    "print(\"1. pip install gradio\")\n",
    "print(\"2. python gradio_demo.py\")\n",
    "print(\"3. Open http://localhost:7860 in your browser\")\n",
    "\n",
    "print(\"\\n‚ú® Features:\")\n",
    "print(\"- Interactive web interface\")\n",
    "print(\"- Single and batch predictions\")\n",
    "print(\"- Confidence visualization\")\n",
    "print(\"- Usage statistics\")\n",
    "print(\"- Example texts\")\n",
    "print(\"- Mobile-friendly design\")\n",
    "print(\"- Real-time processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Testing & Monitoring\n",
    "\n",
    "Test your deployment's performance and implement monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create load testing script\n",
    "load_test_code = '''\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "import statistics\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "class LoadTester:\n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url\n",
    "        self.results = []\n",
    "    \n",
    "    def send_request(self, text: str, request_id: int) -> Dict:\n",
    "        \"\"\"Send a single prediction request\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/predict\",\n",
    "                json={\"text\": text},\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time\": end_time - start_time,\n",
    "                \"success\": response.status_code == 200,\n",
    "                \"timestamp\": start_time\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            return {\n",
    "                \"request_id\": request_id,\n",
    "                \"status_code\": 0,\n",
    "                \"response_time\": end_time - start_time,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": start_time\n",
    "            }\n",
    "    \n",
    "    def run_load_test(self, \n",
    "                     test_texts: List[str],\n",
    "                     num_requests: int = 100,\n",
    "                     concurrent_users: int = 10) -> Dict:\n",
    "        \"\"\"Run load test with multiple concurrent requests\"\"\"\n",
    "        \n",
    "        print(f\"üî• Starting load test...\")\n",
    "        print(f\"üìä Requests: {num_requests}, Concurrent users: {concurrent_users}\")\n",
    "        \n",
    "        # Prepare requests\n",
    "        requests_to_send = []\n",
    "        for i in range(num_requests):\n",
    "            text = test_texts[i % len(test_texts)]\n",
    "            requests_to_send.append((text, i))\n",
    "        \n",
    "        # Execute concurrent requests\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "            future_to_request = {\n",
    "                executor.submit(self.send_request, text, req_id): (text, req_id)\n",
    "                for text, req_id in requests_to_send\n",
    "            }\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_request):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        successful_requests = [r for r in results if r['success']]\n",
    "        failed_requests = [r for r in results if not r['success']]\n",
    "        \n",
    "        response_times = [r['response_time'] for r in successful_requests]\n",
    "        \n",
    "        stats = {\n",
    "            \"total_requests\": num_requests,\n",
    "            \"successful_requests\": len(successful_requests),\n",
    "            \"failed_requests\": len(failed_requests),\n",
    "            \"success_rate\": len(successful_requests) / num_requests * 100,\n",
    "            \"total_time\": total_time,\n",
    "            \"requests_per_second\": num_requests / total_time,\n",
    "            \"avg_response_time\": statistics.mean(response_times) if response_times else 0,\n",
    "            \"median_response_time\": statistics.median(response_times) if response_times else 0,\n",
    "            \"min_response_time\": min(response_times) if response_times else 0,\n",
    "            \"max_response_time\": max(response_times) if response_times else 0,\n",
    "            \"p95_response_time\": statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else 0,\n",
    "            \"p99_response_time\": statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else 0\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_results(self, stats: Dict):\n",
    "        \"\"\"Print formatted test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéØ LOAD TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìä Overview:\")\n",
    "        print(f\"  Total Requests: {stats['total_requests']}\")\n",
    "        print(f\"  Successful: {stats['successful_requests']} ({stats['success_rate']:.1f}%)\")\n",
    "        print(f\"  Failed: {stats['failed_requests']}\")\n",
    "        print(f\"  Total Time: {stats['total_time']:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n‚ö° Performance:\")\n",
    "        print(f\"  Requests/sec: {stats['requests_per_second']:.2f}\")\n",
    "        print(f\"  Avg Response Time: {stats['avg_response_time']:.3f}s\")\n",
    "        print(f\"  Median Response Time: {stats['median_response_time']:.3f}s\")\n",
    "        \n",
    "        print(f\"\\nüìà Response Time Distribution:\")\n",
    "        print(f\"  Min: {stats['min_response_time']:.3f}s\")\n",
    "        print(f\"  P95: {stats['p95_response_time']:.3f}s\")\n",
    "        print(f\"  P99: {stats['p99_response_time']:.3f}s\")\n",
    "        print(f\"  Max: {stats['max_response_time']:.3f}s\")\n",
    "        \n",
    "        # Performance recommendations\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        if stats['success_rate'] < 95:\n",
    "            print(f\"  ‚ö†Ô∏è Success rate is low ({stats['success_rate']:.1f}%). Check for errors.\")\n",
    "        if stats['avg_response_time'] > 1.0:\n",
    "            print(f\"  ‚ö†Ô∏è High average response time ({stats['avg_response_time']:.3f}s). Consider optimization.\")\n",
    "        if stats['requests_per_second'] < 10:\n",
    "            print(f\"  ‚ö†Ô∏è Low throughput ({stats['requests_per_second']:.2f} req/s). Consider scaling.\")\n",
    "        if stats['success_rate'] >= 95 and stats['avg_response_time'] <= 1.0:\n",
    "            print(f\"  ‚úÖ Performance looks good!\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test texts for load testing\n",
    "    test_texts = [\n",
    "        \"I love this product! It's amazing!\",\n",
    "        \"This is terrible and disappointing.\",\n",
    "        \"The weather is okay today.\",\n",
    "        \"Just deployed my first model!\",\n",
    "        \"The meeting was cancelled again.\",\n",
    "        \"Great work on this project!\",\n",
    "        \"I'm not sure about this decision.\",\n",
    "        \"The performance is outstanding!\",\n",
    "        \"This could be better.\",\n",
    "        \"Neutral feedback on the implementation.\"\n",
    "    ]\n",
    "    \n",
    "    # Create load tester\n",
    "    tester = LoadTester()\n",
    "    \n",
    "    # Test different load scenarios\n",
    "    scenarios = [\n",
    "        {\"requests\": 50, \"concurrent\": 5, \"name\": \"Light Load\"},\n",
    "        {\"requests\": 100, \"concurrent\": 10, \"name\": \"Medium Load\"},\n",
    "        {\"requests\": 200, \"concurrent\": 20, \"name\": \"Heavy Load\"}\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\nüß™ Running {scenario['name']} Test...\")\n",
    "        stats = tester.run_load_test(\n",
    "            test_texts,\n",
    "            num_requests=scenario['requests'],\n",
    "            concurrent_users=scenario['concurrent']\n",
    "        )\n",
    "        tester.print_results(stats)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(\"\\nüéâ Load testing completed!\")\n",
    "'''\n",
    "\n",
    "# Save load testing code\n",
    "with open('load_test.py', 'w') as f:\n",
    "    f.write(load_test_code)\n",
    "\n",
    "print(\"üîß Load testing script created successfully!\")\n",
    "print(\"\\nüìÅ File saved: load_test.py\")\n",
    "print(\"\\nüß™ To run load tests:\")\n",
    "print(\"1. Start your FastAPI server: uvicorn fastapi_deployment:app\")\n",
    "print(\"2. Run tests: python load_test.py\")\n",
    "\n",
    "print(\"\\nüìä Test scenarios:\")\n",
    "print(\"- Light Load: 50 requests, 5 concurrent users\")\n",
    "print(\"- Medium Load: 100 requests, 10 concurrent users\") \n",
    "print(\"- Heavy Load: 200 requests, 20 concurrent users\")\n",
    "\n",
    "print(\"\\n‚ú® Metrics tracked:\")\n",
    "print(\"- Success rate and error rate\")\n",
    "print(\"- Response time statistics (avg, median, p95, p99)\")\n",
    "print(\"- Throughput (requests per second)\")\n",
    "print(\"- Performance recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}