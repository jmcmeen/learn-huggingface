{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Classification with Hugging Face üè∑Ô∏è\n",
    "\n",
    "Token classification assigns labels to individual tokens (words or subwords) in a sequence. Unlike text classification which labels entire documents, token classification works at the word level to identify entities, parts of speech, or other token-specific information.\n",
    "\n",
    "## What is Token Classification?\n",
    "\n",
    "**Token Classification** assigns labels to individual tokens:\n",
    "- **Input**: Sequence of tokens (words/subwords)\n",
    "- **Output**: Label for each token with confidence scores\n",
    "- **Examples**: Named Entity Recognition (NER), Part-of-Speech tagging, keyword extraction\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Use pre-trained NER models for entity extraction\n",
    "2. Handle different entity types and tagging schemes\n",
    "3. Process and aggregate token-level predictions\n",
    "4. Build custom token classification models\n",
    "5. Evaluate token classification performance\n",
    "6. Handle real-world text processing challenges\n",
    "\n",
    "Let's start extracting meaningful information from text! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Named Entity Recognition (NER)\n",
    "\n",
    "Let's start with pre-trained NER models to extract entities from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition with pre-trained models\n",
    "print(\"üè∑Ô∏è Named Entity Recognition\")\n",
    "print(\"=\" * 28)\n",
    "\n",
    "# Load NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test texts with various entities\n",
    "test_texts = [\n",
    "    \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\",\n",
    "    \"The meeting with Microsoft is scheduled for next Tuesday at 3 PM in New York.\",\n",
    "    \"Dr. Sarah Johnson from Harvard University published a paper about AI in Nature.\",\n",
    "    \"Amazon reported $469 billion in revenue for 2021, up from $386 billion in 2020.\",\n",
    "    \"The concert featuring Taylor Swift will be held at Madison Square Garden on December 15th.\"\n",
    "]\n",
    "\n",
    "# Entity type colors for visualization\n",
    "entity_colors = {\n",
    "    'PER': 'üë§',  # Person\n",
    "    'ORG': 'üè¢',  # Organization  \n",
    "    'LOC': 'üìç',  # Location\n",
    "    'MISC': 'üè∑Ô∏è'  # Miscellaneous\n",
    "}\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n{i}. Text: '{text}'\")\n",
    "    entities = ner_pipeline(text)\n",
    "    \n",
    "    if entities:\n",
    "        print(\"   Entities found:\")\n",
    "        for entity in entities:\n",
    "            emoji = entity_colors.get(entity['entity_group'], 'üîç')\n",
    "            print(f\"     {emoji} {entity['word']} ({entity['entity_group']}) - Score: {entity['score']:.3f}\")\n",
    "    else:\n",
    "        print(\"   No entities found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Entity Extraction\n",
    "\n",
    "Let's explore different NER models and handle overlapping entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced entity extraction with different models\n",
    "print(\"üîç Advanced Entity Extraction\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Load different NER models\n",
    "models = {\n",
    "    \"General NER\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    \"Biomedical NER\": \"d4data/biomedical-ner-all\"\n",
    "}\n",
    "\n",
    "# Sample texts for different domains\n",
    "sample_texts = {\n",
    "    \"Business\": \"Elon Musk announced Tesla's quarterly earnings at the SpaceX facility in Hawthorne.\",\n",
    "    \"Medical\": \"The patient was diagnosed with diabetes and prescribed metformin by Dr. Smith.\"\n",
    "}\n",
    "\n",
    "for domain, text in sample_texts.items():\n",
    "    print(f\"\\nüìã {domain} Text: '{text}'\\n\")\n",
    "    \n",
    "    for model_name, model_path in models.items():\n",
    "        try:\n",
    "            # Load model-specific pipeline\n",
    "            ner = pipeline(\"ner\", model=model_path, tokenizer=model_path, aggregation_strategy=\"simple\")\n",
    "            entities = ner(text)\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            if entities:\n",
    "                for entity in entities:\n",
    "                    print(f\"     ‚Ä¢ {entity['word']} ‚Üí {entity['entity_group']} ({entity['score']:.3f})\")\n",
    "            else:\n",
    "                print(\"     No entities detected\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   {model_name}: Model not available ({str(e)[:30]}...)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Token Classification\n",
    "\n",
    "Create a custom dataset and understand the token classification process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom token classification dataset\n",
    "print(\"üõ†Ô∏è Custom Token Classification\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Sample data in IOB format (Inside-Outside-Beginning)\n",
    "sample_sentences = [\n",
    "    {\n",
    "        \"tokens\": [\"John\", \"works\", \"at\", \"Google\", \"in\", \"Mountain\", \"View\"],\n",
    "        \"labels\": [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\", \"I-LOC\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Apple\", \"Inc\", \"was\", \"founded\", \"in\", \"Cupertino\"],\n",
    "        \"labels\": [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-LOC\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Microsoft\", \"CEO\", \"Satya\", \"Nadella\", \"announced\", \"new\", \"products\"],\n",
    "        \"labels\": [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"O\", \"O\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create label mappings\n",
    "unique_labels = set()\n",
    "for sentence in sample_sentences:\n",
    "    unique_labels.update(sentence[\"labels\"])\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Label mappings:\")\n",
    "for label, idx in sorted(label_to_id.items()):\n",
    "    print(f\"  {idx}: {label}\")\n",
    "\n",
    "# Show sample data structure\n",
    "print(\"\\nSample training data:\")\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(\"Tokens: \", sentence[\"tokens\"])\n",
    "    print(\"Labels: \", sentence[\"labels\"])\n",
    "    \n",
    "    # Show token-label pairs\n",
    "    print(\"Pairs:  \", end=\"\")\n",
    "    for token, label in zip(sentence[\"tokens\"], sentence[\"labels\"]):\n",
    "        print(f\"({token}/{label}) \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entity Extraction and Aggregation\n",
    "\n",
    "Process token-level predictions and extract complete entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction and aggregation\n",
    "print(\"üìä Entity Extraction and Aggregation\")\n",
    "print(\"=\" * 38)\n",
    "\n",
    "def extract_entities_from_tokens(tokens, labels):\n",
    "    \"\"\"Extract complete entities from token-label pairs using IOB format\"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith('B-'):  # Beginning of entity\n",
    "            if current_entity:  # Save previous entity\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\n",
    "                'text': token,\n",
    "                'label': label[2:],  # Remove 'B-' prefix\n",
    "                'tokens': [token]\n",
    "            }\n",
    "        elif label.startswith('I-') and current_entity:  # Inside entity\n",
    "            current_entity['text'] += ' ' + token\n",
    "            current_entity['tokens'].append(token)\n",
    "        else:  # Outside entity ('O') or start of new entity\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Don't forget the last entity\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test entity extraction\n",
    "print(\"Testing entity extraction:\")\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    entities = extract_entities_from_tokens(sentence[\"tokens\"], sentence[\"labels\"])\n",
    "    \n",
    "    print(f\"\\nSentence {i+1}: {' '.join(sentence['tokens'])}\")\n",
    "    print(\"Extracted entities:\")\n",
    "    \n",
    "    if entities:\n",
    "        for entity in entities:\n",
    "            emoji = {'PER': 'üë§', 'ORG': 'üè¢', 'LOC': 'üìç'}.get(entity['label'], 'üè∑Ô∏è')\n",
    "            print(f\"  {emoji} '{entity['text']}' ‚Üí {entity['label']} (tokens: {len(entity['tokens'])})\")\n",
    "    else:\n",
    "        print(\"  No entities found\")\n",
    "\n",
    "# Demonstrate real NER pipeline with aggregation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Real NER Pipeline Comparison:\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "test_text = \"Steve Jobs founded Apple Inc. in Cupertino, California.\"\n",
    "\n",
    "print(f\"\\nText: '{test_text}'\")\n",
    "entities = ner_pipeline(test_text)\n",
    "\n",
    "print(\"Pipeline results:\")\n",
    "for entity in entities:\n",
    "    emoji = {'PER': 'üë§', 'ORG': 'üè¢', 'LOC': 'üìç'}.get(entity['entity_group'], 'üè∑Ô∏è')\n",
    "    print(f\"  {emoji} '{entity['word']}' ‚Üí {entity['entity_group']} (score: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-world Information Extraction System\n",
    "\n",
    "Build a comprehensive information extraction system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world information extraction system\n",
    "print(\"üåç Real-world Information Extraction\")\n",
    "print(\"=\" * 39)\n",
    "\n",
    "class InformationExtractor:\n",
    "    def __init__(self):\n",
    "        self.ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "        \n",
    "        # Entity type mappings\n",
    "        self.entity_types = {\n",
    "            'PER': {'name': 'People', 'emoji': 'üë§', 'color': 'blue'},\n",
    "            'ORG': {'name': 'Organizations', 'emoji': 'üè¢', 'color': 'green'},\n",
    "            'LOC': {'name': 'Locations', 'emoji': 'üìç', 'color': 'red'},\n",
    "            'MISC': {'name': 'Miscellaneous', 'emoji': 'üè∑Ô∏è', 'color': 'orange'}\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract and organize entities from text\"\"\"\n",
    "        entities = self.ner_pipeline(text)\n",
    "        \n",
    "        # Organize by entity type\n",
    "        organized = {entity_type: [] for entity_type in self.entity_types.keys()}\n",
    "        \n",
    "        for entity in entities:\n",
    "            entity_type = entity['entity_group']\n",
    "            if entity_type in organized:\n",
    "                organized[entity_type].append({\n",
    "                    'text': entity['word'],\n",
    "                    'score': entity['score'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end']\n",
    "                })\n",
    "        \n",
    "        return organized\n",
    "    \n",
    "    def analyze_text(self, text, min_confidence=0.5):\n",
    "        \"\"\"Comprehensive text analysis\"\"\"\n",
    "        entities = self.extract_entities(text)\n",
    "        \n",
    "        analysis = {\n",
    "            'text': text,\n",
    "            'total_entities': 0,\n",
    "            'entities_by_type': {},\n",
    "            'high_confidence_entities': [],\n",
    "            'summary': {}\n",
    "        }\n",
    "        \n",
    "        for entity_type, entity_list in entities.items():\n",
    "            if entity_list:\n",
    "                filtered_entities = [e for e in entity_list if e['score'] >= min_confidence]\n",
    "                analysis['entities_by_type'][entity_type] = filtered_entities\n",
    "                analysis['total_entities'] += len(filtered_entities)\n",
    "                \n",
    "                # Add to high confidence list\n",
    "                for entity in filtered_entities:\n",
    "                    if entity['score'] >= 0.9:\n",
    "                        analysis['high_confidence_entities'].append({\n",
    "                            'text': entity['text'],\n",
    "                            'type': entity_type,\n",
    "                            'score': entity['score']\n",
    "                        })\n",
    "        \n",
    "        # Generate summary\n",
    "        for entity_type, entity_list in analysis['entities_by_type'].items():\n",
    "            if entity_list:\n",
    "                type_info = self.entity_types[entity_type]\n",
    "                analysis['summary'][type_info['name']] = len(entity_list)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = InformationExtractor()\n",
    "print(\"Information extractor initialized!\")\n",
    "\n",
    "# Test comprehensive information extraction\n",
    "news_articles = [\n",
    "    \"\"\"\n",
    "    Tesla CEO Elon Musk announced during a press conference in Austin, Texas, that the company \n",
    "    will be expanding its Gigafactory operations to include battery production for SpaceX rockets. \n",
    "    The announcement was made alongside CFO Zachary Kirkhorn and followed Tesla's record quarterly \n",
    "    earnings reported last week. The expansion is expected to create 500 new jobs in the Austin area.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Apple Inc. reported strong iPhone sales growth in China and India during the third quarter. \n",
    "    CEO Tim Cook praised the performance of Apple stores in Shanghai and Mumbai, noting increased \n",
    "    demand for the iPhone 15 Pro. The company's services revenue also grew significantly, driven \n",
    "    by App Store sales and Apple Music subscriptions across Asian markets.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"\\nüì∞ News Article Analysis\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "for i, article in enumerate(news_articles, 1):\n",
    "    article = article.strip()\n",
    "    print(f\"\\nüìã Article {i}:\")\n",
    "    print(f\"'{article[:100]}...'\\n\")\n",
    "    \n",
    "    analysis = extractor.analyze_text(article)\n",
    "    \n",
    "    print(f\"üîç Analysis Results:\")\n",
    "    print(f\"   Total entities found: {analysis['total_entities']}\")\n",
    "    \n",
    "    # Show summary\n",
    "    if analysis['summary']:\n",
    "        print(f\"   Entity breakdown:\")\n",
    "        for entity_type, count in analysis['summary'].items():\n",
    "            emoji = {'People': 'üë§', 'Organizations': 'üè¢', 'Locations': 'üìç', 'Miscellaneous': 'üè∑Ô∏è'}.get(entity_type, 'üè∑Ô∏è')\n",
    "            print(f\"     {emoji} {entity_type}: {count}\")\n",
    "    \n",
    "    # Show high confidence entities\n",
    "    if analysis['high_confidence_entities']:\n",
    "        print(f\"\\n   üéØ High Confidence Entities (>0.9):\")\n",
    "        for entity in analysis['high_confidence_entities']:\n",
    "            type_info = extractor.entity_types[entity['type']]\n",
    "            print(f\"     {type_info['emoji']} {entity['text']} ({entity['score']:.3f})\")\n",
    "    \n",
    "    # Show detailed entities by type\n",
    "    print(f\"\\n   üìä Detailed Entity List:\")\n",
    "    for entity_type, entities in analysis['entities_by_type'].items():\n",
    "        if entities:\n",
    "            type_info = extractor.entity_types[entity_type]\n",
    "            print(f\"     {type_info['emoji']} {type_info['name']}:\")\n",
    "            for entity in entities:\n",
    "                print(f\"        ‚Ä¢ {entity['text']} (confidence: {entity['score']:.3f})\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "**What you've learned about token classification:**\n",
    "\n",
    "‚úÖ **Named Entity Recognition**: Extract people, organizations, and locations from text  \n",
    "‚úÖ **Token-level Processing**: Understand how models work at the word level  \n",
    "‚úÖ **Entity Aggregation**: Combine token predictions into complete entities  \n",
    "‚úÖ **IOB Tagging**: Work with Inside-Outside-Beginning annotation schemes  \n",
    "‚úÖ **Multiple Models**: Compare different NER models for various domains  \n",
    "‚úÖ **Real-world Applications**: Build comprehensive information extraction systems  \n",
    "‚úÖ **Performance Analysis**: Evaluate and filter entity predictions by confidence  \n",
    "\n",
    "## üîß Best Practices\n",
    "\n",
    "1. **Use aggregation strategies** to combine subword tokens into complete entities\n",
    "2. **Set confidence thresholds** to filter out low-quality predictions\n",
    "3. **Choose domain-specific models** for better performance (e.g., biomedical NER)\n",
    "4. **Handle overlapping entities** carefully in complex text\n",
    "5. **Post-process results** to clean and organize extracted information\n",
    "6. **Validate entities** against known databases or rules when possible\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Ready for the next challenge?\n",
    "\n",
    "**Continue to**: `07_text_generation.ipynb` - Learn about generating text with language models!\n",
    "\n",
    "**Practice**: Try extracting entities from documents in your domain (legal, medical, financial)!\n",
    "\n",
    "Excellent work mastering token classification! üèÜ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}