{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Using Pre-trained Models\n",
    "\n",
    "Pre-trained models are the backbone of modern NLP. Instead of training from scratch, we can use models that have already learned from massive datasets and fine-tune them for our specific tasks.\n",
    "\n",
    "## What are Pre-trained Models?\n",
    "\n",
    "Pre-trained models are neural networks that have been trained on large datasets and can be:\n",
    "- **Used directly** for inference on various tasks\n",
    "- **Fine-tuned** for specific applications\n",
    "- **Used as feature extractors** for downstream tasks\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll know how to:\n",
    "1. Load models and tokenizers from Hugging Face Hub\n",
    "2. Understand different model types and architectures\n",
    "3. Use models for inference\n",
    "4. Handle model configurations and parameters\n",
    "5. Work with different model formats and sizes\n",
    "6. Manage GPU/CPU usage and memory\n",
    "\n",
    "Let's start exploring! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForMaskedLM\n",
    ")\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Model Loading\n",
    "\n",
    "Let's start with the simplest way to load a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple model - BERT base\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "print(f\"üì• Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìä Model type: {type(model).__name__}\")\n",
    "print(f\"üî¢ Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model configuration\n",
    "config = model.config\n",
    "\n",
    "print(\"üîß Model Configuration:\")\n",
    "print(f\"  Architecture: {config.model_type}\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Max position embeddings: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_mb\n",
    "\n",
    "# Test with different models\n",
    "models_to_test = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\", \n",
    "    \"roberta-base\"\n",
    "]\n",
    "\n",
    "memory_info = []\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"üîç Analyzing {model_name}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Calculate memory\n",
    "    model_size = get_model_size(model)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Get GPU memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    else:\n",
    "        gpu_memory = 0\n",
    "    \n",
    "    memory_info.append({\n",
    "        'model': model_name,\n",
    "        'parameters': param_count,\n",
    "        'size_mb': model_size,\n",
    "        'gpu_memory_mb': gpu_memory\n",
    "    })\n",
    "    \n",
    "    print(f\"  Parameters: {param_count:,}\")\n",
    "    print(f\"  Model size: {model_size:.1f} MB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU memory: {gpu_memory:.1f} MB\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "memory_df = pd.DataFrame(memory_info)\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "display(memory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Different Model Types\n",
    "\n",
    "Hugging Face provides specialized model classes for different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of different model types\n",
    "model_types = {\n",
    "    \"Base Model\": (AutoModel, \"bert-base-uncased\"),\n",
    "    \"Sequence Classification\": (AutoModelForSequenceClassification, \"bert-base-uncased\"),\n",
    "    \"Question Answering\": (AutoModelForQuestionAnswering, \"bert-base-uncased\"),\n",
    "    \"Masked Language Modeling\": (AutoModelForMaskedLM, \"bert-base-uncased\")\n",
    "}\n",
    "\n",
    "print(\"üéØ Different Model Types:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for task_name, (model_class, model_name) in model_types.items():\n",
    "    print(f\"\\n{task_name}:\")\n",
    "    try:\n",
    "        model = model_class.from_pretrained(model_name)\n",
    "        print(f\"  ‚úÖ Class: {model_class.__name__}\")\n",
    "        print(f\"  üìè Output shape info: {type(model).__name__}\")\n",
    "        \n",
    "        # Show model head if it has one\n",
    "        if hasattr(model, 'classifier'):\n",
    "            print(f\"  üéØ Classifier output: {model.classifier.out_features} classes\")\n",
    "        elif hasattr(model, 'qa_outputs'):\n",
    "            print(\"  ‚ùì QA outputs: start/end positions\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model for sequence classification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"üé≠ Sentiment Analysis with {model_name}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentence in sentences:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "        \n",
    "        # Map to labels (0: negative, 1: positive for this model)\n",
    "        label = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "        \n",
    "        print(f\"'{sentence}'\")\n",
    "        print(f\"  ‚Üí {label} (confidence: {confidence:.3f})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and modify model configuration\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "print(\"‚öôÔ∏è Working with Model Configurations\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Load configuration first\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "print(\"Original configuration:\")\n",
    "print(f\"  Hidden dropout: {config.hidden_dropout_prob}\")\n",
    "print(f\"  Attention dropout: {config.attention_probs_dropout_prob}\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "\n",
    "# Modify configuration\n",
    "config.hidden_dropout_prob = 0.2\n",
    "config.attention_probs_dropout_prob = 0.2\n",
    "\n",
    "print(\"\\nModified configuration:\")\n",
    "print(f\"  Hidden dropout: {config.hidden_dropout_prob}\")\n",
    "print(f\"  Attention dropout: {config.attention_probs_dropout_prob}\")\n",
    "\n",
    "# Load model with modified config\n",
    "model_with_custom_config = AutoModel.from_pretrained(model_name, config=config)\n",
    "print(\"\\n‚úÖ Model loaded with custom configuration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "‚úÖ **Basic model loading** with `AutoModel` and `AutoTokenizer`\n",
    "‚úÖ **Understanding model architecture** and configurations\n",
    "‚úÖ **Memory usage analysis** for different models\n",
    "‚úÖ **Different model types** for specific tasks\n",
    "‚úÖ **Model inference** with real examples\n",
    "‚úÖ **Configuration management** and customization\n",
    "\n",
    "### Next Steps:\n",
    "- Explore fine-tuning pre-trained models\n",
    "- Learn about different architectures (BERT, GPT, T5)\n",
    "- Practice with domain-specific models\n",
    "- Optimize models for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
