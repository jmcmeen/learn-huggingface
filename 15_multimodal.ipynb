{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal AI with Hugging Face\n",
    "\n",
    "This notebook demonstrates various multimodal AI tasks using Hugging Face Transformers, including:\n",
    "- Image captioning\n",
    "- Visual question answering (VQA)\n",
    "- Text-to-image generation\n",
    "- Image-text similarity\n",
    "- Document understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision pillow diffusers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    ViltProcessor, ViltForQuestionAnswering,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    ")\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Captioning with BLIP\n",
    "\n",
    "Generate descriptive captions for images using the BLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model for image captioning\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load sample image\n",
    "url = \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=400\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Generate caption\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs, max_length=50, num_beams=5)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Generated Caption: {caption}\", fontsize=14, pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Question Answering with ViLT\n",
    "\n",
    "Answer questions about images using Vision-and-Language Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ViLT model for VQA\n",
    "vqa_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "vqa_model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Use the same image from before\n",
    "questions = [\n",
    "    \"What animal is in the image?\",\n",
    "    \"What color is the animal?\",\n",
    "    \"Is the animal indoors or outdoors?\",\n",
    "    \"What is the animal doing?\"\n",
    "]\n",
    "\n",
    "print(\"Visual Question Answering Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for question in questions:\n",
    "    # Prepare inputs\n",
    "    encoding = vqa_processor(image, question, return_tensors=\"pt\")\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = vqa_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    answer = vqa_model.config.id2label[idx]\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print()\n",
    "\n",
    "# Display the image again for reference\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Image for VQA\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text-to-Image Generation with Stable Diffusion\n",
    "\n",
    "Generate images from text descriptions using Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Generate images from different prompts\n",
    "prompts = [\n",
    "    \"A serene mountain landscape at sunset with a crystal clear lake\",\n",
    "    \"A futuristic city with flying cars and neon lights\",\n",
    "    \"A cozy library with books floating magically in the air\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(prompts), figsize=(15, 5))\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating image for: {prompt}\")\n",
    "    \n",
    "    # Generate image\n",
    "    image = pipe(prompt, num_inference_steps=20, guidance_scale=7.5).images[0]\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(prompt[:30] + \"...\", fontsize=10, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image-Text Similarity with CLIP\n",
    "\n",
    "Measure similarity between images and text descriptions using CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load multiple images for comparison\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d?w=300\",  # Dog\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=300\",  # Cat\n",
    "    \"https://images.unsplash.com/photo-1449824913935-59a10b8d2000?w=300\",  # Car\n",
    "]\n",
    "\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]\n",
    "image_labels = [\"Dog\", \"Cat\", \"Car\"]\n",
    "\n",
    "# Text descriptions to compare\n",
    "text_queries = [\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a cat\",\n",
    "    \"a photo of a car\",\n",
    "    \"a photo of an animal\",\n",
    "    \"a photo of a vehicle\"\n",
    "]\n",
    "\n",
    "# Process inputs\n",
    "inputs = clip_processor(\n",
    "    text=text_queries,\n",
    "    images=images,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Get features\n",
    "outputs = clip_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Show images\n",
    "for i, (image, label) in enumerate(zip(images, image_labels)):\n",
    "    row, col = i // 2, i % 2\n",
    "    if i < 3:\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis('off')\n",
    "        axes[row, col].set_title(f\"Image {i+1}: {label}\", fontsize=12)\n",
    "\n",
    "# Show similarity heatmap\n",
    "similarity_matrix = logits_per_image.detach().numpy()\n",
    "im = axes[1, 1].imshow(similarity_matrix, cmap='viridis', aspect='auto')\n",
    "axes[1, 1].set_xticks(range(len(text_queries)))\n",
    "axes[1, 1].set_xticklabels([q[:20] + \"...\" if len(q) > 20 else q for q in text_queries], rotation=45)\n",
    "axes[1, 1].set_yticks(range(len(image_labels)))\n",
    "axes[1, 1].set_yticklabels(image_labels)\n",
    "axes[1, 1].set_title(\"Image-Text Similarity Scores\")\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top matches for each image\n",
    "print(\"\\nTop text matches for each image:\")\n",
    "print(\"=\" * 40)\n",
    "for i, label in enumerate(image_labels):\n",
    "    top_match_idx = probs[i].argmax().item()\n",
    "    confidence = probs[i][top_match_idx].item() * 100\n",
    "    print(f\"{label}: '{text_queries[top_match_idx]}' ({confidence:.1f}% confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Understanding with LayoutLMv3\n",
    "\n",
    "Extract and classify information from document images using LayoutLMv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LayoutLMv3 for document understanding\n",
    "layout_processor = LayoutLMv3Processor.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\", \n",
    "    apply_ocr=False\n",
    ")\n",
    "layout_model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\"\n",
    ")\n",
    "\n",
    "# Create a simple document-like image with text\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Create a sample invoice-like document\n",
    "img = Image.new('RGB', (800, 600), color='white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Try to use a default font, fallback to default if not available\n",
    "try:\n",
    "    font_large = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "    font_medium = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    font_small = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "except:\n",
    "    font_large = ImageFont.load_default()\n",
    "    font_medium = ImageFont.load_default()\n",
    "    font_small = ImageFont.load_default()\n",
    "\n",
    "# Add document content\n",
    "draw.text((50, 50), \"INVOICE\", fill='black', font=font_large)\n",
    "draw.text((50, 100), \"Invoice #: INV-2024-001\", fill='black', font=font_medium)\n",
    "draw.text((50, 130), \"Date: January 15, 2024\", fill='black', font=font_medium)\n",
    "draw.text((50, 180), \"Bill To:\", fill='black', font=font_medium)\n",
    "draw.text((50, 210), \"John Doe\", fill='black', font=font_medium)\n",
    "draw.text((50, 240), \"123 Main Street\", fill='black', font=font_medium)\n",
    "draw.text((50, 270), \"New York, NY 10001\", fill='black', font=font_medium)\n",
    "draw.text((50, 320), \"Description: Web Development Services\", fill='black', font=font_medium)\n",
    "draw.text((50, 350), \"Amount: $2,500.00\", fill='black', font=font_medium)\n",
    "draw.text((50, 400), \"Total: $2,500.00\", fill='black', font=font_large)\n",
    "\n",
    "# For this example, we'll simulate OCR data\n",
    "# In practice, you would use an OCR service to extract text and bounding boxes\n",
    "words = [\n",
    "    \"INVOICE\", \"Invoice\", \"#:\", \"INV-2024-001\", \"Date:\", \"January\", \"15,\", \"2024\",\n",
    "    \"Bill\", \"To:\", \"John\", \"Doe\", \"123\", \"Main\", \"Street\", \"New\", \"York,\", \"NY\", \"10001\",\n",
    "    \"Description:\", \"Web\", \"Development\", \"Services\", \"Amount:\", \"$2,500.00\",\n",
    "    \"Total:\", \"$2,500.00\"\n",
    "]\n",
    "\n",
    "# Simulate bounding boxes (normalized coordinates)\n",
    "boxes = [\n",
    "    [50, 50, 150, 74], [50, 100, 100, 116], [101, 100, 115, 116], [116, 100, 220, 116],\n",
    "    [50, 130, 85, 146], [86, 130, 140, 146], [141, 130, 165, 146], [166, 130, 210, 146],\n",
    "    [50, 180, 85, 196], [86, 180, 115, 196], [50, 210, 90, 226], [91, 210, 130, 226],\n",
    "    [50, 240, 75, 256], [76, 240, 115, 256], [116, 240, 160, 256], [50, 270, 85, 286],\n",
    "    [86, 270, 130, 286], [131, 270, 155, 286], [156, 270, 200, 286],\n",
    "    [50, 320, 120, 336], [121, 320, 155, 336], [156, 320, 250, 336], [251, 320, 310, 336],\n",
    "    [50, 350, 105, 366], [106, 350, 180, 366], [50, 400, 95, 416], [96, 400, 170, 416]\n",
    "]\n",
    "\n",
    "# Normalize bounding boxes to 0-1000 scale (LayoutLM expects this)\n",
    "normalized_boxes = []\n",
    "for box in boxes:\n",
    "    normalized_box = [\n",
    "        int(box[0] * 1000 / 800),  # x1\n",
    "        int(box[1] * 1000 / 600),  # y1\n",
    "        int(box[2] * 1000 / 800),  # x2\n",
    "        int(box[3] * 1000 / 600)   # y2\n",
    "    ]\n",
    "    normalized_boxes.append(normalized_box)\n",
    "\n",
    "# Display the document\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Document for Layout Analysis\", fontsize=14, pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"Document Analysis Complete!\")\n",
    "print(f\"Extracted {len(words)} words from the document.\")\n",
    "print(\"\\nSample extracted words:\")\n",
    "for i, (word, box) in enumerate(zip(words[:10], normalized_boxes[:10])):\n",
    "    print(f\"{i+1:2d}. '{word}' at position {box}\")\n",
    "\n",
    "print(\"\\nThis example demonstrates document understanding capabilities.\")\n",
    "print(\"In production, you would:\")\n",
    "print(\"1. Use OCR to extract text and bounding boxes\")\n",
    "print(\"2. Fine-tune LayoutLMv3 for your specific document types\")\n",
    "print(\"3. Extract structured information like dates, amounts, addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated five key multimodal AI capabilities:\n",
    "\n",
    "1. **Image Captioning**: Automatically generating descriptive text for images\n",
    "2. **Visual Question Answering**: Answering natural language questions about image content\n",
    "3. **Text-to-Image Generation**: Creating images from textual descriptions\n",
    "4. **Image-Text Similarity**: Measuring semantic similarity between visual and textual content\n",
    "5. **Document Understanding**: Extracting structured information from document images\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **BLIP** excels at image captioning and conditional text generation\n",
    "- **ViLT** enables sophisticated visual question answering\n",
    "- **Stable Diffusion** generates high-quality images from text prompts\n",
    "- **CLIP** provides powerful image-text similarity matching\n",
    "- **LayoutLMv3** specializes in understanding document structure and layout\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Content Moderation**: Automatically analyzing and describing user-uploaded images\n",
    "- **Accessibility**: Generating alt-text for images to assist visually impaired users\n",
    "- **Search & Retrieval**: Finding images based on natural language descriptions\n",
    "- **Creative Tools**: Generating artwork and designs from text descriptions\n",
    "- **Document Processing**: Automating invoice processing, form extraction, and document analysis\n",
    "\n",
    "These multimodal models represent the cutting edge of AI, bridging the gap between visual and textual understanding to enable more sophisticated and intuitive AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}