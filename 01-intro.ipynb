{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to HuggingFace ðŸ¤— Transformers\n",
    "\n",
    "This notebook introduces the basics of using HuggingFace Transformers library for natural language processing tasks.\n",
    "\n",
    "## What you'll learn:\n",
    "- Installing and setting up HuggingFace Transformers\n",
    "- Loading pre-trained models and tokenizers\n",
    "- Basic text classification\n",
    "- Text generation\n",
    "- Working with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch torchvision torchaudio accelerate\n",
    "!pip install sentencepiece  # For some tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Pipelines (Easiest Way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification Pipeline\n",
    "classifier = pipeline(\"text-classification\")\n",
    "result = classifier(\"I love this movie! It's amazing.\")\n",
    "print(f\"Classification result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with specific model\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"I'm so happy today!\",\n",
    "    \"This is terrible.\",\n",
    "    \"The weather is okay, I guess.\"\n",
    "]\n",
    "\n",
    "results = sentiment_pipeline(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "generated = generator(\n",
    "    prompt,\n",
    "    max_length=100,\n",
    "    num_return_sequences=2,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "for i, gen in enumerate(generated):\n",
    "    print(f\"Generation {i+1}:\")\n",
    "    print(gen['generated_text'])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "text = \"Hello, how are you today?\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Input IDs shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {tokens['input_ids']}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First token embedding (first 10 dimensions): {embeddings[0, 0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Question Answering Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "The Transformer architecture was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n",
    "It revolutionized natural language processing by using self-attention mechanisms instead of recurrent or \n",
    "convolutional layers. The model consists of an encoder and decoder, each made up of multiple layers.\n",
    "BERT, GPT, and T5 are all based on the Transformer architecture.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was the Transformer architecture introduced?\",\n",
    "    \"What paper introduced the Transformer?\",\n",
    "    \"What models are based on the Transformer architecture?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Named Entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"- {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Different Model Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models for sentiment analysis\n",
    "models_to_compare = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "]\n",
    "\n",
    "test_text = \"I'm not sure if I like this new update.\"\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    try:\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "        result = classifier(test_text)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Result: {result[0]}\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing for efficiency\n",
    "texts = [\n",
    "    \"This movie is fantastic!\",\n",
    "    \"I didn't enjoy this film at all.\",\n",
    "    \"The plot was confusing but the acting was good.\",\n",
    "    \"Best movie I've seen this year!\",\n",
    "    \"Boring and predictable.\"\n",
    "]\n",
    "\n",
    "# Process all texts at once\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "results = classifier(texts)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'sentiment': [r['label'] for r in results],\n",
    "    'confidence': [r['score'] for r in results]\n",
    "})\n",
    "\n",
    "print(\"Batch Processing Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Device Management (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Create pipeline with specific device\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
    "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "print(f\"\\nPipeline running on: {'GPU' if device >= 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following notebooks, we'll explore:\n",
    "- Loading and preprocessing datasets\n",
    "- Text classification fine-tuning\n",
    "- Token classification (NER) fine-tuning\n",
    "- Question answering fine-tuning\n",
    "- Text generation fine-tuning\n",
    "- Advanced techniques like LoRA\n",
    "- Model evaluation and deployment\n",
    "\n",
    "This notebook provided a foundation for working with HuggingFace Transformers. Practice with different models and tasks to get comfortable with the library!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
