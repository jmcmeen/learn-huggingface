{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Fine-Tuning Techniques\n",
    "\n",
    "This notebook covers advanced fine-tuning techniques including:\n",
    "- LoRA (Low-Rank Adaptation)\n",
    "- QLoRA (Quantized LoRA)\n",
    "- Custom training loops with gradient accumulation\n",
    "- Multi-GPU training with DeepSpeed\n",
    "- Advanced optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets accelerate peft bitsandbytes deepspeed wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LoRA Fine-Tuning\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by adding trainable low-rank matrices to existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # Alpha parameter\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]  # Target attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Percentage of trainable params: {100 * model.num_parameters(only_trainable=True) / model.num_parameters():.2f}%\")\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\n",
    "    \"This movie is amazing!\",\n",
    "    \"I hate this film.\",\n",
    "    \"Great acting and storyline.\",\n",
    "    \"Boring and predictable.\",\n",
    "    \"Best movie ever!\",\n",
    "    \"Waste of time.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Tokenize\n",
    "tokenized = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "    \"labels\": labels\n",
    "})\n",
    "\n",
    "# Training arguments optimized for LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=3e-4,  # Higher LR for LoRA\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA weights (much smaller than full model)\n",
    "model.save_pretrained(\"./lora-weights\")\n",
    "print(\"LoRA fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QLoRA with 4-bit Quantization\n",
    "\n",
    "QLoRA combines quantization with LoRA for even more memory-efficient fine-tuning of large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,  # Nested quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normal float 4-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Computation dtype\n",
    ")\n",
    "\n",
    "# Load quantized model (using a larger model to show benefits)\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\",  # Automatic device mapping\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA for quantized model\n",
    "qlora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"c_attn\",  # Attention projection\n",
    "        \"c_proj\",  # Output projection\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "# Apply QLoRA\n",
    "model = get_peft_model(model, qlora_config)\n",
    "\n",
    "print(f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "\n",
    "# Training with QLoRA (using same dataset)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,  # Smaller batch size\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # Mixed precision\n",
    "    optim=\"adamw_8bit\",  # 8-bit optimizer\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_pin_memory=False,  # Reduce memory usage\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./qlora-weights\")\n",
    "print(\"QLoRA fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Training Loop with Advanced Features\n",
    "\n",
    "Implementing a custom training loop with gradient accumulation, learning rate scheduling, and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup, AdamW\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize accelerator for mixed precision and distributed training\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=\"fp16\",  # or \"bf16\"\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Initialize Weights & Biases for logging\n",
    "if accelerator.is_main_process:\n",
    "    wandb.init(\n",
    "        project=\"advanced-fine-tuning\",\n",
    "        name=\"custom-training-loop\",\n",
    "        config={\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 8,\n",
    "            \"epochs\": 3,\n",
    "            \"warmup_steps\": 100,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Model setup (using standard model for simplicity)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Create larger dataset for demonstration\n",
    "texts = texts * 20  # Repeat for more data\n",
    "labels = labels * 20\n",
    "\n",
    "tokenized = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "    \"labels\": labels\n",
    "})\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer with weight decay and different learning rates\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr\": 2e-5,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"lr\": 2e-5,\n",
    "    },\n",
    "    {\n",
    "        \"params\": model.classifier.parameters(),\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr\": 1e-4,  # Higher LR for classifier\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "num_warmup_steps = min(100, num_training_steps // 10)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Prepare for accelerated training\n",
    "model, optimizer, dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, dataloader, scheduler\n",
    ")\n",
    "\n",
    "# Custom training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Forward pass\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Logging\n",
    "        current_loss = loss.detach().float()\n",
    "        epoch_loss += current_loss\n",
    "        total_loss += current_loss\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{current_loss:.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        if accelerator.is_main_process and global_step % 10 == 0:\n",
    "            wandb.log({\n",
    "                \"train_loss\": current_loss,\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step\n",
    "            })\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    accelerator.print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(f\"./custom-checkpoint-epoch-{epoch+1}\")\n",
    "\n",
    "accelerator.print(\"Custom training completed!\")\n",
    "if accelerator.is_main_process:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DeepSpeed Integration for Large Model Training\n",
    "\n",
    "Using DeepSpeed ZeRO for memory-efficient training of large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepSpeed configuration file\n",
    "import json\n",
    "\n",
    "deepspeed_config = {\n",
    "    \"train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 2e-5,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": 2e-5,\n",
    "            \"warmup_num_steps\": 100\n",
    "        }\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,  # ZeRO stage 2: Optimizer state partitioning\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",  # Offload optimizer to CPU\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": True,\n",
    "        \"cpu_checkpointing\": True,\n",
    "        \"contiguous_memory_optimization\": False,\n",
    "        \"number_checkpoints\": 4,\n",
    "        \"synchronize_checkpoint_boundary\": False,\n",
    "        \"profile\": False\n",
    "    },\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open(\"deepspeed_config.json\", \"w\") as f:\n",
    "    json.dump(deepspeed_config, f, indent=2)\n",
    "\n",
    "print(\"DeepSpeed configuration saved to deepspeed_config.json\")\n",
    "\n",
    "# Training with DeepSpeed (using Trainer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepspeed-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    deepspeed=\"deepspeed_config.json\",  # DeepSpeed config\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Model (can be much larger with DeepSpeed)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for memory savings\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting DeepSpeed training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nDeepSpeed training completed!\")\n",
    "print(f\"Model saved to: {training_args.output_dir}\")\n",
    "\n",
    "# Example command to run with DeepSpeed from command line:\n",
    "print(\"\\nTo run with DeepSpeed from command line:\")\n",
    "print(\"deepspeed --num_gpus=1 train_script.py --deepspeed deepspeed_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Optimization Strategies\n",
    "\n",
    "Implementing sophisticated optimization techniques including layer-wise learning rates and adaptive training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "import math\n",
    "\n",
    "class LayerWiseLROptimizer:\n",
    "    \"\"\"Custom optimizer with layer-wise learning rate decay\"\"\"\n",
    "    \n",
    "    def __init__(self, model, base_lr=2e-5, decay_rate=0.9):\n",
    "        self.model = model\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "    def get_optimizer_groups(self):\n",
    "        \"\"\"Create optimizer groups with layer-wise learning rates\"\"\"\n",
    "        # Get all transformer layers\n",
    "        if hasattr(self.model, 'distilbert'):\n",
    "            layers = self.model.distilbert.transformer.layer\n",
    "            num_layers = len(layers)\n",
    "        elif hasattr(self.model, 'bert'):\n",
    "            layers = self.model.bert.encoder.layer\n",
    "            num_layers = len(layers)\n",
    "        else:\n",
    "            # Fallback for other architectures\n",
    "            return [{'params': self.model.parameters(), 'lr': self.base_lr}]\n",
    "        \n",
    "        optimizer_groups = []\n",
    "        \n",
    "        # Different learning rates for different layers\n",
    "        for i, layer in enumerate(layers):\n",
    "            # Lower layers get lower learning rates\n",
    "            layer_lr = self.base_lr * (self.decay_rate ** (num_layers - i - 1))\n",
    "            \n",
    "            optimizer_groups.append({\n",
    "                'params': layer.parameters(),\n",
    "                'lr': layer_lr,\n",
    "                'weight_decay': 0.01\n",
    "            })\n",
    "        \n",
    "        # Higher learning rate for classifier\n",
    "        optimizer_groups.append({\n",
    "            'params': self.model.classifier.parameters(),\n",
    "            'lr': self.base_lr * 2,\n",
    "            'weight_decay': 0.01\n",
    "        })\n",
    "        \n",
    "        # Embeddings with lower learning rate\n",
    "        if hasattr(self.model, 'distilbert'):\n",
    "            optimizer_groups.append({\n",
    "                'params': self.model.distilbert.embeddings.parameters(),\n",
    "                'lr': self.base_lr * 0.1,\n",
    "                'weight_decay': 0.0\n",
    "            })\n",
    "        \n",
    "        return optimizer_groups\n",
    "\n",
    "class AdaptiveTrainer:\n",
    "    \"\"\"Trainer with adaptive strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.lr_reduction_factor = 0.5\n",
    "        self.patience = 3\n",
    "        \n",
    "    def adaptive_batch_size(self, initial_batch_size=8, max_batch_size=32):\n",
    "        \"\"\"Dynamically adjust batch size based on GPU memory\"\"\"\n",
    "        batch_size = initial_batch_size\n",
    "        \n",
    "        while batch_size <= max_batch_size:\n",
    "            try:\n",
    "                # Test with dummy batch\n",
    "                dummy_input = torch.randint(0, 1000, (batch_size, 128)).to(self.device)\n",
    "                dummy_mask = torch.ones(batch_size, 128).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(dummy_input, attention_mask=dummy_mask)\n",
    "                \n",
    "                batch_size *= 2  # Try larger batch size\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    batch_size = max(batch_size // 2, 1)\n",
    "                    break\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        print(f\"Optimal batch size: {batch_size}\")\n",
    "        return batch_size\n",
    "    \n",
    "    def train_with_adaptive_strategies(self, dataset, num_epochs=3):\n",
    "        \"\"\"Train with multiple adaptive strategies\"\"\"\n",
    "        \n",
    "        # Setup layer-wise learning rates\n",
    "        lr_optimizer = LayerWiseLROptimizer(self.model)\n",
    "        optimizer_groups = lr_optimizer.get_optimizer_groups()\n",
    "        optimizer = AdamW(optimizer_groups, eps=1e-8)\n",
    "        \n",
    "        # Adaptive batch size\n",
    "        optimal_batch_size = self.adaptive_batch_size()\n",
    "        dataloader = DataLoader(dataset, batch_size=optimal_batch_size, shuffle=True)\n",
    "        \n",
    "        # Polynomial decay scheduler\n",
    "        num_training_steps = num_epochs * len(dataloader)\n",
    "        scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=min(100, num_training_steps // 10),\n",
    "            num_training_steps=num_training_steps,\n",
    "            power=2.0\n",
    "        )\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping with adaptive norm\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update metrics\n",
    "                current_loss = loss.item()\n",
    "                total_loss += current_loss\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'grad_norm': f'{grad_norm:.3f}',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                })\n",
    "            \n",
    "            # Epoch statistics\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Adaptive learning rate reduction\n",
    "            if avg_loss < self.best_loss:\n",
    "                self.best_loss = avg_loss\n",
    "                self.patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), \"best_model.pt\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                \n",
    "                if self.patience_counter >= self.patience:\n",
    "                    print(f\"Reducing learning rate by factor {self.lr_reduction_factor}\")\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] *= self.lr_reduction_factor\n",
    "                    self.patience_counter = 0\n",
    "            \n",
    "            # Memory cleanup\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"Adaptive training completed!\")\n",
    "        return self.model\n",
    "\n",
    "# Initialize and run adaptive training\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "adaptive_trainer = AdaptiveTrainer(model, tokenizer)\n",
    "trained_model = adaptive_trainer.train_with_adaptive_strategies(dataset, num_epochs=2)\n",
    "\n",
    "print(\"\\nAdvanced optimization training completed!\")\n",
    "print(\"Key features used:\")\n",
    "print(\"- Layer-wise learning rates\")\n",
    "print(\"- Adaptive batch sizing\")\n",
    "print(\"- Polynomial decay scheduling\")\n",
    "print(\"- Adaptive learning rate reduction\")\n",
    "print(\"- Gradient norm monitoring\")\n",
    "print(\"- Memory optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered five advanced fine-tuning techniques:\n",
    "\n",
    "1. **LoRA**: Parameter-efficient fine-tuning with low-rank adaptation\n",
    "2. **QLoRA**: Memory-efficient training combining quantization and LoRA\n",
    "3. **Custom Training Loop**: Full control with gradient accumulation and mixed precision\n",
    "4. **DeepSpeed Integration**: Scalable training for large models with ZeRO optimization\n",
    "5. **Advanced Optimization**: Layer-wise learning rates and adaptive strategies\n",
    "\n",
    "Each technique addresses different challenges in fine-tuning:\n",
    "- **Memory efficiency**: QLoRA, DeepSpeed ZeRO\n",
    "- **Parameter efficiency**: LoRA, QLoRA\n",
    "- **Training stability**: Custom loops with proper scheduling\n",
    "- **Scalability**: DeepSpeed for multi-GPU training\n",
    "- **Optimization**: Advanced learning rate strategies\n",
    "\n",
    "Choose the appropriate technique based on your model size, available hardware, and training requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}