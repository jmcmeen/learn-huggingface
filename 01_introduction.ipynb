{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7BHPTMH9NJ3"
      },
      "source": [
        "# Introduction to Hugging Face ðŸ¤—\n",
        "\n",
        "Welcome to the world of Hugging Face! This notebook will introduce you to the basics of the Hugging Face ecosystem and show you how to get started with pre-trained models.\n",
        "\n",
        "## What is Hugging Face?\n",
        "\n",
        "Hugging Face is a platform and library that provides:\n",
        "- **Pre-trained models** for NLP, computer vision, and audio\n",
        "- **Easy-to-use APIs** for common ML tasks\n",
        "- **Community hub** for sharing models and datasets\n",
        "- **Tools** for training and deploying models\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you'll know how to:\n",
        "1. Install and import Hugging Face libraries\n",
        "2. Use pre-trained models with pipelines\n",
        "3. Understand the Hugging Face Hub\n",
        "4. Perform basic NLP tasks\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFrky9J09NJ4"
      },
      "source": [
        "## 1. Installation and Setup\n",
        "\n",
        "First, let's make sure we have the necessary libraries installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FrJbn6b9NJ4"
      },
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install transformers torch datasets tokenizers\n",
        "\n",
        "# Import essential libraries\n",
        "import transformers\n",
        "import torch\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYRnUaMN9NJ4"
      },
      "source": [
        "## 2. Your First Hugging Face Pipeline\n",
        "\n",
        "Pipelines are the easiest way to use pre-trained models. They handle tokenization, model inference, and post-processing automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSZDuVdF9NJ4"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Test it with some examples\n",
        "texts = [\n",
        "    \"I love learning about AI and machine learning!\",\n",
        "    \"This movie was terrible and boring.\",\n",
        "    \"The weather is okay today.\"\n",
        "]\n",
        "\n",
        "results = sentiment_pipeline(texts)\n",
        "\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.3f})\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZd6rziy9NJ4"
      },
      "source": [
        "## 3. Different Types of Pipelines\n",
        "\n",
        "Hugging Face supports many different tasks through pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6thnFagt9NJ4"
      },
      "outputs": [],
      "source": [
        "# Text Generation\n",
        "print(\"=== TEXT GENERATION ===\")\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "result = generator(\"Artificial intelligence is\", max_length=50, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zIJC4Gu9NJ5"
      },
      "outputs": [],
      "source": [
        "# Question Answering\n",
        "print(\"=== QUESTION ANSWERING ===\")\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "\n",
        "context = \"\"\"\n",
        "Hugging Face is a company that develops tools for building applications using machine learning.\n",
        "The company was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf.\n",
        "Their main product is the Transformers library, which provides thousands of pre-trained models.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"When was Hugging Face founded?\",\n",
        "    \"What is the main product of Hugging Face?\",\n",
        "    \"Who are the founders?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = qa_pipeline(question=question, context=context)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer['answer']} (confidence: {answer['score']:.3f})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e6XCSjW9NJ5"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition (NER)\n",
        "print(\"=== NAMED ENTITY RECOGNITION ===\")\n",
        "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
        "entities = ner_pipeline(text)\n",
        "\n",
        "print(f\"Text: {text}\\n\")\n",
        "print(\"Entities found:\")\n",
        "for entity in entities:\n",
        "    print(f\"- {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN1aBXFV9NJ5"
      },
      "source": [
        "## 4. Exploring the Hugging Face Hub\n",
        "\n",
        "The Hugging Face Hub hosts thousands of models. Let's see how to explore and use different models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_FNDAeO9NJ5"
      },
      "outputs": [],
      "source": [
        "# List available pipeline tasks\n",
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "\n",
        "print(\"Available pipeline tasks:\")\n",
        "for i, task in enumerate(SUPPORTED_TASKS.keys(), 1):\n",
        "    print(f\"{i:2d}. {task}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GP_vd049NJ5"
      },
      "outputs": [],
      "source": [
        "# Using a specific model from the Hub\n",
        "print(\"=== USING A SPECIFIC MODEL ===\")\n",
        "\n",
        "# Use a different sentiment analysis model\n",
        "specific_sentiment = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        ")\n",
        "\n",
        "tweet_texts = [\n",
        "    \"Just finished my first machine learning project! ðŸš€\",\n",
        "    \"Traffic is so bad today ðŸ˜¤\",\n",
        "    \"Reading a good book by the fireplace\"\n",
        "]\n",
        "\n",
        "for text in tweet_texts:\n",
        "    result = specific_sentiment(text)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Sentiment: {result[0]['label']} (score: {result[0]['score']:.3f})\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDlB1RRV9NJ5"
      },
      "source": [
        "## 5. Understanding Model Information\n",
        "\n",
        "Let's learn how to get information about models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXfI2BvM9NJ5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load a model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
        "print(f\"Max sequence length: {tokenizer.model_max_length:,}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model size in MB: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfIvQlO39NJ5"
      },
      "source": [
        "## 6. Working with Tokenizers\n",
        "\n",
        "Tokenizers convert text into numbers that models can understand:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGpEUJUe9NJ5"
      },
      "outputs": [],
      "source": [
        "# Example text\n",
        "text = \"Hello, Hugging Face! Let's tokenize this text.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.encode(text)\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"Decoded text: {decoded_text}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9sNHDzU9NJ5"
      },
      "source": [
        "## 7. Batch Processing\n",
        "\n",
        "Process multiple texts efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbOL2qED9NJ5"
      },
      "outputs": [],
      "source": [
        "# Batch processing with sentiment analysis\n",
        "batch_texts = [\n",
        "    \"I love this tutorial!\",\n",
        "    \"This is confusing.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"I'm not sure about this.\",\n",
        "    \"Great explanation, thank you!\"\n",
        "]\n",
        "\n",
        "print(\"=== BATCH SENTIMENT ANALYSIS ===\")\n",
        "batch_results = sentiment_pipeline(batch_texts)\n",
        "\n",
        "for text, result in zip(batch_texts, batch_results):\n",
        "    label = result['label']\n",
        "    score = result['score']\n",
        "    emoji = \"ðŸ˜Š\" if label == \"POSITIVE\" else \"ðŸ˜ž\"\n",
        "    print(f\"{emoji} '{text}' â†’ {label} ({score:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srtV1ZTU9NJ5"
      },
      "source": [
        "## 8. Error Handling and Best Practices\n",
        "\n",
        "Let's learn about common issues and how to handle them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7keWDMyN9NJ5"
      },
      "outputs": [],
      "source": [
        "# Handling long texts\n",
        "long_text = \"This is a very long text. \" * 100  # Repeat to make it long\n",
        "\n",
        "print(f\"Text length: {len(long_text)} characters\")\n",
        "print(f\"Token count: {len(tokenizer.tokenize(long_text))} tokens\")\n",
        "print(f\"Max model length: {tokenizer.model_max_length} tokens\")\n",
        "\n",
        "# Truncate if necessary\n",
        "truncated_encoding = tokenizer(\n",
        "    long_text,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"Truncated token count: {truncated_encoding['input_ids'].shape[1]} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJtBud719NJ6"
      },
      "source": [
        "## 9. Saving and Loading Models\n",
        "\n",
        "Learn how to save models locally for offline use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPAU2OLC9NJ6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Save model and tokenizer locally\n",
        "local_model_path = \"./local_distilbert\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(local_model_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(local_model_path)\n",
        "tokenizer.save_pretrained(local_model_path)\n",
        "\n",
        "print(f\"Model saved to: {local_model_path}\")\n",
        "print(f\"Files in directory: {os.listdir(local_model_path)}\")\n",
        "\n",
        "# Load from local path\n",
        "local_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "local_model = AutoModel.from_pretrained(local_model_path)\n",
        "\n",
        "print(\"\\nSuccessfully loaded model from local directory!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HS02O8h9NJ6"
      },
      "source": [
        "## 10. Performance Tips and Device Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mnqteDv9NJ6"
      },
      "outputs": [],
      "source": [
        "# Check available devices\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "# Create pipeline with specific device\n",
        "sentiment_gpu = pipeline(\"sentiment-analysis\", device=0 if torch.cuda.is_available() else -1)\n",
        "print(f\"\\nPipeline created on device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoseaxr09NJ6"
      },
      "source": [
        "## ðŸŽ‰ Congratulations!\n",
        "\n",
        "You've completed your first Hugging Face tutorial! Here's what you've learned:\n",
        "\n",
        "âœ… **Installation and setup** of Hugging Face libraries  \n",
        "âœ… **Using pipelines** for quick model inference  \n",
        "âœ… **Different NLP tasks** (sentiment analysis, QA, NER, text generation)  \n",
        "âœ… **Working with tokenizers** and understanding tokenization  \n",
        "âœ… **Batch processing** for efficiency  \n",
        "âœ… **Model management** (saving and loading)  \n",
        "âœ… **Best practices** for handling long texts and devices  \n",
        "\n",
        "## ðŸš€ Next Steps\n",
        "\n",
        "**Explore more models** on the [Hugging Face Hub](https://huggingface.co/models)\n",
        "**Try different tasks** with various pipeline types in the blank cell below\n",
        "**Move to the next notebook**: `02_tokenizers.ipynb`\n",
        "\n",
        "Happy learning! ðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD6qKQKM9NJ6"
      },
      "outputs": [],
      "source": [
        "# Dream here!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}