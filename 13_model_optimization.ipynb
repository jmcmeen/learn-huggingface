{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated five key model optimization techniques:\n\n1. **Quantization**: Reduces precision to int8, providing significant size and speed improvements with minimal accuracy loss\n2. **Pruning**: Removes less important weights, creating sparse models that can be further optimized\n3. **Knowledge Distillation**: Creates smaller student models that mimic larger teacher models\n4. **ONNX Export**: Enables optimized inference across different platforms and runtimes\n5. **Comprehensive Comparison**: Understanding trade-offs between different optimization approaches\n\n### Best Practices:\n- Start with quantization for quick wins\n- Use ONNX for production deployments\n- Combine techniques for maximum optimization\n- Always validate that accuracy remains acceptable\n- Profile your specific use case to choose the best approach\n\n### Next Steps:\n- Experiment with different quantization methods (static quantization, QAT)\n- Explore structured pruning techniques\n- Try progressive knowledge distillation\n- Implement model compression pipelines for your specific models\n\n### Production Recommendations:\n1. **For CPU inference**: Quantization + ONNX Runtime\n2. **For edge devices**: Knowledge Distillation + Quantization\n3. **For cloud deployment**: ONNX Runtime with optimized providers\n4. **For research**: Combine multiple techniques based on specific requirements"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all optimization techniques\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect all performance metrics\n",
    "optimization_summary = {\n",
    "    'Technique': ['Original', 'Quantization', 'Pruning (20%)', 'Knowledge Distillation'],\n",
    "    'Model Size (MB)': [original_size, quantized_size, original_size, student_size],\n",
    "    'Inference Time (s)': [original_time, quantized_time, results[1]['time'], student_time],\n",
    "    'Size Reduction (%)': [0, (1-quantized_size/original_size)*100, results[1]['sparsity'], (1-student_size/original_size)*100],\n",
    "    'Speed Improvement (%)': [0, (original_time/quantized_time-1)*100, (original_time/results[1]['time']-1)*100, (original_time/student_time-1)*100],\n",
    "    'Accuracy Impact': ['Baseline', 'Minimal', 'Low', 'Moderate']\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame(optimization_summary)\n",
    "print(\"Model Optimization Comparison Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Model size comparison\n",
    "techniques = df['Technique']\n",
    "sizes = df['Model Size (MB)']\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "ax1.bar(techniques, sizes, color=colors)\n",
    "ax1.set_title('Model Size Comparison')\n",
    "ax1.set_ylabel('Size (MB)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Inference time comparison\n",
    "times = df['Inference Time (s)']\n",
    "ax2.bar(techniques, times, color=colors)\n",
    "ax2.set_title('Inference Time Comparison')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Size reduction percentage\n",
    "size_reductions = df['Size Reduction (%)']\n",
    "ax3.bar(techniques, size_reductions, color=colors)\n",
    "ax3.set_title('Size Reduction Percentage')\n",
    "ax3.set_ylabel('Reduction (%)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Speed improvement percentage\n",
    "speed_improvements = df['Speed Improvement (%)']\n",
    "ax4.bar(techniques, speed_improvements, color=colors)\n",
    "ax4.set_title('Speed Improvement Percentage')\n",
    "ax4.set_ylabel('Improvement (%)')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"Key Optimization Insights:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• Quantization: Best balance of size and speed with minimal accuracy loss\")\n",
    "print(\"• Pruning: Reduces model complexity, good for specialized hardware\")\n",
    "print(\"• Knowledge Distillation: Significant size reduction but requires retraining\")\n",
    "print(\"• ONNX Export: Great for production deployment with runtime optimizations\")\n",
    "print(\"\\\\nRecommendation: Combine techniques (e.g., quantized ONNX) for maximum benefit!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Comprehensive Performance Summary\n\nLet's summarize all the optimization techniques and their trade-offs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "# Convert model to ONNX format\n",
    "onnx_model_path = \"./onnx_model\"\n",
    "\n",
    "# Export to ONNX (this will create the ONNX model)\n",
    "print(\"Converting model to ONNX format...\")\n",
    "try:\n",
    "    ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        export=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    # Save ONNX model\n",
    "    ort_model.save_pretrained(onnx_model_path)\n",
    "    tokenizer.save_pretrained(onnx_model_path)\n",
    "    \n",
    "    print(f\"ONNX model saved to {onnx_model_path}\")\n",
    "    \n",
    "    # Load the ONNX model for inference\n",
    "    ort_model = ORTModelForSequenceClassification.from_pretrained(onnx_model_path)\n",
    "    ort_tokenizer = AutoTokenizer.from_pretrained(onnx_model_path)\n",
    "    \n",
    "    # Create pipelines for comparison\n",
    "    pytorch_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=-1  # CPU\n",
    "    )\n",
    "    \n",
    "    onnx_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=ort_model,\n",
    "        tokenizer=ort_tokenizer,\n",
    "        device=-1  # CPU\n",
    "    )\n",
    "    \n",
    "    # Test with sample texts\n",
    "    test_texts = [\n",
    "        \"This product exceeded my expectations!\",\n",
    "        \"The service was disappointing.\",\n",
    "        \"Average quality, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nBenchmarking inference performance...\")\n",
    "    \n",
    "    # Benchmark PyTorch pipeline\n",
    "    start_time = time.time()\n",
    "    pytorch_results = pytorch_pipeline(test_texts)\n",
    "    pytorch_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark ONNX pipeline\n",
    "    start_time = time.time()\n",
    "    onnx_results = onnx_pipeline(test_texts)\n",
    "    onnx_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"PyTorch pipeline: {pytorch_time:.4f} seconds\")\n",
    "    print(f\"ONNX pipeline:    {onnx_time:.4f} seconds\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if onnx_time > 0:\n",
    "        speedup = pytorch_time / onnx_time\n",
    "        print(f\"ONNX speedup: {speedup:.2f}x faster\")\n",
    "    \n",
    "    # Compare predictions\n",
    "    print(\"\\\\nPrediction comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (text, pt_result, onnx_result) in enumerate(zip(test_texts, pytorch_results, onnx_results)):\n",
    "        print(f\"Text {i+1}: {text}\")\n",
    "        print(f\"  PyTorch: {pt_result['label']} ({pt_result['score']:.4f})\")\n",
    "        print(f\"  ONNX:    {onnx_result['label']} ({onnx_result['score']:.4f})\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ONNX conversion failed: {e}\")\n",
    "    print(\"This is common in some environments. The concept remains valid for production use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. ONNX Export for Optimized Inference\n\nONNX (Open Neural Network Exchange) allows models to run on optimized runtimes, providing faster inference across different platforms."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller student model\n",
    "student_config = DistilBertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=384,  # Smaller than original (768)\n",
    "    num_hidden_layers=4,  # Fewer layers than original (6)\n",
    "    num_attention_heads=6,  # Fewer heads than original (12)\n",
    "    intermediate_size=1536,  # Smaller than original (3072)\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "student_model = DistilBertForSequenceClassification(student_config)\n",
    "\n",
    "# Compare model sizes\n",
    "teacher_size = get_model_size(model)\n",
    "student_size = get_model_size(student_model)\n",
    "\n",
    "print(f\"Teacher model size: {teacher_size:.2f} MB\")\n",
    "print(f\"Student model size: {student_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - student_size/teacher_size)*100:.1f}%\")\n",
    "\n",
    "# Simple knowledge distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, true_labels, temperature=4.0, alpha=0.7):\n",
    "    \"\"\"Compute knowledge distillation loss\"\"\"\n",
    "    # Soft targets from teacher\n",
    "    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    soft_prob = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    # Hard targets loss\n",
    "    hard_loss = F.cross_entropy(student_logits, true_labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
    "\n",
    "# Simple training loop for knowledge distillation\n",
    "def train_student_model(student, teacher, train_data, num_epochs=2, batch_size=8):\n",
    "    \"\"\"Train student model with knowledge distillation\"\"\"\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=5e-5)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Simple batching\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch data\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "            labels = torch.stack([item['label'] for item in batch])\n",
    "            \n",
    "            # Get teacher predictions\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "            # Get student predictions\n",
    "            student_outputs = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            student_logits = student_outputs.logits\n",
    "            \n",
    "            # Calculate distillation loss\n",
    "            loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return student\n",
    "\n",
    "print(\"\\nTraining student model with knowledge distillation...\")\n",
    "student_model = train_student_model(student_model, model, train_dataset, num_epochs=2)\n",
    "\n",
    "# Benchmark the student model\n",
    "student_time, _ = benchmark_inference(student_model, tokenizer, sample_texts, num_runs=5)\n",
    "print(f\"\\nStudent model inference time: {student_time:.4f} seconds\")\n",
    "print(f\"Speed improvement over teacher: {(original_time/student_time - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Create synthetic training data for demonstration\n",
    "def create_synthetic_dataset(size=500):\n",
    "    \"\"\"Create synthetic text classification data\"\"\"\n",
    "    positive_templates = [\n",
    "        \"This is amazing!\", \"Great work!\", \"Fantastic product!\",\n",
    "        \"I love this!\", \"Excellent quality!\", \"Outstanding service!\"\n",
    "    ]\n",
    "    negative_templates = [\n",
    "        \"This is terrible!\", \"Poor quality!\", \"I hate this!\",\n",
    "        \"Awful experience!\", \"Complete waste!\", \"Very disappointing!\"\n",
    "    ]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        if i % 2 == 0:\n",
    "            text = np.random.choice(positive_templates)\n",
    "            label = 1  # Positive\n",
    "        else:\n",
    "            text = np.random.choice(negative_templates)\n",
    "            label = 0  # Negative\n",
    "        \n",
    "        # Add some variation\n",
    "        text += f\" Item {i}\"\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return Dataset.from_dict({'text': texts, 'label': labels})\n",
    "\n",
    "# Create smaller datasets for quick training\n",
    "train_dataset = create_synthetic_dataset(200)\n",
    "eval_dataset = create_synthetic_dataset(50)\n",
    "\n",
    "print(f\"Created {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples\")\n",
    "print(f\"Sample data: {train_dataset[0]}\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Knowledge Distillation\n\nKnowledge distillation trains a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model, achieving similar performance with reduced computational requirements."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "# Create a copy of the original model for pruning\n",
    "model_to_prune = copy.deepcopy(model)\n",
    "\n",
    "def apply_global_pruning(model, pruning_ratio=0.2):\n",
    "    \"\"\"Apply global magnitude-based pruning to the model\"\"\"\n",
    "    # Collect all linear layers for pruning\n",
    "    parameters_to_prune = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    # Apply global magnitude pruning\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=pruning_ratio,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    return zero_params / total_params * 100\n",
    "\n",
    "# Apply different pruning ratios and compare\n",
    "pruning_ratios = [0.1, 0.2, 0.3, 0.5]\n",
    "results = []\n",
    "\n",
    "print(\"Pruning Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for ratio in pruning_ratios:\n",
    "    # Create a fresh copy of the model\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Apply pruning\n",
    "    pruned_model = apply_global_pruning(pruned_model, ratio)\n",
    "    \n",
    "    # Calculate sparsity\n",
    "    sparsity = calculate_sparsity(pruned_model)\n",
    "    \n",
    "    # Benchmark performance\n",
    "    pruned_time, _ = benchmark_inference(pruned_model, tokenizer, sample_texts, num_runs=5)\n",
    "    \n",
    "    # Test accuracy\n",
    "    pruned_preds = test_model_outputs(pruned_model, tokenizer, sample_texts)\n",
    "    \n",
    "    # Calculate average prediction difference\n",
    "    avg_diff = 0\n",
    "    for orig, pruned in zip(original_preds, pruned_preds):\n",
    "        avg_diff += np.abs(orig - pruned).mean()\n",
    "    avg_diff /= len(original_preds)\n",
    "    \n",
    "    results.append({\n",
    "        'ratio': ratio,\n",
    "        'sparsity': sparsity,\n",
    "        'time': pruned_time,\n",
    "        'accuracy_diff': avg_diff\n",
    "    })\n",
    "    \n",
    "    print(f\"Pruning Ratio: {ratio:4.1f} | Sparsity: {sparsity:5.1f}% | \"\n",
    "          f\"Time: {pruned_time:6.4f}s | Acc Diff: {avg_diff:.6f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Original Time: {original_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Model Pruning\n\nPruning removes less important weights from the model, reducing its size while maintaining most of its performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {nn.Linear},  # Quantize linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Benchmark quantized model\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "quantized_time, quantized_std = benchmark_inference(quantized_model, tokenizer, sample_texts)\n",
    "\n",
    "print(\"\\nQuantized Model:\")\n",
    "print(f\"  Size: {quantized_size:.2f} MB\")\n",
    "print(f\"  Inference time: {quantized_time:.4f} ± {quantized_std:.4f} seconds\")\n",
    "\n",
    "# Calculate improvements\n",
    "size_reduction = (1 - quantized_size / original_size) * 100\n",
    "speed_improvement = (original_time / quantized_time - 1) * 100\n",
    "\n",
    "print(\"\\nImprovements:\")\n",
    "print(f\"  Size reduction: {size_reduction:.1f}%\")\n",
    "print(f\"  Speed improvement: {speed_improvement:.1f}%\")\n",
    "\n",
    "# Test accuracy comparison\n",
    "def test_model_outputs(model, tokenizer, texts):\n",
    "    \"\"\"Get model predictions\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            pred = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "original_preds = test_model_outputs(model, tokenizer, sample_texts)\n",
    "quantized_preds = test_model_outputs(quantized_model, tokenizer, sample_texts)\n",
    "\n",
    "# Calculate prediction differences\n",
    "print(\"\\nPrediction Accuracy Comparison:\")\n",
    "for i, (orig, quant) in enumerate(zip(original_preds, quantized_preds)):\n",
    "    diff = np.abs(orig - quant).max()\n",
    "    print(f\"  Text {i+1} max difference: {diff:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for sentiment analysis\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create sample text for testing\n",
    "sample_texts = [\n",
    "    \"This movie is absolutely fantastic!\",\n",
    "    \"I really disliked this film.\",\n",
    "    \"The plot was okay, nothing special.\",\n",
    "    \"Outstanding performance by the actors!\"\n",
    "]\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    return (param_size + buffer_size) / 1024 / 1024\n",
    "\n",
    "def benchmark_inference(model, tokenizer, texts, num_runs=10):\n",
    "    \"\"\"Benchmark inference time\"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            for text in texts:\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                _ = model(**inputs)\n",
    "            times.append(time.time() - start_time)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Benchmark original model\n",
    "original_size = get_model_size(model)\n",
    "original_time, original_std = benchmark_inference(model, tokenizer, sample_texts)\n",
    "\n",
    "print(\"Original Model:\")\n",
    "print(f\"  Size: {original_size:.2f} MB\")\n",
    "print(f\"  Inference time: {original_time:.4f} ± {original_std:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Model Quantization\n\nQuantization reduces the precision of model weights from 32-bit floats to 8-bit integers, significantly reducing model size and improving inference speed with minimal accuracy loss."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 13. Model Optimization with Hugging Face\n\nThis notebook covers various techniques for optimizing Hugging Face models for better performance, reduced memory usage, and faster inference.\n\n## Topics Covered:\n1. **Model Quantization** - Reducing model precision for efficiency\n2. **Dynamic Quantization** - Runtime quantization for inference\n3. **Model Pruning** - Removing less important weights\n4. **Knowledge Distillation** - Training smaller models from larger ones\n5. **ONNX Export** - Converting models for optimized inference\n\nLet's start by installing the required packages and importing necessary libraries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchvision torchaudio\n",
    "!pip install torch-audio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install optimum[onnxruntime]\n",
    "!pip install datasets accelerate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}