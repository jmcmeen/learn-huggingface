{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook demonstrated five key model optimization techniques:\n\n1. **Quantization**: Reduces precision to int8, providing significant size and speed improvements with minimal accuracy loss\n2. **Pruning**: Removes less important weights, creating sparse models that can be further optimized\n3. **Knowledge Distillation**: Creates smaller student models that mimic larger teacher models\n4. **ONNX Export**: Enables optimized inference across different platforms and runtimes\n5. **Comprehensive Comparison**: Understanding trade-offs between different optimization approaches\n\n### Best Practices:\n- Start with quantization for quick wins\n- Use ONNX for production deployments\n- Combine techniques for maximum optimization\n- Always validate that accuracy remains acceptable\n- Profile your specific use case to choose the best approach\n\n### Next Steps:\n- Experiment with different quantization methods (static quantization, QAT)\n- Explore structured pruning techniques\n- Try progressive knowledge distillation\n- Implement model compression pipelines for your specific models\n\n### Production Recommendations:\n1. **For CPU inference**: Quantization + ONNX Runtime\n2. **For edge devices**: Knowledge Distillation + Quantization\n3. **For cloud deployment**: ONNX Runtime with optimized providers\n4. **For research**: Combine multiple techniques based on specific requirements",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive comparison of all optimization techniques\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Collect all performance metrics\noptimization_summary = {\n    'Technique': ['Original', 'Quantization', 'Pruning (20%)', 'Knowledge Distillation'],\n    'Model Size (MB)': [original_size, quantized_size, original_size, student_size],\n    'Inference Time (s)': [original_time, quantized_time, results[1]['time'], student_time],\n    'Size Reduction (%)': [0, (1-quantized_size/original_size)*100, results[1]['sparsity'], (1-student_size/original_size)*100],\n    'Speed Improvement (%)': [0, (original_time/quantized_time-1)*100, (original_time/results[1]['time']-1)*100, (original_time/student_time-1)*100],\n    'Accuracy Impact': ['Baseline', 'Minimal', 'Low', 'Moderate']\n}\n\n# Create comparison DataFrame\ndf = pd.DataFrame(optimization_summary)\nprint(\"Model Optimization Comparison Summary:\")\nprint(\"=\" * 70)\nprint(df.to_string(index=False, float_format='%.2f'))\n\n# Create visualization\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# Model size comparison\ntechniques = df['Technique']\nsizes = df['Model Size (MB)']\ncolors = ['blue', 'orange', 'green', 'red']\nax1.bar(techniques, sizes, color=colors)\nax1.set_title('Model Size Comparison')\nax1.set_ylabel('Size (MB)')\nax1.tick_params(axis='x', rotation=45)\n\n# Inference time comparison\ntimes = df['Inference Time (s)']\nax2.bar(techniques, times, color=colors)\nax2.set_title('Inference Time Comparison')\nax2.set_ylabel('Time (seconds)')\nax2.tick_params(axis='x', rotation=45)\n\n# Size reduction percentage\nsize_reductions = df['Size Reduction (%)']\nax3.bar(techniques, size_reductions, color=colors)\nax3.set_title('Size Reduction Percentage')\nax3.set_ylabel('Reduction (%)')\nax3.tick_params(axis='x', rotation=45)\n\n# Speed improvement percentage\nspeed_improvements = df['Speed Improvement (%)']\nax4.bar(techniques, speed_improvements, color=colors)\nax4.set_title('Speed Improvement Percentage')\nax4.set_ylabel('Improvement (%)')\nax4.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\n\" + \"=\"*70)\nprint(\"Key Optimization Insights:\")\nprint(\"-\" * 30)\nprint(\"• Quantization: Best balance of size and speed with minimal accuracy loss\")\nprint(\"• Pruning: Reduces model complexity, good for specialized hardware\")\nprint(\"• Knowledge Distillation: Significant size reduction but requires retraining\")\nprint(\"• ONNX Export: Great for production deployment with runtime optimizations\")\nprint(\"\\\\nRecommendation: Combine techniques (e.g., quantized ONNX) for maximum benefit!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Comprehensive Performance Summary\n\nLet's summarize all the optimization techniques and their trade-offs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from optimum.onnxruntime import ORTModelForSequenceClassification\nimport onnxruntime as ort\n\n# Convert model to ONNX format\nonnx_model_path = \"./onnx_model\"\n\n# Export to ONNX (this will create the ONNX model)\nprint(\"Converting model to ONNX format...\")\ntry:\n    ort_model = ORTModelForSequenceClassification.from_pretrained(\n        model_name,\n        export=True,\n        use_cache=False\n    )\n    \n    # Save ONNX model\n    ort_model.save_pretrained(onnx_model_path)\n    tokenizer.save_pretrained(onnx_model_path)\n    \n    print(f\"ONNX model saved to {onnx_model_path}\")\n    \n    # Load the ONNX model for inference\n    ort_model = ORTModelForSequenceClassification.from_pretrained(onnx_model_path)\n    ort_tokenizer = AutoTokenizer.from_pretrained(onnx_model_path)\n    \n    # Create pipelines for comparison\n    pytorch_pipeline = pipeline(\n        \"text-classification\",\n        model=model,\n        tokenizer=tokenizer,\n        device=-1  # CPU\n    )\n    \n    onnx_pipeline = pipeline(\n        \"text-classification\",\n        model=ort_model,\n        tokenizer=ort_tokenizer,\n        device=-1  # CPU\n    )\n    \n    # Test with sample texts\n    test_texts = [\n        \"This product exceeded my expectations!\",\n        \"The service was disappointing.\",\n        \"Average quality, nothing special.\"\n    ]\n    \n    print(\"\\\\nBenchmarking inference performance...\")\n    \n    # Benchmark PyTorch pipeline\n    start_time = time.time()\n    pytorch_results = pytorch_pipeline(test_texts)\n    pytorch_time = time.time() - start_time\n    \n    # Benchmark ONNX pipeline\n    start_time = time.time()\n    onnx_results = onnx_pipeline(test_texts)\n    onnx_time = time.time() - start_time\n    \n    print(f\"PyTorch pipeline: {pytorch_time:.4f} seconds\")\n    print(f\"ONNX pipeline:    {onnx_time:.4f} seconds\")\n    \n    # Calculate improvement\n    if onnx_time > 0:\n        speedup = pytorch_time / onnx_time\n        print(f\"ONNX speedup: {speedup:.2f}x faster\")\n    \n    # Compare predictions\n    print(\"\\\\nPrediction comparison:\")\n    print(\"-\" * 50)\n    \n    for i, (text, pt_result, onnx_result) in enumerate(zip(test_texts, pytorch_results, onnx_results)):\n        print(f\"Text {i+1}: {text}\")\n        print(f\"  PyTorch: {pt_result['label']} ({pt_result['score']:.4f})\")\n        print(f\"  ONNX:    {onnx_result['label']} ({onnx_result['score']:.4f})\")\n        print()\n\nexcept Exception as e:\n    print(f\"ONNX conversion failed: {e}\")\n    print(\"This is common in some environments. The concept remains valid for production use.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. ONNX Export for Optimized Inference\n\nONNX (Open Neural Network Exchange) allows models to run on optimized runtimes, providing faster inference across different platforms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a smaller student model\nstudent_config = DistilBertConfig(\n    vocab_size=tokenizer.vocab_size,\n    hidden_size=384,  # Smaller than original (768)\n    num_hidden_layers=4,  # Fewer layers than original (6)\n    num_attention_heads=6,  # Fewer heads than original (12)\n    intermediate_size=1536,  # Smaller than original (3072)\n    num_labels=2\n)\n\nstudent_model = DistilBertForSequenceClassification(student_config)\n\n# Compare model sizes\nteacher_size = get_model_size(model)\nstudent_size = get_model_size(student_model)\n\nprint(f\"Teacher model size: {teacher_size:.2f} MB\")\nprint(f\"Student model size: {student_size:.2f} MB\")\nprint(f\"Size reduction: {(1 - student_size/teacher_size)*100:.1f}%\")\n\n# Simple knowledge distillation loss function\ndef distillation_loss(student_logits, teacher_logits, true_labels, temperature=4.0, alpha=0.7):\n    \"\"\"Compute knowledge distillation loss\"\"\"\n    # Soft targets from teacher\n    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n    soft_prob = F.log_softmax(student_logits / temperature, dim=-1)\n    \n    # KL divergence loss\n    soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temperature ** 2)\n    \n    # Hard targets loss\n    hard_loss = F.cross_entropy(student_logits, true_labels)\n    \n    # Combined loss\n    return alpha * soft_loss + (1 - alpha) * hard_loss\n\n# Simple training loop for knowledge distillation\ndef train_student_model(student, teacher, train_data, num_epochs=2, batch_size=8):\n    \"\"\"Train student model with knowledge distillation\"\"\"\n    teacher.eval()\n    student.train()\n    \n    optimizer = torch.optim.AdamW(student.parameters(), lr=5e-5)\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        # Simple batching\n        for i in range(0, len(train_data), batch_size):\n            batch = train_data[i:i+batch_size]\n            \n            # Prepare batch data\n            input_ids = torch.stack([item['input_ids'] for item in batch])\n            attention_mask = torch.stack([item['attention_mask'] for item in batch])\n            labels = torch.stack([item['label'] for item in batch])\n            \n            # Get teacher predictions\n            with torch.no_grad():\n                teacher_outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n                teacher_logits = teacher_outputs.logits\n            \n            # Get student predictions\n            student_outputs = student(input_ids=input_ids, attention_mask=attention_mask)\n            student_logits = student_outputs.logits\n            \n            # Calculate distillation loss\n            loss = distillation_loss(student_logits, teacher_logits, labels)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        avg_loss = total_loss / num_batches\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    return student\n\nprint(\"\\nTraining student model with knowledge distillation...\")\nstudent_model = train_student_model(student_model, model, train_dataset, num_epochs=2)\n\n# Benchmark the student model\nstudent_time, _ = benchmark_inference(student_model, tokenizer, sample_texts, num_runs=5)\nprint(f\"\\nStudent model inference time: {student_time:.4f} seconds\")\nprint(f\"Speed improvement over teacher: {(original_time/student_time - 1)*100:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import DistilBertConfig, DistilBertForSequenceClassification\nfrom torch.nn import functional as F\n\n# Create synthetic training data for demonstration\ndef create_synthetic_dataset(size=500):\n    \"\"\"Create synthetic text classification data\"\"\"\n    positive_templates = [\n        \"This is amazing!\", \"Great work!\", \"Fantastic product!\",\n        \"I love this!\", \"Excellent quality!\", \"Outstanding service!\"\n    ]\n    negative_templates = [\n        \"This is terrible!\", \"Poor quality!\", \"I hate this!\",\n        \"Awful experience!\", \"Complete waste!\", \"Very disappointing!\"\n    ]\n    \n    texts = []\n    labels = []\n    \n    for i in range(size):\n        if i % 2 == 0:\n            text = np.random.choice(positive_templates)\n            label = 1  # Positive\n        else:\n            text = np.random.choice(negative_templates)\n            label = 0  # Negative\n        \n        # Add some variation\n        text += f\" Item {i}\"\n        texts.append(text)\n        labels.append(label)\n    \n    return Dataset.from_dict({'text': texts, 'label': labels})\n\n# Create smaller datasets for quick training\ntrain_dataset = create_synthetic_dataset(200)\neval_dataset = create_synthetic_dataset(50)\n\nprint(f\"Created {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples\")\nprint(f\"Sample data: {train_dataset[0]}\")\n\n# Tokenize datasets\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding=True, max_length=128)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\neval_dataset = eval_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\neval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Knowledge Distillation\n\nKnowledge distillation trains a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model, achieving similar performance with reduced computational requirements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.nn.utils.prune as prune\nimport copy\n\n# Create a copy of the original model for pruning\nmodel_to_prune = copy.deepcopy(model)\n\ndef apply_global_pruning(model, pruning_ratio=0.2):\n    \"\"\"Apply global magnitude-based pruning to the model\"\"\"\n    # Collect all linear layers for pruning\n    parameters_to_prune = []\n    \n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            parameters_to_prune.append((module, 'weight'))\n    \n    # Apply global magnitude pruning\n    prune.global_unstructured(\n        parameters_to_prune,\n        pruning_method=prune.L1Unstructured,\n        amount=pruning_ratio,\n    )\n    \n    return model\n\ndef calculate_sparsity(model):\n    \"\"\"Calculate the sparsity of the model\"\"\"\n    total_params = 0\n    zero_params = 0\n    \n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            total_params += param.numel()\n            zero_params += (param == 0).sum().item()\n    \n    return zero_params / total_params * 100\n\n# Apply different pruning ratios and compare\npruning_ratios = [0.1, 0.2, 0.3, 0.5]\nresults = []\n\nprint(\"Pruning Results:\")\nprint(\"-\" * 60)\n\nfor ratio in pruning_ratios:\n    # Create a fresh copy of the model\n    pruned_model = copy.deepcopy(model)\n    \n    # Apply pruning\n    pruned_model = apply_global_pruning(pruned_model, ratio)\n    \n    # Calculate sparsity\n    sparsity = calculate_sparsity(pruned_model)\n    \n    # Benchmark performance\n    pruned_time, _ = benchmark_inference(pruned_model, tokenizer, sample_texts, num_runs=5)\n    \n    # Test accuracy\n    pruned_preds = test_model_outputs(pruned_model, tokenizer, sample_texts)\n    \n    # Calculate average prediction difference\n    avg_diff = 0\n    for orig, pruned in zip(original_preds, pruned_preds):\n        avg_diff += np.abs(orig - pruned).mean()\n    avg_diff /= len(original_preds)\n    \n    results.append({\n        'ratio': ratio,\n        'sparsity': sparsity,\n        'time': pruned_time,\n        'accuracy_diff': avg_diff\n    })\n    \n    print(f\"Pruning Ratio: {ratio:4.1f} | Sparsity: {sparsity:5.1f}% | \"\n          f\"Time: {pruned_time:6.4f}s | Acc Diff: {avg_diff:.6f}\")\n\nprint(\"-\" * 60)\nprint(f\"Original Time: {original_time:.4f}s\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Model Pruning\n\nPruning removes less important weights from the model, reducing its size while maintaining most of its performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply dynamic quantization\nquantized_model = torch.quantization.quantize_dynamic(\n    model,\n    {nn.Linear},  # Quantize linear layers\n    dtype=torch.qint8\n)\n\n# Benchmark quantized model\nquantized_size = get_model_size(quantized_model)\nquantized_time, quantized_std = benchmark_inference(quantized_model, tokenizer, sample_texts)\n\nprint(f\"\\nQuantized Model:\")\nprint(f\"  Size: {quantized_size:.2f} MB\")\nprint(f\"  Inference time: {quantized_time:.4f} ± {quantized_std:.4f} seconds\")\n\n# Calculate improvements\nsize_reduction = (1 - quantized_size / original_size) * 100\nspeed_improvement = (original_time / quantized_time - 1) * 100\n\nprint(f\"\\nImprovements:\")\nprint(f\"  Size reduction: {size_reduction:.1f}%\")\nprint(f\"  Speed improvement: {speed_improvement:.1f}%\")\n\n# Test accuracy comparison\ndef test_model_outputs(model, tokenizer, texts):\n    \"\"\"Get model predictions\"\"\"\n    model.eval()\n    predictions = []\n    \n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n            outputs = model(**inputs)\n            pred = torch.nn.functional.softmax(outputs.logits, dim=-1)\n            predictions.append(pred.cpu().numpy())\n    \n    return predictions\n\noriginal_preds = test_model_outputs(model, tokenizer, sample_texts)\nquantized_preds = test_model_outputs(quantized_model, tokenizer, sample_texts)\n\n# Calculate prediction differences\nprint(f\"\\nPrediction Accuracy Comparison:\")\nfor i, (orig, quant) in enumerate(zip(original_preds, quantized_preds)):\n    diff = np.abs(orig - quant).max()\n    print(f\"  Text {i+1} max difference: {diff:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load a pre-trained model for sentiment analysis\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Create sample text for testing\nsample_texts = [\n    \"This movie is absolutely fantastic!\",\n    \"I really disliked this film.\",\n    \"The plot was okay, nothing special.\",\n    \"Outstanding performance by the actors!\"\n]\n\ndef get_model_size(model):\n    \"\"\"Calculate model size in MB\"\"\"\n    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n    return (param_size + buffer_size) / 1024 / 1024\n\ndef benchmark_inference(model, tokenizer, texts, num_runs=10):\n    \"\"\"Benchmark inference time\"\"\"\n    model.eval()\n    times = []\n    \n    with torch.no_grad():\n        for _ in range(num_runs):\n            start_time = time.time()\n            for text in texts:\n                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n                _ = model(**inputs)\n            times.append(time.time() - start_time)\n    \n    return np.mean(times), np.std(times)\n\n# Benchmark original model\noriginal_size = get_model_size(model)\noriginal_time, original_std = benchmark_inference(model, tokenizer, sample_texts)\n\nprint(f\"Original Model:\")\nprint(f\"  Size: {original_size:.2f} MB\")\nprint(f\"  Inference time: {original_time:.4f} ± {original_std:.4f} seconds\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Model Quantization\n\nQuantization reduces the precision of model weights from 32-bit floats to 8-bit integers, significantly reducing model size and improving inference speed with minimal accuracy loss.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, pipeline\n)\nfrom datasets import Dataset\nimport numpy as np\nimport time\nimport os\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 13. Model Optimization with Hugging Face\n\nThis notebook covers various techniques for optimizing Hugging Face models for better performance, reduced memory usage, and faster inference.\n\n## Topics Covered:\n1. **Model Quantization** - Reducing model precision for efficiency\n2. **Dynamic Quantization** - Runtime quantization for inference\n3. **Model Pruning** - Removing less important weights\n4. **Knowledge Distillation** - Training smaller models from larger ones\n5. **ONNX Export** - Converting models for optimized inference\n\nLet's start by installing the required packages and importing necessary libraries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers torch torchvision torchaudio\n!pip install torch-audio --index-url https://download.pytorch.org/whl/cpu\n!pip install optimum[onnxruntime]\n!pip install datasets accelerate"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}